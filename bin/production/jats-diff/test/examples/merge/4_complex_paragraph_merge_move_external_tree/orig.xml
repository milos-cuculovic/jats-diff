<?xml version="1.0" encoding="UTF-8"?>
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="review-article" dtd-version="1.1" xml:lang="en">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">remotesensing</journal-id>
      <journal-title-group>
        <journal-title>Remote Sensing</journal-title>
        <abbrev-journal-title abbrev-type="publisher">Remote Sens.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="pubmed">Remote Sensing</abbrev-journal-title>
      </journal-title-group>
      <issn pub-type="epub">2072-4292</issn>
      <publisher>
        <publisher-name>MDPI</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>      
      <article-id pub-id-type="publisher-id">remotesensing-10-006415</article-id>
      <article-categories>
        <subj-group>
            <subject>
                <p>Review</p>
                <p>Additional Review data</p>
                <p>other</p>
            </subject>            
        </subj-group>
      </article-categories>
      <title-group>
          <article-title>
              <p>On the Use of Unmanned</p>
              <p>Aerial Systems for Environmental Monitoring</p>
          </article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-0225-144X</contrib-id>
          <name>
            <surname>Manfreda Oscaro</surname>
            <given-names>Salvatore</given-names>
          </name>
          <xref rid="af1-remotesensing-10-00641" ref-type="aff">1</xref>
          <xref rid="c1-remotesensing-10-00641" ref-type="corresp">*</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1279-5272</contrib-id>
          <name>
            <surname>McCabe</surname>
            <given-names>Matthew F.</given-names>
          </name>
          <xref rid="af2-remotesensing-10-00641" ref-type="aff">2</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-4157-3994</contrib-id>
          <name>
            <surname>Miller</surname>
            <given-names>Pauline E.</given-names>
          </name>
          <xref rid="af3-remotesensing-10-00641" ref-type="aff">3</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Lucas</surname>
            <given-names>Richard</given-names>
          </name>
          <xref rid="af4-remotesensing-10-00641" ref-type="aff">4</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-6823-9443</contrib-id>
          <name>
            <surname>Pajuelo Madrigal</surname>
            <given-names>Victor</given-names>
          </name>
          <xref rid="af5-remotesensing-10-00641" ref-type="aff">5</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-7123-5358</contrib-id>
          <name>
            <surname>Mallinis</surname>
            <given-names>Giorgos</given-names>
          </name>
          <xref rid="af6-remotesensing-10-00641" ref-type="aff">6</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Ben Dor</surname>
            <given-names>Eyal</given-names>
          </name>
          <xref rid="af7-remotesensing-10-00641" ref-type="aff">7</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-0571-8161</contrib-id>
          <name>
            <surname>Helman</surname>
            <given-names>David</given-names>
          </name>
          <xref rid="af8-remotesensing-10-00641" ref-type="aff">8</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-9358-816X</contrib-id>
          <name>
            <surname>Estes</surname>
            <given-names>Lyndon</given-names>
          </name>
          <xref rid="af9-remotesensing-10-00641" ref-type="aff">9</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-6627-0175</contrib-id>
          <name>
            <surname>Ciraolo</surname>
            <given-names>Giuseppe</given-names>
          </name>
          <xref rid="af10-remotesensing-10-00641" ref-type="aff">10</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>M&#xFC;llerov&#xE1;</surname>
            <given-names>Jana</given-names>
          </name>
          <xref rid="af11-remotesensing-10-00641" ref-type="aff">11</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Tauro</surname>
            <given-names>Flavia</given-names>
          </name>
          <xref rid="af12-remotesensing-10-00641" ref-type="aff">12</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-5134-4175</contrib-id>
          <name>
            <surname>de Lima</surname>
            <given-names>M. Isabel</given-names>
          </name>
          <xref rid="af13-remotesensing-10-00641" ref-type="aff">13</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-0135-2249</contrib-id>
          <name>
            <surname>de Lima</surname>
            <given-names>Jo&#xE3;o L. M. P.</given-names>
          </name>
          <xref rid="af13-remotesensing-10-00641" ref-type="aff">13</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-2778-4680</contrib-id>
          <name>
            <surname>Maltese</surname>
            <given-names>Antonino</given-names>
          </name>
          <xref rid="af10-remotesensing-10-00641" ref-type="aff">10</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1173-4969</contrib-id>
          <name>
            <surname>Frances</surname>
            <given-names>Felix</given-names>
          </name>
          <xref rid="af14-remotesensing-10-00641" ref-type="aff">14</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Caylor</surname>
            <given-names>Kelly</given-names>
          </name>
          <xref rid="af15-remotesensing-10-00641" ref-type="aff">15</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Kohv</surname>
            <given-names>Marko</given-names>
          </name>
          <xref rid="af16-remotesensing-10-00641" ref-type="aff">16</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Perks</surname>
            <given-names>Matthew</given-names>
          </name>
          <xref rid="af17-remotesensing-10-00641" ref-type="aff">17</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-9966-9438</contrib-id>
          <name>
            <surname>Ruiz-P&#xE9;rez</surname>
            <given-names>Guiomar</given-names>
          </name>
          <xref rid="af18-remotesensing-10-00641" ref-type="aff">18</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Su</surname>
            <given-names>Zhongbo</given-names>
          </name>
          <xref rid="af19-remotesensing-10-00641" ref-type="aff">19</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-7849-2653</contrib-id>
          <name>
            <surname>Vico</surname>
            <given-names>Giulia</given-names>
          </name>
          <xref rid="af18-remotesensing-10-00641" ref-type="aff">18</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1485-8908</contrib-id>
          <name>
            <surname>Toth</surname>
            <given-names>Brigitta</given-names>
          </name>
          <xref rid="af20-remotesensing-10-00641" ref-type="aff">20</xref>
          <xref rid="af21-remotesensing-10-00641" ref-type="aff">21</xref>
        </contrib>
      </contrib-group>
      <aff id="af1-remotesensing-10-00641"><label>1</label>Dipartimento delle Culture Europee e del Mediterraneo: Architettura, Ambiente, Patrimoni Culturali (DiCEM), Universita degli Studi della Basilicata, 75100 Matera, Italy</aff>
      <aff id="af2-remotesensing-10-00641"><label>2</label>Water Desalination and Reuse Center, King Abdullah University of Science and Technology, 23955 Thuwal, Saudi Arabia</aff>
      <aff id="af3-remotesensing-10-00641"><label>3</label>The James Hutton Institute, Aberdeen AB15 8QH, UK</aff>
      <aff id="af4-remotesensing-10-00641"><label>4</label>Department of Geography and Earth Sciences, Aberystwyth University, Aberystwyth, Ceredigion SY23 3DB, UK</aff>
      <aff id="af5-remotesensing-10-00641"><label>5</label>Svarmi ehf., &#xC1;rleyni 22, 112 Reykjav&#xED;k, Iceland</aff>
      <aff id="af6-remotesensing-10-00641"><label>6</label>Department of Forestry and Management of the Environment and Natural Resources, Democritus University of Thrace, 67100 Xanthi, Greece</aff>
      <aff id="af7-remotesensing-10-00641"><label>7</label>Department of Geography and Human Environment, Tel Aviv University (TAU), Tel Aviv 6997801, Israel</aff>
      <aff id="af8-remotesensing-10-00641"><label>8</label>Department of Geography and the Environment, Bar-Ilan University, Ramat Gan 52900, Israel</aff>
      <aff id="af9-remotesensing-10-00641"><label>9</label>Graduate School of Geography, Clark University, Worcester, MA 01610, USA</aff>
      <aff id="af10-remotesensing-10-00641"><label>10</label>Dipartimento di Ingegneria Civile, Ambientale, Aerospaziale, dei Materiali, University of Palermo, 90128 Palermo, Italy</aff>
      <aff id="af11-remotesensing-10-00641"><label>11</label>Department GIS and Remote Sensing, Institute of Botany, The Czech Acad. Sciences, 252 43 Pr&#x16F;honice, Czech Republic</aff>
      <aff id="af12-remotesensing-10-00641"><label>12</label>Centro per l&#x2019;Innovazione Tecnologica e lo Sviluppo del Territorio (CINTEST), Universita degli Studi della Tuscia, 01100 Viterbo, Italy</aff>
      <aff id="af13-remotesensing-10-00641"><label>13</label>Marine and Environmental Sciences Centre, Department of Civil Engineering, University of Coimbra, 3000-370 Coimbra, Portugal</aff>
      <aff id="af14-remotesensing-10-00641"><label>14</label>Research Group of Hydrological and Environmental Modelling (GIHMA), Research Institute of Water and Environmental Engineering, Universidad Politecnica de Valencia, 46022 Val&#xE8;ncia, Spain</aff>
      <aff id="af15-remotesensing-10-00641"><label>15</label>Department of Geography, University of California, Santa Barbara, CA 93106-3060, USA</aff>
      <aff id="af16-remotesensing-10-00641"><label>16</label>Department of Geology, University of Tartu, 50090 Tartu, Estonia</aff>
      <aff id="af17-remotesensing-10-00641"><label>17</label>School of Geography, Politics and Sociology, Newcastle University, Newcastle upon Tyne NE1 7RU, UK</aff>
      <aff id="af18-remotesensing-10-00641"><label>18</label>Department of Crop Production Ecology, Swedish University of Agricultural Sciences (SLU), 750 07 Uppsala, Sweden</aff>
      <aff id="af19-remotesensing-10-00641"><label>19</label>Department of Water Resources in Faculty of Geo-Information and Earth Observation, University of Twente, 7522 NB Enschede, The Netherlands</aff>
      <aff id="af20-remotesensing-10-00641"><label>20</label>Institute for Soil Sciences and Agricultural Chemistry, Centre for Agricultural Research, Hungarian Academy of Sciences, H-1022 Budapest, Hungary</aff>
      <aff id="af21-remotesensing-10-00641"><label>21</label>Department of Crop Production and Soil Science, University of Pannonia, 8360 Keszthely, Hungary</aff>
      <author-notes>
        <corresp id="c1-remotesensing-10-00641"><label>*</label>Correspondence: <email>salvatore.Manfreda@unibas.it</email></corresp>
      </author-notes>
      <pub-date pub-type="epub">
        <day>20</day>
        <month>04</month>
        <year>2018</year>
      </pub-date>
      <pub-date pub-type="collection">
        <month>04</month>
        <year>2018</year>
      </pub-date>
      <volume>10</volume>
      <issue>4</issue>
      <elocation-id>641</elocation-id>
      <history>
        <date date-type="received">
          <day>12</day>
          <month>03</month>
          <year>2018</year>
        </date>
        <date date-type="accepted">
          <day>17</day>
          <month>04</month>
          <year>2018</year>
        </date>
      </history>
      <permissions>
        <p>permission 1</p>
        <p>permission additional</p>
        <copyright-statement>&#xA9; 2018 by the authors.</copyright-statement>
        <copyright-year>2018</copyright-year>
        <license license-type="open-access">
          <license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p>
        </license>
      </permissions>
      <abstract>
        <p>Environmental monitoring plays a central role in diagnosing climate and management impacts on natural and agricultural systems; enhancing the understanding of hydrological processes; optimizing the allocation and distribution of water resources; and assessing, forecasting, and even preventing natural disasters. Nowadays, most monitoring and data collection systems are based upon a combination of ground-based measurements, manned airborne sensors, and satellite observations. These data are utilized in describing both small- and large-scale processes, but have spatiotemporal constraints inherent to each respective collection system. Bridging the unique spatial and temporal divides that limit current monitoring platforms is key to improving our understanding of environmental systems. In this context, Unmanned Aerial Systems (UAS) have considerable potential to radically improve environmental monitoring. UAS-mounted sensors offer an extraordinary opportunity to bridge the existing gap between field observations and traditional air- and space-borne remote sensing, by providing high spatial detail over relatively large areas in a cost-effective way and an entirely new capacity for enhanced temporal retrieval. As well as showcasing recent advances in the field, there is also a need to identify and understand the potential limitations of UAS technology. For these platforms to reach their monitoring potential, a wide spectrum of unresolved issues and application-specific challenges require focused community attention. Indeed, to leverage the full potential of UAS-based approaches, sensing technologies, measurement protocols, postprocessing techniques, retrieval algorithms, and evaluation techniques need to be harmonized. The aim of this paper is to provide an overview of the existing research and applications of UAS in natural and agricultural ecosystem monitoring in order to identify future directions, applications, developments, and challenges.</p>
        <p>Tag2</p>
        <p>Tag3</p>
      </abstract>
      <kwd-group>
        <kwd>UAS</kwd>
        <kwd>remote sensing</kwd>
        <kwd>environmental monitoring</kwd>
        <kwd>precision agriculture</kwd>
        <kwd>vegetation indices</kwd>
        <kwd>soil moisture</kwd>
        <kwd>river monitoring</kwd>
      </kwd-group>
    </article-meta>
  </front>
  <body>
    <sec id="sec1-remotesensing-10-00641" sec-type="intro">
      <title>1. Introduction</title>
      <p>Despite the recent and rapid increase in the number and range of Earth observing satellites [<xref ref-type="bibr" rid="B1-remotesensing-10-00641">1</xref>,<xref ref-type="bibr" rid="B2-remotesensing-10-00641">2</xref>,<xref ref-type="bibr" rid="B3-remotesensing-10-00641">3</xref>], the temporal resolution and availability of current very high spatial resolution satellite sensors (less than 10 m) are generally not sufficient nor flexible enough for many quantitative remote sensing applications, and they are thus of limited use in detecting and monitoring the dynamics of surficial environmental processes. Recent advances in Earth observation (i.e., EO) are opening new opportunities for environmental monitoring at finer scales [<xref ref-type="bibr" rid="B4-remotesensing-10-00641">4</xref>]. For instance, CubeSat platforms represent a promising satellite technology, operating predominantly in the visible to near-infrared portion of the electromagnetic spectrum, and provide high spatial and temporal resolution [<xref ref-type="bibr" rid="B5-remotesensing-10-00641">5</xref>]. Nevertheless, most of these satellites are operated by commercial organizations; hence, if short revisit times are required (i.e., for high-frequency monitoring), the cost of image acquisition can become a limiting factor. While manned airborne platforms can, in principle, provide both high spatial resolution and rapid revisit times, in practice, their use is routinely limited by operational complexity, safety, logistics, and cost. Their use becomes feasible only over medium-sized areas and remains largely within the domain of commercial operators. Recent advances in Unmanned Aerial Systems (UAS) technology have created an alternative monitoring platform that provides an opportunity to capture the spatial, spectral, and temporal requirements across a range of applications with relatively small investment. They offer high versatility, adaptability, and flexibility compared with manned airborne systems or satellites, and have the potential to be rapidly and repeatedly deployed for high spatial and temporal resolution data [<xref ref-type="bibr" rid="B6-remotesensing-10-00641">6</xref>].</p>
      <p>While UAS systems cannot compete with satellite imagery in terms of spatial coverage, they provide unprecedented spatial and temporal resolutions unmatched by satellite alternatives. Furthermore, they do so at a fraction of the satellite acquisition cost if the area of interest has relatively small extent. For example, a newly tasked high-resolution natural color image (50 cm/pixel) from a satellite (e.g., GeoEye-1) can cost up to 3000 USD. On the other hand, the initial outlay to acquire a single UAS with a natural color camera can be less than 1000 USD (see <xref ref-type="app" rid="app1-remotesensing-10-00641">Appendix A</xref>), with this delivering a dataset of high spatial resolution (several cm/pixel). Of course, the additional benefit of the UAS platform is that the temporal resolution is limited only by the number of flights (and power supply/battery capacity), so any cost equivalence is quickly overcome due to repeatability. The costs for acquiring UAS imagery are usually derived from the initial investment, the processing software, data storage, and associated (and ongoing) fieldwork expenses. However, after the initial investment, datasets can be delivered more often and at a higher resolution than by any other EO system.</p>
      <p>Matese et al. [<xref ref-type="bibr" rid="B7-remotesensing-10-00641">7</xref>] provided an intercomparison of the acquisition and processing costs of three different platforms (UAS, Aircraft, and Satellite). Their cost model parametrization allows the derivation of the relative cost for the different configurations, showing that UAS is the most cost-effective solution for fields of an extent equal to or less than 20 ha. Their quantitative analyses showed that the approximate total cost of a UAS-derived Normalized Difference Vegetation Index (NDVI) map over a 5 ha field is equal to 400 &#x20AC;/ha, while satellite products may cost about 30% more.</p>
      <p>A cost&#x2013;benefit analysis for monitoring and maintaining parks facilities, such as the Deleo Regional Sports Park, identified a clear economical convenience in the use of UAS for an area with an extent of approximately 10 ha [<xref ref-type="bibr" rid="B8-remotesensing-10-00641">8</xref>]. Of course, the theoretical limit for such economic convenience may be affected by several parameters (e.g., type of vehicle, sensors adopted, frequency of flights, and postprocessing) and this may lead to a nonunique result, but there is a general coherence in the literature that tends to identify such a limit in a lower bound between 10 and 20 ha. Over larger areas, acquisition, georeferencing, and orthorectification costs impact negatively on the economic costs of UAS-derived images.</p>
      <p>The dynamic nature and spatial variability of environmental processes that occur at very fine scales require data of an equivalent high spatial and temporal resolution. For successful and efficient monitoring, timely data are necessary, and high flexibility makes the UAS imagery ideal for the task. Specific timing and frequent acquisition of data at very fine scales also enables targeted monitoring of rapid (interannual) changes of environmental features, including plant phenology and growth, extreme events, and hydrological processes. For these reasons, environmental studies were among the first civil applications of the technology in 1990s. Thanks to the significant cost reduction of both vehicles and sensors, and recent developments in data processing software, UAS applications have expanded rapidly in the last decade, stimulating a number of additional and complementary topics spanning full automation of single or multiple vehicles, tracking and flight control systems, hardware and software innovations, tracking of moving targets, and image correction and mapping performance assessment. The growing interest in those applications is reflected in the number of UAS-based research papers published over the last 27 years, with a focus on those being directed towards environmental monitoring (based on a search of the ISI (Internation Scientific Indexing)-web of knowledge using the keywords &#x201C;UAS&#x201D; or &#x201C;UAV&#x201D;, and &#x201C;environment&#x201D;). In particular, the number of applications has seen a particularly prominent increase over the last five years (<xref ref-type="fig" rid="remotesensing-10-00641-f001">Figure 1</xref>).</p>
      <p>Of course, it is not an equivalent assessment to compare these platforms on an image-by-image basis, as it is the spatiotemporal richness of the UAS systems that makes their application so transformative. Beyond allowing the high spatial and temporal resolutions needed for many applications, UAS-mounted sensors have several additional advantages which are key across a range of applications. First, they provide rapid access to environmental data, offering the near real-time capabilities required in many applications. The most mature of these is the capacity to share orthomosaic and elevation data, using both commercial and open-source alternatives [<xref ref-type="bibr" rid="B9-remotesensing-10-00641">9</xref>]. Second, UAS satisfy safety requirements and accessibility issues for inspection of otherwise inaccessible sites or for hazard detection and monitoring [<xref ref-type="bibr" rid="B10-remotesensing-10-00641">10</xref>]. Third, the great advantage of UAS is their capacity to collect data in cloudy or hazy conditions that would otherwise obscure satellite retrieval. Analysis of meteorological data has shown that, even with daily revisits of Earth observation satellites, the probability of operating a monitoring service based on optical satellite imagery in rainy regions is about 20%, while the probability of obtaining a usable image with UAS is between 45 and 70% [<xref ref-type="bibr" rid="B11-remotesensing-10-00641">11</xref>]. Perhaps most importantly, operations with UAS are not limited to specific hours (as with sun-synchronous satellite sensors), and, thus, UAS can be used for continuous environmental monitoring.</p>
      <p>These aforementioned capabilities, together with the increasing variety and affordability of both UAS and sensor technologies, have stimulated an explosion of interest from researchers across numerous domains [<xref ref-type="bibr" rid="B12-remotesensing-10-00641">12</xref>,<xref ref-type="bibr" rid="B13-remotesensing-10-00641">13</xref>,<xref ref-type="bibr" rid="B14-remotesensing-10-00641">14</xref>,<xref ref-type="bibr" rid="B15-remotesensing-10-00641">15</xref>,<xref ref-type="bibr" rid="B16-remotesensing-10-00641">16</xref>]. Among others, Singh and Frazier [<xref ref-type="bibr" rid="B17-remotesensing-10-00641">17</xref>] provided a detailed meta-analysis on published articles highlighting the diversity of UAS processing procedures, clearly identifying the critical need for a harmonization and standardization among the many possible strategies to acquire and preprocess data to derive UAS-based products.</p>
      <p>In addition to the increasing availability of UAS, recent advances in sensor technologies and analytical capabilities are rapidly expanding the number of potential UAS applications. Increasing miniaturization allows multispectral, hyperspectral, and thermal imaging, as well as Synthetic Aperture Radar (SAR) and LiDAR (Light Detection and Ranging) sensing to be conducted from UAS. As examples of recent UAS-based environmental monitoring applications, work has focused on (a) land cover mapping [<xref ref-type="bibr" rid="B18-remotesensing-10-00641">18</xref>,<xref ref-type="bibr" rid="B19-remotesensing-10-00641">19</xref>]; (b) vegetation state, phenology, and health [<xref ref-type="bibr" rid="B20-remotesensing-10-00641">20</xref>,<xref ref-type="bibr" rid="B21-remotesensing-10-00641">21</xref>]; (c) precision farming/agriculture [<xref ref-type="bibr" rid="B22-remotesensing-10-00641">22</xref>,<xref ref-type="bibr" rid="B23-remotesensing-10-00641">23</xref>,<xref ref-type="bibr" rid="B24-remotesensing-10-00641">24</xref>]; (d) monitoring crop growth, and invasive species infestation [<xref ref-type="bibr" rid="B25-remotesensing-10-00641">25</xref>,<xref ref-type="bibr" rid="B26-remotesensing-10-00641">26</xref>]; (e) atmospheric observations [<xref ref-type="bibr" rid="B27-remotesensing-10-00641">27</xref>]; (f) disaster mapping [<xref ref-type="bibr" rid="B28-remotesensing-10-00641">28</xref>]; (g) soil erosion [<xref ref-type="bibr" rid="B29-remotesensing-10-00641">29</xref>,<xref ref-type="bibr" rid="B30-remotesensing-10-00641">30</xref>]; (h) mapping soil surface characteristics [<xref ref-type="bibr" rid="B31-remotesensing-10-00641">31</xref>,<xref ref-type="bibr" rid="B32-remotesensing-10-00641">32</xref>]; and (i) change detection [<xref ref-type="bibr" rid="B33-remotesensing-10-00641">33</xref>].</p>
      <p>Given the research and technological advances in recent years and the rapidly evolving landscape with respect to UAS applications, the aim of this paper is to review the current state of the art in the field of UAS applications for environmental monitoring, with a particular focus on hydrological variables, such as vegetation conditions, soil properties and moisture, overland flow, and streamflow.</p>
      <p>This review provides a common shared knowledge framework that can be used to guide and address the future activities of the international research network. We divide our review into three sections that focus on different (but related) aspects of UAS-based environmental monitoring: (1) data collection and processing; (2) monitoring natural and agricultural ecosystems; and (3) monitoring river systems. We conclude by summarizing current and emerging issues, potential roadblocks, and other challenges in further advancing the application of UAS in environmental monitoring.</p>
    </sec>
    <sec id="sec2-remotesensing-10-00641">
      <title>2. Data Collection, Processing, and Limitations</title>
      <p>While offering an unprecedented platform to advance spatiotemporal insights across the Earth and environmental sciences, UAS are not without their own operational, processing, and retrieval problems. These range from image blur due to the forward motion of the platform [<xref ref-type="bibr" rid="B34-remotesensing-10-00641">34</xref>], resolution impacts due to variable flying height, orthorectification issues and geometric distortion associated with inadequate image overlap [<xref ref-type="bibr" rid="B35-remotesensing-10-00641">35</xref>], and the spectral effects induced by variable illumination during flight. These and other factors can all affect the subsequent quality of any orthorectified image and, subsequently, the derived products. These are well described in a recent review paper by Whitehead and Hugenholtz [<xref ref-type="bibr" rid="B13-remotesensing-10-00641">13</xref>]. As such, it is essential to consider best practice in the context of (a) mission and flight planning; (b) preflight camera/sensor configuration; (c) in-flight data collection; (d) ground control/ radiometric calibration and correction; (e) geometric and atmospheric corrections; (f) orthorectification and image mosaicking; and (g) extracting relevant products/metrics for remote sensing application. Items (a) and (b) are preflight tasks, (c) and (d) are conducted in the field at the time of survey, and (e)&#x2013;(g) are postsurvey tasks. Together, these aspects are crucial to data acquisition and postprocessing, which deliver the necessary starting point for subsequent application-specific analysis. However, despite the existence of well-established workflows in photogrammetry, manned aircraft, and satellite-based remote sensing to address such fundamental aspects, UAS systems introduce various additional complexities, which to date have not been thoroughly addressed. Consequently, best practice workflows for producing high-quality remote sensing products from UAS are still lacking, and further studies that focus on validating UAS-collected measurements with robust processing methods are important for improving the final quality of the processed data [<xref ref-type="bibr" rid="B36-remotesensing-10-00641">36</xref>,<xref ref-type="bibr" rid="B37-remotesensing-10-00641">37</xref>].</p>
      <sec id="sec2dot1-remotesensing-10-00641">
        <title>2.1. Preflight Planning</title>
        <p>Flight or mission planning is the first essential step for UAS data acquisition and has a profound impact on the data acquired and the processing workflow. Similar to other remote sensing approaches, a host of parameters must be considered before the actual flight, such as platform specifications, the extent of the study site (area-of-interest), ground sampling distance, payload characteristics, topography of the study site, goals of the study, meteorological forecasts, and local flight regulations. UAS have additional aspects that require further consideration, including the skill level of the pilot, platform characteristics, and actual environmental flight conditions&#x2014;all of which affect the data characteristics and subsequent phases of processing.</p>
        <p>Due to the proliferation of low-cost, off-the-shelf digital cameras, photogrammetry has been the primary implementation of UAS. James and Robson [<xref ref-type="bibr" rid="B38-remotesensing-10-00641">38</xref>] highlighted how unresolved elements of the camera model (lens distortion) can propagate as errors in UAS-DEMs (derived digital elevation models), and how this can be addressed by incorporating oblique images. Other studies have highlighted the importance of flight line configurations [<xref ref-type="bibr" rid="B39-remotesensing-10-00641">39</xref>], as well as minimizing image blur [<xref ref-type="bibr" rid="B34-remotesensing-10-00641">34</xref>]. There is a need to consolidate this evidence to develop best practice guidance for optimizing UAS SfM (structure-from-motion) measurement quality, whilst maintaining ease of use and accessibility.</p>
        <p>Accurate absolute orientation (georeferencing) is an important element for UAS surveys, and is fundamental for any multitemporal monitoring or comparison to other datasets. This task is often referred to as registration, and is conventionally dependent on establishing ground control points (GCPs) which are fixed by a higher-order control method (usually Global Navigation Satellite System&#x2014;GNSS or Global Positioning System). A number of studies have examined the effect of GCP networks (number and distribution) in UAS surveys, showing that significant errors are expected in SfM-based products where GCPs are not adopted [<xref ref-type="bibr" rid="B39-remotesensing-10-00641">39</xref>,<xref ref-type="bibr" rid="B40-remotesensing-10-00641">40</xref>]. Nevertheless, systematic DEM error can be significantly reduced by including properly defined GCPs [<xref ref-type="bibr" rid="B41-remotesensing-10-00641">41</xref>] or incorporating oblique images in the absence of GCP [<xref ref-type="bibr" rid="B38-remotesensing-10-00641">38</xref>].</p>
        <p>Best practice can also be drawn from manned aerial photogrammetry. Direct georeferencing is standard practice in aerial photogrammetry, where the position and orientation of the platform are precisely determined using on-board survey-grade differential GNSS and inertial measurement unit (IMU) data combined through an inertial navigation system (INS) [<xref ref-type="bibr" rid="B42-remotesensing-10-00641">42</xref>]. This allows the camera station (exposure) position and orientation to be derived directly, thus eliminating or minimizing the need for ground control points. Therefore, as discussed by Colomina and Molina [<xref ref-type="bibr" rid="B35-remotesensing-10-00641">35</xref>], there is an increasing drive towards achieving centimeter-level direct georeferencing for UAS using alternative GNSS/IMU configurations, precise point positioning (PPP), and dual-frequency GNSS.</p>
      </sec>
      <sec id="sec2dot2-remotesensing-10-00641">
        <title>2.2. Sensors</title>
        <p>The large availability of UAS equipped with visible (VIS) commercial cameras (see <xref ref-type="table" rid="remotesensing-10-00641-t0A1">Table A1</xref>) has been the main driver for research that has explored the potential use of low-cost sensors for vegetation monitoring [<xref ref-type="bibr" rid="B43-remotesensing-10-00641">43</xref>,<xref ref-type="bibr" rid="B44-remotesensing-10-00641">44</xref>,<xref ref-type="bibr" rid="B45-remotesensing-10-00641">45</xref>,<xref ref-type="bibr" rid="B46-remotesensing-10-00641">46</xref>]. Among the many available visible spectral indices, the Normalized Green&#x2013;Red Difference Index (NGRDI) and Excessive Green (ExG) indices have been used to provide acceptable or high levels of accuracy in vegetation mapping studies. Such vegetation indices may be a cost-effective tool for plant biomass estimation and establishing yield variation maps for site-specific agricultural decision-making.</p>
        <p>Over the last five to eight years, near-infrared (NIR) multi- and hyperspectral sensors have become more widely available for UAS. Modified off-the-shelf RGB (red&#x2013;green&#x2013;blue) cameras&#x2014;initially very popular [<xref ref-type="bibr" rid="B47-remotesensing-10-00641">47</xref>]&#x2014;have now started to be replaced by dedicated multispectral or hyperspectral cameras, as these have reduced in cost and weight. For instance, lightweight hyperspectral sensors for UAS are now available from different vendors (e.g., SPECIM; HYSPEX; HeadWall; see <xref ref-type="app" rid="app1-remotesensing-10-00641">Appendix A</xref>), offering more defined and discrete spectral responses compared to the modified RGB or multiband cameras. Multispectral cameras commonly employ multiple lenses, which introduce band-to-band offsets that need to be adequately corrected in order to avoid artefacts introduced into the combined multiband product [<xref ref-type="bibr" rid="B48-remotesensing-10-00641">48</xref>,<xref ref-type="bibr" rid="B49-remotesensing-10-00641">49</xref>]. Both multispectral and hyperspectral cameras require radiometric calibration and atmospheric corrections to convert the recorded digital numbers (DN) to surface reflectance values to enable reliable assessment of ground features, comparison of repeated measurements, and reliable determination of spectral indices [<xref ref-type="bibr" rid="B50-remotesensing-10-00641">50</xref>]. Although DN are frequently utilized directly to derive vegetation indices (e.g., NDVI), illumination differences between (and within) surveys mean that the use of these values is generally inappropriate, particularly for quantitative studies.</p>
        <p>Radiometric calibration normally involves in-field measurement of reference natural or artificial targets with a field spectroradiometer [<xref ref-type="bibr" rid="B50-remotesensing-10-00641">50</xref>,<xref ref-type="bibr" rid="B51-remotesensing-10-00641">51</xref>,<xref ref-type="bibr" rid="B52-remotesensing-10-00641">52</xref>] and calibration of individual cameras requiring significant additional effort. Some current multispectral cameras (e.g., Parrot Sequoia, MicaSense RedEdge&#x2014;see <xref ref-type="table" rid="remotesensing-10-00641-t0A2">Table A2</xref>) include a downwelling irradiance sensor and calibrated reflectance panel in order to address some of the requirements of radiometric calibration. This is beneficial, but it does not address the full complexity of radiometric calibration, and artefacts will remain. Other aspects, such as bidirectional reflectance (modelled through the bidirectional reflectance distribution function (BRDF)) and image vignetting, introduce further uncertainties for image classification. While the most appropriate workflow for dealing with multispectral imagery depends to some extent on the complexity of the subsequent application (e.g., basic vegetation indices or reflectance-based image classification), the growing body of literature and recent sensor improvements support the development of best practice guidelines for the environmental UAS community.</p>
        <p>Hyperspectral sensors (<xref ref-type="table" rid="remotesensing-10-00641-t0A3">Table A3</xref>) can be briefly mentioned as extensions of the discussion surrounding multispectral sensors and related considerations of radiometric calibration and atmospheric correction. Over the last five years, there has been increasing interest in hyperspectral imaging sensors [<xref ref-type="bibr" rid="B9-remotesensing-10-00641">9</xref>,<xref ref-type="bibr" rid="B53-remotesensing-10-00641">53</xref>]. While these are still more expensive than multispectral systems, they offer significant potential for quantitative soil vegetation and crop studies. UAS hyperspectral imagers typically offer contiguous narrow bands in the VIS-&#x2013;NIR (near-infrared) portion of the spectrum. Existing cameras include pushbroom and, more recently, frame capture technology. Depending on the capture mechanism, there are typically artefacts related to noninstantaneous (time delay) capture across bands, or physical offsets between bands [<xref ref-type="bibr" rid="B53-remotesensing-10-00641">53</xref>]. There has also been interest in (nonimaging) UAS-mounted (hyperspectral) spectrometers [<xref ref-type="bibr" rid="B54-remotesensing-10-00641">54</xref>].</p>
        <p>In the hyperspectral domains, high radiometric accuracy and accurate reflectance retrieval are key factors to further exploit this technology [<xref ref-type="bibr" rid="B55-remotesensing-10-00641">55</xref>]. Accordingly, practices from the manned platforms bearing hyperspectral sensors can be adopted in UAS applications, such as the new super-vicarious calibration method suggested by Brook and Ben-Dor [<xref ref-type="bibr" rid="B51-remotesensing-10-00641">51</xref>,<xref ref-type="bibr" rid="B56-remotesensing-10-00641">56</xref>]. This study used artificial targets to assess data quality, to correct at-sensor radiance, and to generate a high-quality reflectance data-cube. Technologies that have been introduced also include light sensors in the SWIR (shortwave infrared) region, with these produced specifically for UAS applications (HeadWall).</p>
        <p>UAS broadband thermal imaging sensors (see <xref ref-type="table" rid="remotesensing-10-00641-t0A4">Table A4</xref>) measure the emitted radiance of the Earth&#x2019;s surface (from which the brightness temperature can be calculated) typically between 7.5 and 13.5 &#x3BC;m. Key considerations relate to spatial resolution and thermal sensitivity, with the latter now achieving 40&#x2013;50 mK. Thermal UAS remote sensing also requires consideration of radiometric calibration and accounting for vignetting and other systematic effects, as discussed by Smigaj et al. [<xref ref-type="bibr" rid="B57-remotesensing-10-00641">57</xref>]. With the aim to provide a description of the potential of a thermal camera mounted on a UAS, an example of a thermal image providing the surface temperature (in degrees Celsius) obtained over a vineyard of Aglianico is given in <xref ref-type="fig" rid="remotesensing-10-00641-f002">Figure 2</xref>. This information can be used to compute the vegetation state or soil water content given the strong relationship existing between these variables and the surface energy balance. Here, one can appreciate the high level of detail offered by this technology in the description of a patchy area of vegetation.</p>
        <p>LiDAR sensors (see <xref ref-type="table" rid="remotesensing-10-00641-t0A5">Table A5</xref>) are also becoming more commonplace on UAS platforms, as increasingly lightweight systems become achievable (although &lt;3 kg maximum take-off weight is still challenging). There is particular interest in UAS LiDAR for forestry applications, especially in relation to classifying and quantifying structural parameters (e.g., forest height, crown dimensions; [<xref ref-type="bibr" rid="B58-remotesensing-10-00641">58</xref>]).</p>
        <p>Each of the sensors listed in this or the previous section allows one to derive information with some sort of drawback. For instance, hyperspectral and thermal cameras can provide a more appropriate description of the physiological state of vegetation, but at the expense of the spatial resolution, costs, and complexity of processing and calibration. Using LiDAR technology provides detailed information about the vegetation structure but is demanding in terms of the data processing and costs of the sensor. Therefore, there is a critical need to identify a standard approach for specific tasks that can reduce sensor errors and associated elaboration, enhancing the reliability of UAS observations.</p>
        <p>A review of the available cameras and sensors for UAS applications is provided in the <xref ref-type="app" rid="app1-remotesensing-10-00641">Appendix</xref> in order to guide future studies and activities in this field. Tables include a number of pieces of technical information along with an approximate price quote when available. </p>
      </sec>
      <sec id="sec2dot3-remotesensing-10-00641">
        <title>2.3. Software</title>
        <p>Alongside sensor technological developments, low-cost (and particularly open source) software has been vital in enabling the growth in UAS for environmental and other applications. UAS-based photogrammetry can produce products of a similar accuracy to those achievable through manned airborne systems [<xref ref-type="bibr" rid="B35-remotesensing-10-00641">35</xref>]. This has been underpinned by the development of SfM software, which offers a user-friendly and low-cost alternative to conventional digital photogrammetric processing. This includes proprietary structure-from-motion (SfM) software such as Agisoft Photoscan and Pix4D, which is significantly more affordable than most conventional photogrammetric software. Moreover, there has also been development of open source SfM software, including VisualSfM, Bundler, Apero-MicMac, OpenDroneMap, etc. Nevertheless, although different and efficient software solutions are available, the computational cost of the elaboration is critical and it can require several days of data processing. Cloud-based platforms such as DroneDeploy or DroneMapper offer the possibility to integrate and share aerial data, but also to derive orthomosaics with light processing workloads. While this has made photogrammetry more accessible to nonexperts, quantification of uncertainty remains an ongoing challenge [<xref ref-type="bibr" rid="B59-remotesensing-10-00641">59</xref>]. This is because SfM relaxes some of the conventional expectations in terms of image block geometry and data acquisition.</p>
      </sec>
    </sec>
    <sec id="sec3-remotesensing-10-00641">
      <title>3. Monitoring Agricultural and Natural Ecosystems</title>
      <p>Natural and agricultural ecosystems are influenced by climatic forcing, physical characteristics, and management practices that are highly variable in both time and space. Moreover, vegetation state changes can often occur within a short period of time [<xref ref-type="bibr" rid="B60-remotesensing-10-00641">60</xref>,<xref ref-type="bibr" rid="B61-remotesensing-10-00641">61</xref>] due to unfavorable growing conditions or climatic extremes (e.g., heat waves, heavy storms, etc.). Therefore, in order to capture such features, monitoring systems need to provide accurate information over large areas with a high revisit frequency [<xref ref-type="bibr" rid="B62-remotesensing-10-00641">62</xref>]. UAS platforms provide one such technology that is enabling new horizons in vegetation monitoring. For instance, the high resolution of UAS imagery has led to a significant increase in the overall accuracy in species-level vegetation classification, monitoring vegetation status, weed infestations, estimating biomass, predicting yields, detecting crop water stress and/senescent leaves, reviewing herbicide applications, and pest control.</p>
      <sec id="sec3dot1-remotesensing-10-00641">
        <title>3.1. Vegetation Monitoring and Precision Agriculture</title>
        <p>Precision agriculture [<xref ref-type="bibr" rid="B63-remotesensing-10-00641">63</xref>] has been the most common environmental monitoring application of UAS. High-spatial-resolution UAS imagery enables much earlier and more cost-effective detection, diagnosis, and corrective action of agricultural management problems compared to low-resolution satellite imagery. Therefore, UAS may provide the required information to address the needs of farmers or other users at the field scale, enabling them to make better management decisions with minimal costs and environmental impact [<xref ref-type="bibr" rid="B64-remotesensing-10-00641">64</xref>,<xref ref-type="bibr" rid="B65-remotesensing-10-00641">65</xref>,<xref ref-type="bibr" rid="B66-remotesensing-10-00641">66</xref>].</p>
        <p>Vegetation state can be evaluated and quantified through different vegetation indices from images acquired in the visible, red edge, and near-infrared spectral bands. Depending on their formulation, these can display a strong correlation with soil coverage and Leaf and Green Area Index (LAI and GAI), Crop Nitrogen Uptake (QN), chlorophyll content, water stress detection, canopy structure, photosynthesis, yield, and/or growing conditions [<xref ref-type="bibr" rid="B67-remotesensing-10-00641">67</xref>,<xref ref-type="bibr" rid="B68-remotesensing-10-00641">68</xref>,<xref ref-type="bibr" rid="B69-remotesensing-10-00641">69</xref>]. As such, these vegetation indices may be exploited to monitor biophysical parameters.</p>
        <p>Among the many available vegetation indices, the Normalized Difference Vegetation Index (NDVI) is one that is most widely used [<xref ref-type="bibr" rid="B70-remotesensing-10-00641">70</xref>,<xref ref-type="bibr" rid="B71-remotesensing-10-00641">71</xref>,<xref ref-type="bibr" rid="B72-remotesensing-10-00641">72</xref>]. UAS-NDVI maps can be at least comparable to those obtained from satellite visible observations and become highly relevant for a timely assessment of crop health status, with capacity to provide immediate feedback to the farmer. NDVI surveys performed with UAS, aircraft, and satellite demonstrate that low-resolution images generally fail in representing intrafield variability and patterns in fields characterized by small vegetation gradients and high vegetation patchiness [<xref ref-type="bibr" rid="B7-remotesensing-10-00641">7</xref>]. Moreover, UAS-derived NDVIs have shown better agreement with ground-based NDVI observations compared to satellite-derived NDVIs in several crop and natural vegetation types [<xref ref-type="bibr" rid="B73-remotesensing-10-00641">73</xref>,<xref ref-type="bibr" rid="B74-remotesensing-10-00641">74</xref>,<xref ref-type="bibr" rid="B75-remotesensing-10-00641">75</xref>]. As an example of the achievable resolution that can be obtained from UAVs, relative to some available high-resolution commercial satellite sensors, <xref ref-type="fig" rid="remotesensing-10-00641-f003">Figure 3</xref> shows a multi-sensor sequence of imagery collected over a date palm plantation in Saudi Arabia. The observed differences between vegetation patterns and resolvable resolution observed by UAS (compared to available satellite platforms) are clearly identified. The relative advantages of UAS in providing a level of detail that is comparable to field observations (or in deriving NDVI or other related vegetation indices for more in depth assessment) is illustrated by its capability of capturing both within and between canopy behavior.</p>
        <p>In the last decade, particular attention has been given to the monitoring of vineyards because of their high economic value. Johnson et al. [<xref ref-type="bibr" rid="B76-remotesensing-10-00641">76</xref>] proposed one of the first applications where different sensors were used for determining measures related to chlorophyll function and photosynthetic activity, LAI, and plant health status (among other variables) to mapping vigor differences within fields. More recently, Zarco-Tejada et al. [<xref ref-type="bibr" rid="B52-remotesensing-10-00641">52</xref>,<xref ref-type="bibr" rid="B77-remotesensing-10-00641">77</xref>,<xref ref-type="bibr" rid="B78-remotesensing-10-00641">78</xref>,<xref ref-type="bibr" rid="B79-remotesensing-10-00641">79</xref>,<xref ref-type="bibr" rid="B80-remotesensing-10-00641">80</xref>] demonstrated the potential for monitoring specific variables such as crop water stress index, photosynthetic activity, and carotenoid content in vineyards using multispectral, hyperspectral, and thermal cameras.</p>
        <p>Based upon author experiences, farmers have expressed particular interest in monitoring crop conditions for the quantification of water demand, nitrogen status, or infestation treatments. Several of the variables or indices described above may be used for rapid detection of crop pest outbreaks or for mapping the status of crops. Likewise, monitoring soil water content is critical for determining efficient irrigation scheduling. The topsoil moisture content can be derived using RGB, NIR, and thermal bands [<xref ref-type="bibr" rid="B81-remotesensing-10-00641">81</xref>]. The effective amount of water stored in the subsurface can be obtained by exploiting mathematical relationships between surface measurements and the root zone soil moisture, such as the Soil Moisture Analytical Relationship (SMAR) [<xref ref-type="bibr" rid="B82-remotesensing-10-00641">82</xref>,<xref ref-type="bibr" rid="B83-remotesensing-10-00641">83</xref>].</p>
        <p>As a further example, Sullivan et al. [<xref ref-type="bibr" rid="B84-remotesensing-10-00641">84</xref>] observed that the thermal infrared (TIR) emittance was highly sensitive to canopy state and can be used for monitoring soil water content, stomatal conductance, and canopy cover. TIR has similarly been used for the monitoring and estimation of soil surface characteristics such as microrelief and rill morphology [<xref ref-type="bibr" rid="B85-remotesensing-10-00641">85</xref>], soil water repellency [<xref ref-type="bibr" rid="B86-remotesensing-10-00641">86</xref>], soil surface macropores [<xref ref-type="bibr" rid="B87-remotesensing-10-00641">87</xref>], skin surface soil permeability [<xref ref-type="bibr" rid="B88-remotesensing-10-00641">88</xref>], and overland and rill flow velocities by using thermal tracers [<xref ref-type="bibr" rid="B89-remotesensing-10-00641">89</xref>,<xref ref-type="bibr" rid="B90-remotesensing-10-00641">90</xref>].</p>
        <p>More specifically, the TIR emittance displays a negative correlation with stomatal conductance and canopy closure, indicating increasing canopy stress as stomatal conductance and canopy closure decreased. The crop water stress index (CWSI) [<xref ref-type="bibr" rid="B91-remotesensing-10-00641">91</xref>,<xref ref-type="bibr" rid="B92-remotesensing-10-00641">92</xref>] calculated from leaf water potential can be used to determine the required frequency, timing, and duration of watering. In this regard, the CWSI, derived with a UAS equipped with a thermal camera, is frequently adopted to quantify the physiological status of plants, and, more specifically, leaf water potential in experimental vineyards or orchards [<xref ref-type="bibr" rid="B52-remotesensing-10-00641">52</xref>,<xref ref-type="bibr" rid="B80-remotesensing-10-00641">80</xref>,<xref ref-type="bibr" rid="B93-remotesensing-10-00641">93</xref>,<xref ref-type="bibr" rid="B94-remotesensing-10-00641">94</xref>,<xref ref-type="bibr" rid="B95-remotesensing-10-00641">95</xref>,<xref ref-type="bibr" rid="B96-remotesensing-10-00641">96</xref>]. The derived CWSI maps can serve as important inputs for precision irrigation. Time series of thermal images can also be used to determine the variation in water status [<xref ref-type="bibr" rid="B97-remotesensing-10-00641">97</xref>].</p>
        <p>Using VIS&#x2013;NIR (400&#x2013;1000 nm) hyperspectral and multispectral analyses of simulated data has shown that soil attributes can be extracted from these spectral regions, particularly those most commonly used by the current UAS platforms [<xref ref-type="bibr" rid="B98-remotesensing-10-00641">98</xref>,<xref ref-type="bibr" rid="B99-remotesensing-10-00641">99</xref>,<xref ref-type="bibr" rid="B100-remotesensing-10-00641">100</xref>]. These studies demonstrated that the VIS-NIR spectral region alone can open up new frontiers in soil mapping (as well as soil moisture content retrieval) using on-board multi- and hyperspectral UAS sensors without using heavyweight sensors operating in the SWIR (1&#x2013;2.5 &#x3BC;m) region. Aldana-Jague et al. [<xref ref-type="bibr" rid="B32-remotesensing-10-00641">32</xref>] mapped soil surface organic carbon content (&lt;0.5 cm) at 12 cm resolution exploiting six bands between 450 and 1050 nm acquired by low-altitude multispectral imagers. D&#x2019;Oleire-Oltmanns et al. [<xref ref-type="bibr" rid="B30-remotesensing-10-00641">30</xref>] showed the applicability of UAS for measuring, mapping, and monitoring soil erosion at 5 cm resolution with an accuracy between 0.90 and 2.7 cm in the horizontal and 0.70 cm in the vertical directions. Detailed information about soil erosion can enhance proper soil management at the plot scale [<xref ref-type="bibr" rid="B31-remotesensing-10-00641">31</xref>].</p>
        <p>Such tools were further explored by Zhu et al. [<xref ref-type="bibr" rid="B22-remotesensing-10-00641">22</xref>], who investigated the ability to quantify the differences in soil nitrogen application rates using digital images taken from a UAS compared with ground-based hyperspectral reflectance and chlorophyll content data. They suggested that aerial photography from a UAS has the potential to provide input in support of crop decision-making processes, minimizing field sampling efforts, saving both time and money, and enabling accurate assessment of different nitrogen application rates. Therefore, such information may serve as input to other agricultural systems, such as tractors or specific UAS, that optimize fertilizer management.</p>
        <p>UAS can also improve agronomical practices. Costa et al. [<xref ref-type="bibr" rid="B101-remotesensing-10-00641">101</xref>] described an architecture that can be employed to implement a control loop for agricultural applications where UAS are responsible for spraying chemicals on crops. Application of chemicals is controlled by the feedback obtained from a wireless sensor network (WSN) deployed on the crop field. They evaluated an algorithm to adjust the UAS route under changes in wind (intensity and direction) to minimize the waste of pesticides. Pe&#xF1;a et al. [<xref ref-type="bibr" rid="B102-remotesensing-10-00641">102</xref>,<xref ref-type="bibr" rid="B103-remotesensing-10-00641">103</xref>] explored the optimization of herbicide applications in weed&#x2013;crop systems using a series of UAS multispectral images. The authors computed multiple data, which permitted both calculation of herbicide requirements and estimation of the overall cost of weed management operations in advance. They showed that the ability to discriminate weeds was significantly affected by the imagery spectra (type of camera) used as well as the spatial (flight altitude) and temporal (the date of the study) resolutions.</p>
        <p>Among these technical advantages and constraints, the importance of the limitation of operational rules in using UAS in several countries needs to be highlighted. As an example, Jeunnette and Hart [<xref ref-type="bibr" rid="B24-remotesensing-10-00641">24</xref>] developed a parametric numerical model to compare aerial platform options (UAS vs airborne) to support agriculture in developing countries characterized by highly fragmented fields, but manned systems are still more competitive from an operational and cost/efficiency point of view because of the present limitations in altitude, distance, and speed of UAS. In particular, UAS become cost-competitive when they are allowed to fly higher than 300 m AGL (above ground level), while current limits are set around 120&#x2013;150 m. This is a critical limitation for the use of UAS along with the fact that flights should be within visible line of sight (VLOS) in many jurisdictions.</p>
        <p>All the applications described highlight the potential use of UAS in developing advanced tools for precision agriculture applications and for vegetation monitoring in general. With time, both technological advances and legislation will evolve and likely converge, further advancing the efficient use of such technologies.</p>
      </sec>
      <sec id="sec3dot2-remotesensing-10-00641">
        <title>3.2. Monitoring of Natural Ecosystems</title>
        <p>As with agricultural ecosystems, the proliferation of UAS-based remote sensing techniques has opened up new opportunities for monitoring and managing natural ecosystems [<xref ref-type="bibr" rid="B12-remotesensing-10-00641">12</xref>,<xref ref-type="bibr" rid="B104-remotesensing-10-00641">104</xref>,<xref ref-type="bibr" rid="B105-remotesensing-10-00641">105</xref>,<xref ref-type="bibr" rid="B106-remotesensing-10-00641">106</xref>]. In fact, UAS provides options and opportunities to collect data at appropriate spatial and temporal resolutions to describe ecological processes and allow better surveying of natural ecosystems placed in remote, inaccessible, or difficult- and/or dangerous-to-access sites. As examples, some habitats (e.g., peat bogs) can be damaged through on-ground surveys, while UAS positioned several meters above the surface can provide a near-comparable level of information as that obtained through plot-based measurements (e.g., canopy cover by species). UAS are also useful for undertaking rapid surveys of habitats such as mangroves, where access is often difficult and plot-based surveys take far longer to complete (see <xref ref-type="fig" rid="remotesensing-10-00641-f004">Figure 4</xref>). </p>
        <p>UAS therefore offer the potential to overcome these limitations and have been applied to monitor a disparate range of habitats and locations, including tropical forests, riparian forests, dryland ecosystems, boreal forests, and peatlands. Pioneering researchers have been using UAS to monitor attributes such as plant population [<xref ref-type="bibr" rid="B107-remotesensing-10-00641">107</xref>,<xref ref-type="bibr" rid="B108-remotesensing-10-00641">108</xref>]; biodiversity and species richness [<xref ref-type="bibr" rid="B109-remotesensing-10-00641">109</xref>,<xref ref-type="bibr" rid="B110-remotesensing-10-00641">110</xref>]; plant species invasion [<xref ref-type="bibr" rid="B111-remotesensing-10-00641">111</xref>]; restoration ecology [<xref ref-type="bibr" rid="B112-remotesensing-10-00641">112</xref>]; disturbances [<xref ref-type="bibr" rid="B113-remotesensing-10-00641">113</xref>]; phenology [<xref ref-type="bibr" rid="B114-remotesensing-10-00641">114</xref>]; pest infestation in forests [<xref ref-type="bibr" rid="B115-remotesensing-10-00641">115</xref>,<xref ref-type="bibr" rid="B116-remotesensing-10-00641">116</xref>]; and land cover change [<xref ref-type="bibr" rid="B117-remotesensing-10-00641">117</xref>].</p>
        <p>Many studies have focused on the retrieval of vegetation structural information to support forest assessment and management [<xref ref-type="bibr" rid="B118-remotesensing-10-00641">118</xref>,<xref ref-type="bibr" rid="B119-remotesensing-10-00641">119</xref>]. For instance, information on the plant and canopy height can be obtained from stereo images [<xref ref-type="bibr" rid="B120-remotesensing-10-00641">120</xref>,<xref ref-type="bibr" rid="B121-remotesensing-10-00641">121</xref>], which can be used to estimate above-ground biomass (see for example <xref ref-type="fig" rid="remotesensing-10-00641-f004">Figure 4</xref>). 3D maps of canopy can also be used to distinguish between trunks, branches, and foliage [<xref ref-type="bibr" rid="B121-remotesensing-10-00641">121</xref>].</p>
        <p>UAS represent a promising option enabling timely, fast, and precise monitoring that is important for many plant species, particularly those that are invasive [<xref ref-type="bibr" rid="B122-remotesensing-10-00641">122</xref>,<xref ref-type="bibr" rid="B123-remotesensing-10-00641">123</xref>,<xref ref-type="bibr" rid="B124-remotesensing-10-00641">124</xref>]. Flexibility of the data acquisition enabled by the UAS is very important, since plants are often more distinct from the surrounding vegetation in certain times of their growing season [<xref ref-type="bibr" rid="B125-remotesensing-10-00641">125</xref>]. Besides rapid monitoring of newly invaded areas, the UAS methodology enables prediction/modelling of invasion spread that can be driven by a combination of factors, such as habitat and species characteristics, human dispersal, and disturbances [<xref ref-type="bibr" rid="B126-remotesensing-10-00641">126</xref>]. Legal constraints limiting the use of UAS to unpopulated areas can be especially problematic for monitoring invasive species that tend to prefer urban areas. Still, the UAS technology can greatly reduce costs of extensive field campaigns and eradication measures [<xref ref-type="bibr" rid="B127-remotesensing-10-00641">127</xref>].</p>
        <p>UAS are also revolutionizing the management of quasi-natural ecosystems, such as restored habitats and managed forests. They have been used to quantify spatial gap patterns in forests in order to support the planning of common forest management practices such as thinning [<xref ref-type="bibr" rid="B128-remotesensing-10-00641">128</xref>] or to support restoration monitoring. For example, Quilter et al. [<xref ref-type="bibr" rid="B129-remotesensing-10-00641">129</xref>] used UAS for monitoring streams and riparian restoration projects in inaccessible areas on Chalk Creek (Utah). Knoth et al. [<xref ref-type="bibr" rid="B130-remotesensing-10-00641">130</xref>] applied a UAS-based NIR remote sensing approach to monitor a restored cut-over bog and Ludovisi et al. [<xref ref-type="bibr" rid="B21-remotesensing-10-00641">21</xref>] also used TIR data to determine the response of forest to drought in relation to forest tree breeding programs and genetic improvement.</p>
      </sec>
    </sec>
    <sec id="sec4-remotesensing-10-00641">
      <title>4. River Systems and Floods</title>
      <p>Satellite data are widely used to monitor natural hazards (e.g., floods, earthquakes, volcanic eruptions, wildfire, etc.) at national and international scales [<xref ref-type="bibr" rid="B131-remotesensing-10-00641">131</xref>]. This popularity is due to their wide coverage, spectral resolution, safety, and rate of update [<xref ref-type="bibr" rid="B132-remotesensing-10-00641">132</xref>,<xref ref-type="bibr" rid="B133-remotesensing-10-00641">133</xref>]. Nevertheless, UAS have also been adopted for rapid assessment following natural extreme events and in the context of humanitarian relief and infrastructure assessment [<xref ref-type="bibr" rid="B28-remotesensing-10-00641">28</xref>]. According to Quaritsch et al. [<xref ref-type="bibr" rid="B134-remotesensing-10-00641">134</xref>], UAS should be utilized as a component of a network of sensors for natural disaster management. Although there are a number of technological barriers, which must be overcome before UAS can be utilized in a more automated and coordinated manner, their potential for disaster response is significant [<xref ref-type="bibr" rid="B135-remotesensing-10-00641">135</xref>]. Given the UAS potential, we expect significant advances in the fields of hydrology, geomorphology, and hydraulics, where there is a significant opportunity for the use of UAS for monitoring river systems, overland flows, or even urban floods.</p>
      <sec>
        <title>Flow Monitoring</title>
        <p>River systems and stream flows can be monitored by remotely integrating the techniques of water body observation, vegetation mapping, DEM generation, and hydrological modelling. Satellite sensors in the visible, infrared, and microwave ranges are currently used to monitor rivers and to delineate flood zones [<xref ref-type="bibr" rid="B136-remotesensing-10-00641">136</xref>,<xref ref-type="bibr" rid="B137-remotesensing-10-00641">137</xref>,<xref ref-type="bibr" rid="B138-remotesensing-10-00641">138</xref>]. These methods are generally used only over large rivers or areas of inundation in order to detect changes at the pixel level. UAS can describe river dynamics, but with a level of detail that is several orders of magnitude greater and can enable distributed flow measurements over any river system and in difficult-to-access environments.</p>
        <p>In this context, the integration of UAS imagery and optical velocimetry techniques has enabled full remote kinematic characterization of water bodies and surface flows. Optical techniques, such as Large-Scale Particle Image Velocimetry (LSPIV, [<xref ref-type="bibr" rid="B139-remotesensing-10-00641">139</xref>]) and Particle Tracking Velocimetry (PTV [<xref ref-type="bibr" rid="B140-remotesensing-10-00641">140</xref>]), are efficient yet nonintrusive flow visualization methods that yield a spatially distributed estimation of the surface flow velocity field based on the similarity of image sequences. Proof-of-concept experiments have demonstrated the feasibility of applying LSPIV from manned aerial systems to monitor flood events [<xref ref-type="bibr" rid="B141-remotesensing-10-00641">141</xref>,<xref ref-type="bibr" rid="B142-remotesensing-10-00641">142</xref>]. More recently, videos recorded from a UAS have been analyzed with LSPIV to reconstruct surface flow velocity fields of natural stream reaches [<xref ref-type="bibr" rid="B143-remotesensing-10-00641">143</xref>,<xref ref-type="bibr" rid="B144-remotesensing-10-00641">144</xref>]. This provides a detailed Lagrangian insight into river dynamics that is valuable in calibrating numerical models. </p>
        <p>Most of these experimental observations entail a low-cost UAS hovering above the region of interest for a few seconds (the observation time should be adjusted to the flow velocity and camera acquisition frequency). An RGB camera is typically mounted onboard and installed with its optical axis perpendicular to the captured field of view to circumvent orthorectification [<xref ref-type="bibr" rid="B145-remotesensing-10-00641">145</xref>]. To facilitate remote photometric calibration, Tauro et al. [<xref ref-type="bibr" rid="B145-remotesensing-10-00641">145</xref>] adopted a UAS equipped with a system of four lasers that focus points at known distances in the field of view. In several experimental settings, the accuracy of surface flow velocity estimations from UAS was found to be comparable to (or even better than) that of traditional ground-based LSPIV configurations [<xref ref-type="bibr" rid="B146-remotesensing-10-00641">146</xref>]. In fact, compared to fixed implementations, UAS enable capture of larger fields of view with a diffuse rather than direct illumination. Such optical image velocimetry techniques can measure flow velocity fields over extended regions rather than pointwise, and at temporal resolutions comparable to or even better than Acoustic Doppler Velocimetry (ADV) based on the presence of detectable features on the water surface [<xref ref-type="bibr" rid="B147-remotesensing-10-00641">147</xref>].</p>
        <p>In this context, UAS technology is expected to considerably aid in flood monitoring and mapping. In fact, flood observation is a considerable challenge for space-borne passive imagery, mostly due to the presence of dense cloud cover, closed vegetation canopies, and the satellite revisit time and viewing angle [<xref ref-type="bibr" rid="B133-remotesensing-10-00641">133</xref>,<xref ref-type="bibr" rid="B148-remotesensing-10-00641">148</xref>]. Although SAR satellite sensors (e.g., Sentinel-1, TerraSAR-X, RADARSAT-2) can overcome these visibility limitations, they are unable to provide the submeter-level spatial resolution necessary for detailed understanding of flood routing and susceptibility. Applying UAS with an appropriate flight mode may overcome some of these issues, allowing for rapid and safe monitoring of inundations and measurement of flood hydrological parameters [<xref ref-type="bibr" rid="B149-remotesensing-10-00641">149</xref>]. This is possible also because most platforms are quite stable in windy conditions (less than 5 m/s in the case of multirotors).</p>
        <p>Challenges for the widespread adoption and incorporation of UAS for flow monitoring have commonalities with both agricultural and ecosystems monitoring, including the coupling of measurements from multiple sensors through accurate and efficient processing workflows. Specific to streamflow measurement, these include (i) optimization of SfM workflows to enable extraction of terrestrial and subsurface topographies through accurate image registration using automatic or direct georeferencing techniques; (ii) the determination of water levels through image- (e.g., SfM; [<xref ref-type="bibr" rid="B150-remotesensing-10-00641">150</xref>]), sensor- (e.g., laser, radar; [<xref ref-type="bibr" rid="B151-remotesensing-10-00641">151</xref>]), and turbulence-derived metrics [<xref ref-type="bibr" rid="B152-remotesensing-10-00641">152</xref>]; and (iii) the derivation of flow velocities through appropriate techniques (e.g., PIV/PTV), based on the characteristics of flow, duration of observation, seeding density, etc.). The task of combining these data and developing workflows that are capable of rapidly producing synoptic river flow measurements based on the range of available inputs is an ongoing challenge to ensure UAS-based measurements are able to fully support water resource management and civil protection agencies.</p>
        <p>In this context, hyperspectral sensors can also be used to extend the range of water monitoring applications. Potential examples include sediment concentration, chlorophyll distribution, blooming algae status, submerged vegetation mapping, bathymetry, and chemical and organic waste contaminations [<xref ref-type="bibr" rid="B153-remotesensing-10-00641">153</xref>,<xref ref-type="bibr" rid="B154-remotesensing-10-00641">154</xref>].</p>
      </sec>
    </sec>
    <sec id="sec5-remotesensing-10-00641">
      <title>5. Final Remarks and Challenges</title>
      <p>UAS-based remote sensing provides new advanced procedures to monitor key variables, including vegetation status, soil moisture content, and stream flow. A detailed description of such variables will increase our capacity to describe water resource availability and assist agricultural and ecosystem management. Our manuscript provides an overview of some of the recent applications in field-based UAS surficial environmental monitoring. The wide range of applications testifies to the great potential of these techniques, but, at the same time, the variety of methodologies adopted is evidence that there is still need for harmonization efforts. The variety of available platforms, sensors, and specificity of any particular case study have stimulated a proliferation of a huge number of specific algorithms addressing flight planning, image registration, calibration and correction, and derivation of indices or variables. However, there is no evidence of comprehensive comparative studies that enable the user to select the most appropriate procedure for any specific need.</p>
      <p>A review of the literature carried out herein allowed the identification of a number of outstanding issues in the use of UAS for environmental monitoring. Among others, we selected the following that require specific attention:<list list-type="roman-lower"><list-item><label>(i)</label><p>While a direct comparison between different methodologies (UAS, manned airborne, and satellite) is challenging, it was found that UAS systems represent a cost-effective monitoring technique over small regions (&lt;20 ha). For larger extents, manned airborne or satellite platforms may become more effective options, but only when the temporal advantage of the UAS is not considered.</p></list-item><list-item><label>(ii)</label><p>The limited extent of the studied areas reduces the relative budget available, increasing the fragmentation of the adopted procedures and methodologies.</p></list-item><list-item><label>(iii)</label><p>Government regulations restricting the Ground Sample Distance (GSD) and the UAS flight mode are limiting the economic advantages related to their use and some potential applications, particularly in urban environments.</p></list-item><list-item><label>(iv)</label><p>The wide range of experiences described highlighted the huge variability in the strategies, methodologies, and sensors adopted for each specific environmental variable monitored. This identifies the need to find unifying principles in UAS-based studies.</p></list-item><list-item><label>(v)</label><p>Vulnerability of UAS to weather conditions (e.g., wind, rain) can alter quality of the surveys.</p></list-item><list-item><label>(vi)</label><p>There are also technical limits, such as weather constraints (strong wind and/or rain), high elevations, or high-temperature environments that can be challenging for most of the devices/sensors and respective UAS operators (see, e.g., [<xref ref-type="bibr" rid="B155-remotesensing-10-00641">155</xref>]).</p></list-item><list-item><label>(vii)</label><p>The geometric and radiometric limitations of current lightweight sensors make the use of this technology challenging.</p></list-item><list-item><label>(viii)</label><p>The high spatial resolution of UAS data generates high demand on data storage and processing capacity.</p></list-item><list-item><label>(ix)</label><p>There is a clear need for procedures to characterize and correct the sensor errors that can propagate in the subsequent mosaicking and related data processing.</p></list-item><list-item><label>(x)</label><p>Finally, a disadvantage in the use of UAS is represented by the complexity associated to their use that is comparable to that of satellites. In fact, satellite applications are generally associated to a chain of processing assuring the final quality of data. In the case of UAS, all this is left to the final user or researcher, requiring additional steps in order to be able to use the retrieved data.</p></list-item></list></p>
      <p>It should be recognized that the UAS sector has received much less funding to address the existing gaps in technology and processing chains needed to produce useable images than, for instance, satellite-based programs (<xref ref-type="fig" rid="remotesensing-10-00641-f005">Figure 5</xref>). However, this is one of the reasons why there is much potential for further improvements in the technology and its use. One particular benefit of UAV improvements is related to the potential benefit that could be directed towards satellite-based observations, which can leverage the utilization of highly detailed UAS data. Given the spatiotemporal advantage of UAS systems, they can provide much higher return periods, offering several flights per day to study very dynamic processes at high spatial resolution, such as physiological response of vegetation to heat or even rapid flooding events. The combination of these data allows an advanced satellite test-bed for examining scale effects due to spatial resolution, identifying the most suitable acquisition time, establishing the effects of temporal resolution, incorporating suitable spectral bands, and establishing needed radiometric resolution, etc., all of which provide feedback to developing improved space-borne platforms in a way that ground-based monitoring alone can never replicate. Moreover, the capability to achieve a resolution comparable with the scale of field measurements gives the opportunity to address the issue of within-pixel spatial heterogeneity observed by satellites.</p>
      <p>With time, natural selection will likely deliver the most efficient collection and processing solutions for different contexts and applications, but there is still a significant amount of work needed to drive this change. Therefore, a major challenge for the scientific community is to foster this process by providing some guidance in the wide range of possibilities offered by the market. On the other side of this, the private sector of UAS developers are also investing in this field, accelerating the evolution of the technology. Among the many advances, it is interesting to mention the following:<list list-type="bullet"><list-item><p>One of the aspects directly impacting the area that is able to be sensed is the limited flight times of UAS. This problem is currently managed by mission planning that enables management of multiple flights. Technology is also offering new solutions that will extend the flight endurance up to several hours, making the use of UAS more competitive. For instance, new developments in batteries suggest that the relatively short flying time imposed by current capacity will be significantly improved in the future [<xref ref-type="bibr" rid="B156-remotesensing-10-00641">156</xref>]. In this context, another innovation introduced in the most recent vehicles is an integrated energy supply system connected with onboard solar panels that allow flight endurance to be extended from 40&#x2013;50 min up to 5 h, depending on the platform.</p></list-item><list-item><p>The relative ground sampling distance affects the quality of the surveys, but is often not compensated for. This limitation can now be solved by implementing 3D flight paths that follow the surface in order to maintain a uniform GSD. Currently, only a few software suites (e.g., UgCS, eMotion 3) use digital terrain models to adjust the height path of the mission in order to maintain consistent GSD.</p></list-item><list-item><p>The influence of GSD may be reduced by increasing flight height, making UAS even more cost-competitive (by increasing sensed areas), but current legislation in many jurisdictions limits this to between 120 and 150 m and to within visible line of sight (VLOS). In this context, the development of microdrones will significantly reduce risk associated with their use, and relax some of the constraints due to safety requirements.</p></list-item><list-item><p>Recent and rapid developments in sensor miniaturization, standardization, and cost reduction have opened new possibilities for UAS applications. However, limits remain, especially for commercial readymade platforms that are used the most among the scientific community.</p></list-item><list-item><p>Sensor calibration remains an issue, especially for hyperspectral sensors. For example, vegetation can be measured in its state and distribution using RGB, multispectral, hyperspectral, and thermal cameras, as well as with LiDAR.</p></list-item><list-item><p>Image registration, correction, and calibration remain major challenges. The vulnerability of UAS to weather conditions (wind, rain) and the geometric and radiometric limitations of current lightweight sensors have stimulated the development of new algorithms for image mosaicking and correction. In this context, the development of open source and commercial SfM software allows image mosaicking to be addressed, but radiometric correction and calibration is still an open question that may find a potential solution through experience with EO. Moreover, the development of new mapping-quality cameras has already significantly improved spatial registration and will likely help to also improve the overall quality of the UAS imagery.</p></list-item></list></p>
      <p>Technological advances are strongly supporting the diffusion of these technologies over a wide range of fields, including hydrology. On the other hand, the research community must address significant challenges in standardizing the methodologies adopted. All environmental variables (e.g., vegetation, soil moisture, and river flow) can be measured using different sensors and algorithms, but a comprehensive assessment of the performances of each of these methods and procedures is required. Such efforts may help to improve our capacity in describing the spatiotemporal processes at both the field and the river basin scale. Moreover, UAS technology can be easily integrated with other devices and tools (cell phone, fixed installations, etc.), allowing advances in agricultural practices and hydrometeorological monitoring. </p>
      <p>Technology and scientific research have a clear path to follow that has already (largely) been traced by manned aerial photogrammetry and Earth observation from satellites. In fact, current observational practices have already addressed several of the problems that UAS-based observations are facing (e.g., image mosaicking, sensor calibration, radiometric calibration, etc.). Nevertheless, the ensemble of such problems connected to the proper use of UAS introduces a data processing complexity that is comparable to or slightly larger than that of satellites (see <xref ref-type="fig" rid="remotesensing-10-00641-f005">Figure 5</xref>) and makes their use difficult even for an experienced scientist without clear guidance.</p>
      <p>There is a growing need to define harmonized approaches able to channel the efforts of all these studies and identify the optimal strategy for UAS-based monitoring. The challenge for the research is to define a clear and referenced workflow starting from the planning and acquisition of the data to the generation and interpretation of maps. In particular, we envisage the need to stimulate a comparative experiment able to assess the reliability of different procedures and a combination of algorithms in order to identify the most appropriate methodology for environmental monitoring in different hydroclimatic conditions. The definition of clear and specific procedures may also help in the definition of new legislation at the European scale, removing some of the actual restrictions that limit the potential use of UAS in a wider range of contexts.</p>
      <p>Ultimately, it will be the integration of UAS platforms with different techniques, including traditional instruments, fixed and mobile camera surveys, satellite observations, and geomorphological analyses, that will almost certainly deliver an improved characterization of Earth and environmental systems. Apart from providing improved spatial and temporal coverage, such a strategy of integrated observation will inevitably improve our knowledge of agricultural, hydraulic, geomorphological, ecological, and hydrological dynamics, and provide a basis for advancing our understanding of process description and behavior across space and time scales. </p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      <p>The present work has been funded by the COST Action CA16219 &#x201C;HARMONIOUS&#x2014;Harmonization of UAS techniques for agricultural and natural ecosystems monitoring&#x201D;. B. T&#xF3;th acknowledges financial support by the Hungarian National Research, Development and Innovation Office (NRDI) under grant KH124765. J. M&#xFC;llerov&#xE1; was supported by projects GA17-13998S and RVO67985939. Isabel and Jo&#xE3;o de Lima were supported by project HIRT (PTDC/ECM-HID/4259/2014&#x2014;POCI-01-0145-FEDER016668). We would like to thank reviewers for their insightful comments on the paper, as these comments led us to an improvement of the work.</p>
    </ack>
    <notes>
      <title>Author Contributions</title>
      <p>S.M. conceived and coordinated the review work and the writing; M.F.M., R.L., and L.E. provided a guidance in the interpretation of the results and in the writing; P.E.M., V.P.M., G.M., and E.B.D. supported the analysis and interpretation of the results referring to <xref ref-type="sec" rid="sec2-remotesensing-10-00641">Section 2</xref>; J.M., A.M., D. H., G. R.-P., G.C., J.L.M.P.d.L., K.C., Z.S., G.V., B.T. and F.F. supported the analysis and interpretation of the results referring to <xref ref-type="sec" rid="sec3-remotesensing-10-00641">Section 3</xref>; F.T., M.P., M.I.d.L., and M.K. supported the analysis and interpretation of the results referring to <xref ref-type="sec" rid="sec4-remotesensing-10-00641">Section 4</xref>.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <app-group>
      <app id="app1-remotesensing-10-00641">
        <title>Appendix A. Available Sensors and Cameras</title>
        <p>Given the variety of sensors available for UAS applications, we consider it extremely useful to provide an overview of the available cameras and sensors and their characteristics. In the following, we summarize some of the most common optical cameras (<xref ref-type="table" rid="remotesensing-10-00641-t0A1">Table A1</xref>), multispectral cameras (<xref ref-type="table" rid="remotesensing-10-00641-t0A2">Table A2</xref>), hyperspectral cameras (<xref ref-type="table" rid="remotesensing-10-00641-t0A3">Table A3</xref>), thermal cameras (<xref ref-type="table" rid="remotesensing-10-00641-t0A4">Table A4</xref>), and laser scanners (<xref ref-type="table" rid="remotesensing-10-00641-t0A5">Table A5</xref>). The ideal solution always depends on the (a) purpose of the study, (b) unmanned platform deployed, and (c) budget available. These tables expand the list of sensors provided by Casagrande et al. (2017).</p>
        <table-wrap id="remotesensing-10-00641-t0A1" position="float">
          <object-id pub-id-type="pii">remotesensing-10-00641-t0A1_Table A1</object-id>
          <label>Table A1</label>
          <caption>
            <p>Optical cameras available for UAS and their main characteristics (* Abbreviations: APS = Advanced Photo System type-C; FF = Full Frame; MILC = Mirrorless Interchangeable-Lens Camera; SF = Small Frame).</p>
          </caption>
          <table>
            <thead>
              <tr>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Manufacturer and Model</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Sensor Type Resolution <break/>(MPx)</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">FormatType *</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Sensor Size <break/>(mm<sup>2</sup>)</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Pixel Pitch (&#x3BC;m)</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Weight <break/>(kg)</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Frame Rate <break/>(fps)</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Max Shutter Speed <break/>(s<sup>&#x2212;1</sup>)</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Approx. Price <break/>($)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="center" valign="middle">Canon EOS 5DS</td>
                <td align="center" valign="middle">CMOS 51</td>
                <td align="center" valign="middle">FF</td>
                <td align="center" valign="middle">36.0 &#xD7; 24.0</td>
                <td align="center" valign="middle">4.1</td>
                <td align="center" valign="middle">0.930</td>
                <td align="center" valign="middle">5.0</td>
                <td align="center" valign="middle">8000</td>
                <td align="center" valign="middle">3400</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Sony Alpha 7R II</td>
                <td align="center" valign="middle">CMOS 42</td>
                <td align="center" valign="middle">FF MILC</td>
                <td align="center" valign="middle">35.9 &#xD7; 24.0</td>
                <td align="center" valign="middle">4.5</td>
                <td align="center" valign="middle">0.625</td>
                <td align="center" valign="middle">5.0</td>
                <td align="center" valign="middle">8000</td>
                <td align="center" valign="middle">3200</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Pentax 645D</td>
                <td align="center" valign="middle">CCD 40</td>
                <td align="center" valign="middle">FF</td>
                <td align="center" valign="middle">44.0 &#xD7; 33.0</td>
                <td align="center" valign="middle">6.1</td>
                <td align="center" valign="middle">1.480</td>
                <td align="center" valign="middle">1.1</td>
                <td align="center" valign="middle">4000</td>
                <td align="center" valign="middle">3400</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Nikon D750</td>
                <td align="center" valign="middle">CMOS 24</td>
                <td align="center" valign="middle">FF</td>
                <td align="center" valign="middle">35.9 &#xD7; 24.0</td>
                <td align="center" valign="middle">6.0</td>
                <td align="center" valign="middle">0.750</td>
                <td align="center" valign="middle">6.5</td>
                <td align="center" valign="middle">4000</td>
                <td align="center" valign="middle">2000</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Nikon D7200</td>
                <td align="center" valign="middle">CMOS 24</td>
                <td align="center" valign="middle">SF</td>
                <td align="center" valign="middle">23.5 &#xD7; 15.6</td>
                <td align="center" valign="middle">3.9</td>
                <td align="center" valign="middle">0.675</td>
                <td align="center" valign="middle">6.0</td>
                <td align="center" valign="middle">8000</td>
                <td align="center" valign="middle">1100</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Sony Alpha a6300</td>
                <td align="center" valign="middle">CMOS 24</td>
                <td align="center" valign="middle">SF MILC</td>
                <td align="center" valign="middle">23.5 &#xD7; 15.6</td>
                <td align="center" valign="middle">3.9</td>
                <td align="center" valign="middle">0.404</td>
                <td align="center" valign="middle">11.0</td>
                <td align="center" valign="middle">4000</td>
                <td align="center" valign="middle">1000</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Pentax K-3 II</td>
                <td align="center" valign="middle">CMOS 24</td>
                <td align="center" valign="middle">SF</td>
                <td align="center" valign="middle">23.5 &#xD7; 15.6</td>
                <td align="center" valign="middle">3.9</td>
                <td align="center" valign="middle">0.800</td>
                <td align="center" valign="middle">8.3</td>
                <td align="center" valign="middle">8000</td>
                <td align="center" valign="middle">800</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Foxtech Map-01</td>
                <td align="center" valign="middle">CMOS 24</td>
                <td align="center" valign="middle">APS-C</td>
                <td align="center" valign="middle">23.5 &#xD7; 15.6</td>
                <td align="center" valign="middle">3.9</td>
                <td align="center" valign="middle">0.155</td>
                <td align="center" valign="middle">6</td>
                <td align="center" valign="middle">4000</td>
                <td align="center" valign="middle">880</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Canon EOS 7D Mark II</td>
                <td align="center" valign="middle">CMOS 20</td>
                <td align="center" valign="middle">SF</td>
                <td align="center" valign="middle">22.3 &#xD7; 14.9</td>
                <td align="center" valign="middle">4.1</td>
                <td align="center" valign="middle">0.910</td>
                <td align="center" valign="middle">10.0</td>
                <td align="center" valign="middle">8000</td>
                <td align="center" valign="middle">1500</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Panasonic Lumix DMC GX8</td>
                <td align="center" valign="middle">CMOS 20</td>
                <td align="center" valign="middle">SF MILC</td>
                <td align="center" valign="middle">17.3 &#xD7; 13.0</td>
                <td align="center" valign="middle">3.3</td>
                <td align="center" valign="middle">0.487</td>
                <td align="center" valign="middle">10.0</td>
                <td align="center" valign="middle">8000</td>
                <td align="center" valign="middle">1000</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Sony QX1</td>
                <td align="center" valign="middle">CMOS 20</td>
                <td align="center" valign="middle">APS-C</td>
                <td align="center" valign="middle">23.2 &#xD7; 15.4</td>
                <td align="center" valign="middle">4.3</td>
                <td align="center" valign="middle">0.216</td>
                <td align="center" valign="middle">3.5</td>
                <td align="center" valign="middle">4000</td>
                <td align="center" valign="middle">500</td>
              </tr>
              <tr>
                <td align="center" valign="middle" style="border-bottom:solid thin">Ricoh GXR A16</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">CMOS 16</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">SF</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">23.6 &#xD7; 15.7</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">4.8</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">0.550</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">2.5</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">3200</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">650</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <table-wrap id="remotesensing-10-00641-t0A2" position="float">
          <object-id pub-id-type="pii">remotesensing-10-00641-t0A2_Table A2</object-id>
          <label>Table A2</label>
          <caption>
            <p>Multispectral cameras available on the market for UAS and their main characteristics (* selectable bands).</p>
          </caption>
          <table>
            <thead>
              <tr>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Manufacturer and Model</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Resolution (Mpx)</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Size (mm)</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Pixel Size (&#x3BC;m)</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Weight (kg)</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Number of Spectral Bands</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Spectral Range (nm)</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Approx. Price ($)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="center" valign="middle">Tetracam MCAW6 (Global shutter)</td>
                <td align="center" valign="middle">1.3</td>
                <td align="center" valign="middle">-</td>
                <td align="center" valign="middle">4.8 &#xD7; 4.8</td>
                <td align="center" valign="middle">0.55</td>
                <td align="center" valign="middle">6</td>
                <td align="center" valign="middle">450&#x2013;1000 (*)</td>
                <td align="center" valign="middle">16,995</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Tetracam MCAW12 (Global shutter)</td>
                <td align="center" valign="middle">1.3</td>
                <td align="center" valign="middle">-</td>
                <td align="center" valign="middle">4.8 &#xD7; 4.8</td>
                <td align="center" valign="middle">0.6</td>
                <td align="center" valign="middle">12</td>
                <td align="center" valign="middle">450&#x2013;1000 (*)</td>
                <td align="center" valign="middle">34,000</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Tetracam MicroMCA4 Snap (Global shutter)</td>
                <td align="center" valign="middle">1.3</td>
                <td align="center" valign="middle">115.6 &#xD7; 80.3 &#xD7; 68.1</td>
                <td align="center" valign="middle">4.8 &#xD7; 4.8</td>
                <td align="center" valign="middle">0.497</td>
                <td align="center" valign="middle">4</td>
                <td align="center" valign="middle">450&#x2013;1000 (*)</td>
                <td align="center" valign="middle">9995</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Tetracam MicroMCA6 Snap (Global shutter)</td>
                <td align="center" valign="middle">1.3</td>
                <td align="center" valign="middle">115.6 &#xD7; 80.3 &#xD7; 68.1</td>
                <td align="center" valign="middle">4.8 &#xD7; 4.8</td>
                <td align="center" valign="middle">0.53</td>
                <td align="center" valign="middle">6</td>
                <td align="center" valign="middle">450&#x2013;1000 (*)</td>
                <td align="center" valign="middle">14,995</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Tetracam MicroMCA12 Snap (Global shutter)</td>
                <td align="center" valign="middle">1.3</td>
                <td align="center" valign="middle">115.6 &#xD7; 155 &#xD7; 68.1</td>
                <td align="center" valign="middle">4.8 &#xD7; 4.8</td>
                <td align="center" valign="middle">1</td>
                <td align="center" valign="middle">12</td>
                <td align="center" valign="middle">450&#x2013;1000 (*)</td>
                <td align="center" valign="middle">29,995</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Tetracam MicroMCA6 RS (Rolling shutter)</td>
                <td align="center" valign="middle">1.3</td>
                <td align="center" valign="middle">115.6 &#xD7; 80.3 &#xD7; 68.1</td>
                <td align="center" valign="middle">4.8 &#xD7; 4.8</td>
                <td align="center" valign="middle">0.53</td>
                <td align="center" valign="middle">6</td>
                <td align="center" valign="middle">450&#x2013;1000 (*)</td>
                <td align="center" valign="middle">12,995</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Tetracam MicroMCA12 RS (Rolling shutter)</td>
                <td align="center" valign="middle">1.3</td>
                <td align="center" valign="middle">115.6 &#xD7; 155 &#xD7; 68.1</td>
                <td align="center" valign="middle">4.8 &#xD7; 4.8</td>
                <td align="center" valign="middle">1</td>
                <td align="center" valign="middle">12</td>
                <td align="center" valign="middle">450&#x2013;1000 (*)</td>
                <td align="center" valign="middle">25,995</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Tetracam ADC micro</td>
                <td align="center" valign="middle">3.2</td>
                <td align="center" valign="middle">75 &#xD7; 59 &#xD7; 33</td>
                <td align="center" valign="middle">3.2 &#xD7; 3.2</td>
                <td align="center" valign="middle">0.9</td>
                <td align="center" valign="middle">6</td>
                <td align="center" valign="middle">520&#x2013;920 (Equiv. to Landsat TM2, 3, 4)</td>
                <td align="center" valign="middle">2995</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Quest Innovations Condor-5 ICX 285</td>
                <td align="center" valign="middle">7</td>
                <td align="center" valign="middle">150 &#xD7; 130 &#xD7; 177</td>
                <td align="center" valign="middle">6.45 &#xD7; 6.45</td>
                <td align="center" valign="middle">1.4</td>
                <td align="center" valign="middle">5</td>
                <td align="center" valign="middle">400&#x2013;1000</td>
                <td align="center" valign="middle">-</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Parrot Sequoia</td>
                <td align="center" valign="middle">1.2</td>
                <td align="center" valign="middle">59 &#xD7; 41 &#xD7; 28</td>
                <td align="center" valign="middle">3.75 &#xD7; 3.75</td>
                <td align="center" valign="middle">0.72</td>
                <td align="center" valign="middle">4</td>
                <td align="center" valign="middle">550&#x2013;810</td>
                <td align="center" valign="middle">5300</td>
              </tr>
              <tr>
                <td align="center" valign="middle">MicaSense RedEdge</td>
                <td align="center" valign="middle"> </td>
                <td align="center" valign="middle">120 &#xD7; 66 &#xD7; 46</td>
                <td align="center" valign="middle"> </td>
                <td align="center" valign="middle">0.18</td>
                <td align="center" valign="middle">5</td>
                <td align="center" valign="middle">475&#x2013;840</td>
                <td align="center" valign="middle">4900</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Sentera Quad</td>
                <td align="center" valign="middle">1.2</td>
                <td align="center" valign="middle">76 &#xD7; 62 &#xD7; 48</td>
                <td align="center" valign="middle">3.75 &#xD7; 3.75</td>
                <td align="center" valign="middle">0.170</td>
                <td align="center" valign="middle">4</td>
                <td align="center" valign="middle">400&#x2013;825</td>
                <td align="center" valign="middle">8500</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Sentera High Precision NDVI and NDRE</td>
                <td align="center" valign="middle">1.2</td>
                <td align="center" valign="middle">25.4 &#xD7; 33.8 &#xD7; 37.3</td>
                <td align="center" valign="middle">3.75 &#xD7; 3.75</td>
                <td align="center" valign="middle">0.030</td>
                <td align="center" valign="middle">2</td>
                <td align="center" valign="middle">525&#x2013;890</td>
                <td align="center" valign="middle">-</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Sentera Multispectral Double 4K</td>
                <td align="center" valign="middle">12.3</td>
                <td align="center" valign="middle">59 &#xD7; 41 &#xD7; 44.5</td>
                <td align="center" valign="middle">-</td>
                <td align="center" valign="middle">0.080</td>
                <td align="center" valign="middle">5</td>
                <td align="center" valign="middle">386&#x2013;860</td>
                <td align="center" valign="middle">5000</td>
              </tr>
              <tr>
                <td align="center" valign="middle">SLANTRANGE 3P NDVI</td>
                <td align="center" valign="middle">3</td>
                <td align="center" valign="middle">146 &#xD7; 69 &#xD7; 57</td>
                <td align="center" valign="middle">-</td>
                <td align="center" valign="middle">0.350</td>
                <td align="center" valign="middle">4</td>
                <td align="center" valign="middle">410&#x2013;950</td>
                <td align="center" valign="middle">4500</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Mappir Survey2</td>
                <td align="center" valign="middle">16</td>
                <td align="center" valign="middle">59 &#xD7; 41 &#xD7; 30</td>
                <td align="center" valign="middle">1.34 &#xD7; 1.34</td>
                <td align="center" valign="middle">0.047</td>
                <td align="center" valign="middle">1&#x2013;6 (filters)&#x2014;one lens</td>
                <td align="center" valign="middle">395&#x2013;945</td>
                <td align="center" valign="middle">280</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Mappir Survey3</td>
                <td align="center" valign="middle">12</td>
                <td align="center" valign="middle">59 &#xD7; 41.5 &#xD7; 36</td>
                <td align="center" valign="middle">1.55 &#xD7; 1.55</td>
                <td align="center" valign="middle">0.050</td>
                <td align="center" valign="middle">1&#x2013;4 (filters)&#x2014;one lens</td>
                <td align="center" valign="middle">395&#x2013;945</td>
                <td align="center" valign="middle">400</td>
              </tr>
              <tr>
                <td align="center" valign="middle" style="border-bottom:solid thin">Mappir Kernel</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">14.4</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">34 &#xD7; 34 &#xD7; 40</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">1.4 &#xD7; 1.4</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">0.045</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">19+ (filters)&#x2014;six array lens</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">395&#x2013;945</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">1299</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <table-wrap id="remotesensing-10-00641-t0A3" position="float">
          <object-id pub-id-type="pii">remotesensing-10-00641-t0A3_Table A3</object-id>
          <label>Table A3</label>
          <caption>
            <p>Hyperspectral cameras suitable for UAS and their main characteristics.</p>
          </caption>
          <table>
            <thead>
              <tr>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Manufacturer and Model</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Lens</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Size (mm<sup>2</sup>)</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Pixel Size (&#x3BC;m)</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Weight (kg)</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Spectral Range (nm)</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Spectral Bands (N) (Resolution, nm)</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Peak SNR</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Approx. Price ($)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="center" valign="middle">Rikola Ltd. hyperspectral camera</td>
                <td align="center" valign="middle">CMOS</td>
                <td align="center" valign="middle">5.6 &#xD7; 5.6</td>
                <td align="center" valign="middle">5.5</td>
                <td align="center" valign="middle">0.6</td>
                <td align="center" valign="middle">500&#x2013;900</td>
                <td align="center" valign="middle">40 (10 nm)</td>
                <td align="center" valign="middle">-</td>
                <td align="center" valign="middle">40,000</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Headwall Photonics Micro-hyperspec X-series NIR</td>
                <td align="center" valign="middle">InGaAs</td>
                <td align="center" valign="middle">9.6 &#xD7; 9.6</td>
                <td align="center" valign="middle">30</td>
                <td align="center" valign="middle">1.025</td>
                <td align="center" valign="middle">900&#x2013;1700</td>
                <td align="center" valign="middle">62 (12.9 nm)</td>
                <td align="center" valign="middle">-</td>
                <td align="center" valign="middle">-</td>
              </tr>
              <tr>
                <td align="center" valign="middle">BaySpec&#x2019;s OCI-UAV-1000</td>
                <td align="center" valign="middle">C-mount</td>
                <td align="center" valign="middle">10 &#xD7; 10 &#xD7; 10</td>
                <td align="center" valign="middle">N/A</td>
                <td align="center" valign="middle">0.272</td>
                <td align="center" valign="middle">600&#x2013;1000</td>
                <td align="center" valign="middle">100 (5 nm)/20&#x2013;12 (15 nm)</td>
                <td align="center" valign="middle">-</td>
                <td align="center" valign="middle">-</td>
              </tr>
              <tr>
                <td align="center" valign="middle">HySpex Mjolnir V-1240</td>
                <td align="center" valign="middle">-</td>
                <td align="center" valign="middle">25 &#xD7; 17.5 &#xD7; 17</td>
                <td align="center" valign="middle">0.27 mrad</td>
                <td align="center" valign="middle">4.0</td>
                <td align="center" valign="middle">400&#x2013;1000</td>
                <td align="center" valign="middle">200 (3 nm)</td>
                <td align="center" valign="middle">&gt;180</td>
                <td align="center" valign="middle">-</td>
              </tr>
              <tr>
                <td align="center" valign="middle">HySpex Mjolnir S-620</td>
                <td align="center" valign="middle">-</td>
                <td align="center" valign="middle">25.4 &#xD7; 17.5 &#xD7; 17</td>
                <td align="center" valign="middle">0.54 mrad</td>
                <td align="center" valign="middle">4.5</td>
                <td align="center" valign="middle">970&#x2013;2500</td>
                <td align="center" valign="middle">300 (5.1 nm)</td>
                <td align="center" valign="middle">&gt;900</td>
                <td align="center" valign="middle">-</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Specim-AISA KESTREL16</td>
                <td align="center" valign="middle">push-broom</td>
                <td align="center" valign="middle">99 &#xD7; 215 &#xD7; 240</td>
                <td align="center" valign="middle"> </td>
                <td align="center" valign="middle">2.3</td>
                <td align="center" valign="middle">600&#x2013;1640</td>
                <td align="center" valign="middle">Up to 350 (3&#x2013;8 nm)</td>
                <td align="center" valign="middle">400&#x2013;600</td>
                <td align="center" valign="middle">-</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Cornirg microHSI 410 SHARK</td>
                <td align="center" valign="middle">CCD/CMOS</td>
                <td align="center" valign="middle">136 &#xD7; 87 &#xD7; 70.35</td>
                <td align="center" valign="middle">11.7 &#x3BC;m</td>
                <td align="center" valign="middle">0.68</td>
                <td align="center" valign="middle">400&#x2013;1000</td>
                <td align="center" valign="middle">300 (2 nm)</td>
                <td align="center" valign="middle">-</td>
                <td align="center" valign="middle">-</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Resonon Pika L</td>
                <td align="center" valign="middle"> </td>
                <td align="center" valign="middle">10.0 &#xD7; 12.5 &#xD7; 5.3</td>
                <td align="center" valign="middle">5.86</td>
                <td align="center" valign="middle">0.6</td>
                <td align="center" valign="middle">400&#x2013;1000</td>
                <td align="center" valign="middle">281 (2.1 nm)</td>
                <td align="center" valign="middle">368&#x2013;520</td>
                <td align="center" valign="middle">-</td>
              </tr>
              <tr>
                <td align="center" valign="middle" style="border-bottom:solid thin">CUBERT (S185)</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">Snapshot + PAN</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">19 &#xD7; 42 &#xD7; 65</td>
                <td align="center" valign="middle" style="border-bottom:solid thin"> </td>
                <td align="center" valign="middle" style="border-bottom:solid thin">0.49</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">450&#x2013;995</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">125 (8 mm)</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">-</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">50,000</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <table-wrap id="remotesensing-10-00641-t0A4" position="float">
          <object-id pub-id-type="pii">remotesensing-10-00641-t0A4_Table A4</object-id>
          <label>Table A4</label>
          <caption>
            <p>Representative thermal cameras suitable for UAS and their main characteristics.</p>
          </caption>
          <table>
            <thead>
              <tr>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Manufacturer and Model</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Resolution <break/>(Px)</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Sensor Size <break/>(mm<sup>2</sup>)</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Pixel Pitch <break/>(&#x3BC;m)</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Weight <break/>(kg)</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Spectral Range <break/>(&#x3BC;m)</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Thermal Sensitivity <break/>(mK)</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Approx. Price <break/>($)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="center" valign="middle">FLIR Duo Pro 640</td>
                <td align="center" valign="middle">640 &#xD7; 512</td>
                <td align="center" valign="middle">10.8 &#xD7; 8.7</td>
                <td align="center" valign="middle">17</td>
                <td align="center" valign="middle">&lt;0.115</td>
                <td align="center" valign="middle">7.5&#x2013;13.5</td>
                <td align="center" valign="middle">50</td>
                <td align="center" valign="middle">10,500</td>
              </tr>
              <tr>
                <td align="center" valign="middle">FLIR Duo Pro 336</td>
                <td align="center" valign="middle">336 &#xD7; 256</td>
                <td align="center" valign="middle">5.7 &#xD7; 4.4</td>
                <td align="center" valign="middle">17</td>
                <td align="center" valign="middle">&lt;0.115</td>
                <td align="center" valign="middle">7.5&#x2013;13.5</td>
                <td align="center" valign="middle">50</td>
                <td align="center" valign="middle">7500</td>
              </tr>
              <tr>
                <td align="center" valign="middle">FLIR Duo R</td>
                <td align="center" valign="middle">160 &#xD7; 120</td>
                <td align="center" valign="middle">-</td>
                <td align="center" valign="middle">-</td>
                <td align="center" valign="middle">0.084</td>
                <td align="center" valign="middle">7.5&#x2013;13.5</td>
                <td align="center" valign="middle">50</td>
                <td align="center" valign="middle">2200</td>
              </tr>
              <tr>
                <td align="center" valign="middle">FLIR Tau2 640</td>
                <td align="center" valign="middle">640 &#xD7; 512</td>
                <td align="center" valign="middle">N/A</td>
                <td align="center" valign="middle">17</td>
                <td align="center" valign="middle">&lt;0.112</td>
                <td align="center" valign="middle">7.5&#x2013;13.5</td>
                <td align="center" valign="middle">50</td>
                <td align="center" valign="middle">9000</td>
              </tr>
              <tr>
                <td align="center" valign="middle">FLIR Tau2 336</td>
                <td align="center" valign="middle">336 &#xD7; 256</td>
                <td align="center" valign="middle">N/A</td>
                <td align="center" valign="middle">17</td>
                <td align="center" valign="middle">&lt;0.112</td>
                <td align="center" valign="middle">7.5&#x2013;13.5</td>
                <td align="center" valign="middle">50</td>
                <td align="center" valign="middle">4000</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Optris PI 450</td>
                <td align="center" valign="middle">382 &#xD7; 288</td>
                <td align="center" valign="middle">-</td>
                <td align="center" valign="middle">-</td>
                <td align="center" valign="middle">0.320</td>
                <td align="center" valign="middle">7.5&#x2013;13</td>
                <td align="center" valign="middle">130</td>
                <td align="center" valign="middle">7000</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Optris PI 640</td>
                <td align="center" valign="middle">640 &#xD7; 480</td>
                <td align="center" valign="middle">-</td>
                <td align="center" valign="middle">-</td>
                <td align="center" valign="middle">0.320</td>
                <td align="center" valign="middle">7.5&#x2013;13</td>
                <td align="center" valign="middle">130</td>
                <td align="center" valign="middle">9700</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Thermoteknix Miricle 307 K</td>
                <td align="center" valign="middle">640 &#xD7; 480</td>
                <td align="center" valign="middle">16.0 &#xD7; 12.0</td>
                <td align="center" valign="middle">25</td>
                <td align="center" valign="middle">&lt;0.170</td>
                <td align="center" valign="middle">8.0&#x2013;12.0</td>
                <td align="center" valign="middle">50</td>
                <td align="center" valign="middle">-</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Thermoteknix Miricle 110 K</td>
                <td align="center" valign="middle">384 &#xD7; 288</td>
                <td align="center" valign="middle">9.6 &#xD7; 7.2</td>
                <td align="center" valign="middle">25</td>
                <td align="center" valign="middle">&lt;0.170</td>
                <td align="center" valign="middle">8.0&#x2013;12.0</td>
                <td align="center" valign="middle">50/70</td>
                <td align="center" valign="middle">-</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Workswell WIRIS 640</td>
                <td align="center" valign="middle">640 &#xD7; 512</td>
                <td align="center" valign="middle">16.0 &#xD7; 12.8</td>
                <td align="center" valign="middle">25</td>
                <td align="center" valign="middle">&lt;0.400</td>
                <td align="center" valign="middle">7.5&#x2013;13.5</td>
                <td align="center" valign="middle">30/50</td>
                <td align="center" valign="middle">-</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Workswell WIRIS 336</td>
                <td align="center" valign="middle">336 &#xD7; 256</td>
                <td align="center" valign="middle">8.4 &#xD7; 6.4</td>
                <td align="center" valign="middle">25</td>
                <td align="center" valign="middle">&lt;0.400</td>
                <td align="center" valign="middle">7.5&#x2013;13.5</td>
                <td align="center" valign="middle">30/50</td>
                <td align="center" valign="middle">-</td>
              </tr>
              <tr>
                <td align="center" valign="middle" style="border-bottom:solid thin">YUNCGOETEU</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">160 &#xD7; 120</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">81 &#xD7; 108 &#xD7; 138</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">12</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">0.278</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">8.0&#x2013;14.0</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">&lt;50</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">-</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <table-wrap id="remotesensing-10-00641-t0A5" position="float">
          <object-id pub-id-type="pii">remotesensing-10-00641-t0A5_Table A5</object-id>
          <label>Table A5</label>
          <caption>
            <p>Laser scanners suitable for UAS and their main characteristics.</p>
          </caption>
          <table>
            <thead>
              <tr>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Manufacturer and Model</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Scanning Pattern</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Range (m)</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Weight (kg)</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Angular Res. (deg)</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">FOV (deg)</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Laser Class and &#x3BB; (nm)</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Frequency (kp/s)</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Aprox. Price ($)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="center" valign="middle">ibeo Automotive Systems IBEO LUX</td>
                <td align="center" valign="middle">4 Scanning parallel lines</td>
                <td align="center" valign="middle">200</td>
                <td align="center" valign="middle">1</td>
                <td align="center" valign="middle">(H) 0.125 (V) 0.8</td>
                <td align="center" valign="middle">(H) 110 (V) 3.2</td>
                <td align="center" valign="middle">Class A 905</td>
                <td align="center" valign="middle">22</td>
                <td align="center" valign="middle">-</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Velodyne HDL-32E</td>
                <td align="center" valign="middle">32 Laser/detector pairs</td>
                <td align="center" valign="middle">100</td>
                <td align="center" valign="middle">2</td>
                <td align="center" valign="middle">(H)&#x2013;(V) 1.33</td>
                <td align="center" valign="middle">(H) 360 (V) 41</td>
                <td align="center" valign="middle">Class A 905</td>
                <td align="center" valign="middle">700</td>
                <td align="center" valign="middle">-</td>
              </tr>
              <tr>
                <td align="center" valign="middle">RIEGL VQ-820-GU</td>
                <td align="center" valign="middle">1 Scanning line</td>
                <td align="center" valign="middle">&gt;1000</td>
                <td align="center" valign="middle">25.5</td>
                <td align="center" valign="middle">(H) 0.01 (V) N/A</td>
                <td align="center" valign="middle">(H) 60 (V) N/A</td>
                <td align="center" valign="middle">Class 3B 532</td>
                <td align="center" valign="middle">200</td>
                <td align="center" valign="middle">-</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Hokuyo UTM-30LX-EW</td>
                <td align="center" valign="middle">1080 distances in a plane</td>
                <td align="center" valign="middle">30</td>
                <td align="center" valign="middle">0.37</td>
                <td align="center" valign="middle">(H) 0.25 (V) N/A</td>
                <td align="center" valign="middle">(H) 270 (V) N/A</td>
                <td align="center" valign="middle">Class 1905</td>
                <td align="center" valign="middle">200</td>
                <td align="center" valign="middle">-</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Velodyne Puck Hi-Res</td>
                <td align="center" valign="middle">Dual Returns</td>
                <td align="center" valign="middle">100</td>
                <td align="center" valign="middle">0.590</td>
                <td align="center" valign="middle">(H)&#x2013;(V) 0.1&#x2013;0.4</td>
                <td align="center" valign="middle">(H) 360 (V) 20</td>
                <td align="center" valign="middle">Class A-903</td>
                <td align="center" valign="middle">-</td>
                <td align="center" valign="middle">-</td>
              </tr>
              <tr>
                <td align="center" valign="middle">RIEGL VUX-1UAV</td>
                <td align="center" valign="middle">Parallel scan lines</td>
                <td align="center" valign="middle">150</td>
                <td align="center" valign="middle">3.5</td>
                <td align="center" valign="middle">0.001&#xB0;</td>
                <td align="center" valign="middle">330</td>
                <td align="center" valign="middle">Class A-NIR</td>
                <td align="center" valign="middle">200</td>
                <td align="center" valign="middle">&gt;120,000</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Routescene&#x2014;UAV LidarPod</td>
                <td align="center" valign="middle">32 Laser/detector pairs</td>
                <td align="center" valign="middle">100</td>
                <td align="center" valign="middle">1.3</td>
                <td align="center" valign="middle">(H)&#x2013;(V) 1.33</td>
                <td align="center" valign="middle">(H) 360 (V) 41</td>
                <td align="center" valign="middle">Class A-905</td>
                <td align="center" valign="middle">-</td>
                <td align="center" valign="middle">-</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Quanergy M8-1</td>
                <td align="center" valign="middle">8 laser/detector pairs</td>
                <td align="center" valign="middle">150</td>
                <td align="center" valign="middle">0.9</td>
                <td align="center" valign="middle">0.03&#x2013;0.2&#xB0;</td>
                <td align="center" valign="middle">(H) 360 (V) 20</td>
                <td align="center" valign="middle">Class A-905</td>
                <td align="center" valign="middle">-</td>
                <td align="center" valign="middle">-</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Phoenix Scout</td>
                <td align="center" valign="middle">Dual Returns</td>
                <td align="center" valign="middle">120</td>
                <td align="center" valign="middle">1.65</td>
                <td align="center" valign="middle">-</td>
                <td align="center" valign="middle">(H) 360 (V) 15</td>
                <td align="center" valign="middle">Class 1-905</td>
                <td align="center" valign="middle">300</td>
                <td align="center" valign="middle">&gt;66,000</td>
              </tr>
              <tr>
                <td align="center" valign="middle">Phoenix ALS-32</td>
                <td align="center" valign="middle">32 Laser/detector pairs</td>
                <td align="center" valign="middle">120</td>
                <td align="center" valign="middle">2.4</td>
                <td align="center" valign="middle">-</td>
                <td align="center" valign="middle">(H) 360 (V) 10&#x2013;30</td>
                <td align="center" valign="middle">Class 1-905</td>
                <td align="center" valign="middle">700</td>
                <td align="center" valign="middle">&gt;120,500</td>
              </tr>
              <tr>
                <td align="center" valign="middle">YellowScan Surveyor</td>
                <td align="center" valign="middle">Dual returns</td>
                <td align="center" valign="middle">100</td>
                <td align="center" valign="middle">1.6</td>
                <td align="center" valign="middle">0.125</td>
                <td align="center" valign="middle">360</td>
                <td align="center" valign="middle">Class 1-905</td>
                <td align="center" valign="middle">300</td>
                <td align="center" valign="middle">&gt;93,000</td>
              </tr>
              <tr>
                <td align="center" valign="middle" style="border-bottom:solid thin">YellowScan Vx</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">Parallel scan lines</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">100</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">2.5&#x2013;3</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">-</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">360</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">Class 1-905</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">100</td>
                <td align="center" valign="middle" style="border-bottom:solid thin">&gt;93,000</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </app>
    </app-group>
    <ref-list>
      <title>References</title>
      <ref id="B1-remotesensing-10-00641">
        <label>1.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Belward</surname>
              <given-names>A.S.</given-names>
            </name>
            <name>
              <surname>Sk&#xF8;ien</surname>
              <given-names>J.O.</given-names>
            </name>
          </person-group>
          <article-title>Who Launched What, When and Why; Trends in Global Land-Cover Observation Capacity from Civilian Earth Observation Satellites</article-title>
          <source>ISPRS J. Photogramm. Remote Sens.</source>
          <year>2015</year>
          <volume>103</volume>
          <fpage>115</fpage>
          <lpage>128</lpage>
          <pub-id pub-id-type="doi">10.1016/j.isprsjprs.2014.03.009</pub-id>
        </element-citation>
      </ref>
      <ref id="B2-remotesensing-10-00641">
        <label>2.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hand</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <article-title>Startup Liftoff</article-title>
          <source>Science</source>
          <year>2015</year>
          <volume>348</volume>
          <fpage>172</fpage>
          <lpage>177</lpage>
          <pub-id pub-id-type="doi">10.1126/science.348.6231.172</pub-id>
          <pub-id pub-id-type="pmid">25859026</pub-id>
        </element-citation>
      </ref>
      <ref id="B3-remotesensing-10-00641">
        <label>3.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wekerle</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Bezerra Pessoa Filho</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Eduardo Vergueiro Loures da Costa</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Gonzaga Trabasso</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <article-title>Status and Trends of Smallsats and Their Launch Vehicles&#x2014;An Up-to-Date Review</article-title>
          <source>J. Aerosp. Technol. Manag.</source>
          <year>2017</year>
          <volume>9</volume>
          <fpage>269</fpage>
          <lpage>286</lpage>
          <pub-id pub-id-type="doi">10.5028/jatm.v9i3.853</pub-id>
        </element-citation>
      </ref>
      <ref id="B4-remotesensing-10-00641">
        <label>4.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>McCabe</surname>
              <given-names>M.F.</given-names>
            </name>
            <name>
              <surname>Rodell</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Alsdorf</surname>
              <given-names>D.E.</given-names>
            </name>
            <name>
              <surname>Miralles</surname>
              <given-names>D.G.</given-names>
            </name>
            <name>
              <surname>Uijlenhoet</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Wagner</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Lucieer</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Houborg</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Verhoest</surname>
              <given-names>N.E.C.</given-names>
            </name>
            <name>
              <surname>Franz</surname>
              <given-names>T.E.</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>The future of Earth observation in hydrology</article-title>
          <source>Hydrol. Earth Syst. Sci.</source>
          <year>2017</year>
          <volume>21</volume>
          <fpage>3879</fpage>
          <lpage>3914</lpage>
          <pub-id pub-id-type="doi">10.5194/hess-21-3879-2017</pub-id>
        </element-citation>
      </ref>
      <ref id="B5-remotesensing-10-00641">
        <label>5.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>McCabe</surname>
              <given-names>M.F.</given-names>
            </name>
            <name>
              <surname>Aragon</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Houborg</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Mascaro</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>CubeSats in hydrology: Ultrahigh-resolution insights into vegetation dynamics and terrestrial evaporation</article-title>
          <source>Water Resour. Res.</source>
          <year>2017</year>
          <volume>53</volume>
          <fpage>10017</fpage>
          <lpage>10024</lpage>
          <pub-id pub-id-type="doi">10.1002/2017WR022240</pub-id>
        </element-citation>
      </ref>
      <ref id="B6-remotesensing-10-00641">
        <label>6.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Pajares</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Overview and Current Status of Remote Sensing Applications Based on Unmanned Aerial Vehicles (UAVs)</article-title>
          <source>Photogramm. Eng. Remote Sens.</source>
          <year>2015</year>
          <volume>81</volume>
          <fpage>281</fpage>
          <lpage>329</lpage>
          <pub-id pub-id-type="doi">10.14358/PERS.81.4.281</pub-id>
        </element-citation>
      </ref>
      <ref id="B7-remotesensing-10-00641">
        <label>7.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Matese</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Toscano</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Di Gennaro</surname>
              <given-names>S.F.</given-names>
            </name>
            <name>
              <surname>Genesio</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Vaccari</surname>
              <given-names>F.P.</given-names>
            </name>
            <name>
              <surname>Primicerio</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Belli</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Zaldei</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Bianconi</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Gioli</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <article-title>Intercomparison of UAV, Aircraft and Satellite Remote Sensing Platforms for Precision Viticulture</article-title>
          <source>Remote Sens.</source>
          <year>2015</year>
          <volume>7</volume>
          <fpage>2971</fpage>
          <lpage>2990</lpage>
          <pub-id pub-id-type="doi">10.3390/rs70302971</pub-id>
        </element-citation>
      </ref>
      <ref id="B8-remotesensing-10-00641">
        <label>8.</label>
        <element-citation publication-type="thesis">
          <person-group person-group-type="author">
            <name>
              <surname>Dustin</surname>
              <given-names>M.C.</given-names>
            </name>
          </person-group>
          <article-title>Monitoring Parks with Inexpensive UAVs: Cost Benefits Analysis for Monitoring and Maintaining Parks Facilities</article-title>
          <source>Ph.D. Thesis</source>
          <publisher-name>University of Southern California</publisher-name>
          <publisher-loc>Los Angeles, CA, USA</publisher-loc>
          <year>2015</year>
        </element-citation>
      </ref>
      <ref id="B9-remotesensing-10-00641">
        <label>9.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lucieer</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Jong</surname>
              <given-names>S.M.D.</given-names>
            </name>
            <name>
              <surname>Turner</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>Mapping landslide displacements using Structure from Motion (SfM) and image correlation of multi-temporal UAV photography</article-title>
          <source>Progr. Phys. Geog.</source>
          <year>2014</year>
          <volume>38</volume>
          <fpage>97</fpage>
          <lpage>116</lpage>
          <pub-id pub-id-type="doi">10.1177/0309133313515293</pub-id>
        </element-citation>
      </ref>
      <ref id="B10-remotesensing-10-00641">
        <label>10.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Watts</surname>
              <given-names>A.C.</given-names>
            </name>
            <name>
              <surname>Ambrosia</surname>
              <given-names>V.G.</given-names>
            </name>
            <name>
              <surname>Hinkley</surname>
              <given-names>E.A.</given-names>
            </name>
          </person-group>
          <article-title>Unmanned aircraft systems in remote sensing and scientific research: Classification and Considerations of use</article-title>
          <source>Remote Sens.</source>
          <year>2012</year>
          <volume>4</volume>
          <fpage>1671</fpage>
          <lpage>1692</lpage>
          <pub-id pub-id-type="doi">10.3390/rs4061671</pub-id>
        </element-citation>
      </ref>
      <ref id="B11-remotesensing-10-00641">
        <label>11.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Van der Wal</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Abma</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Viguria</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Previnaire</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Zarco-Tejada</surname>
              <given-names>P.J.</given-names>
            </name>
            <name>
              <surname>Serruys</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>van Valkengoed</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>van der Voet</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>Fieldcopter: Unmanned aerial systems for crop monitoring services</article-title>
          <source>Precis. Agric.</source>
          <year>2013</year>
          <volume>13</volume>
          <fpage>169</fpage>
          <lpage>175</lpage>
        </element-citation>
      </ref>
      <ref id="B12-remotesensing-10-00641">
        <label>12.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Anderson</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Gaston</surname>
              <given-names>K.J.</given-names>
            </name>
          </person-group>
          <article-title>Lightweight unmanned aerial vehicles will revolutionize spatial ecology</article-title>
          <source>Front. Ecol. Environ.</source>
          <year>2013</year>
          <volume>11</volume>
          <fpage>138</fpage>
          <lpage>146</lpage>
          <pub-id pub-id-type="doi">10.1890/120150</pub-id>
        </element-citation>
      </ref>
      <ref id="B13-remotesensing-10-00641">
        <label>13.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Whitehead</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Hugenholtz</surname>
              <given-names>C.H.</given-names>
            </name>
          </person-group>
          <article-title>Remote sensing of the environment with small unmanned aircraft systems (UASs), part 1: A review of progress and challenges</article-title>
          <source>J. Unmanned Veh. Syst.</source>
          <year>2014</year>
          <volume>2</volume>
          <fpage>69</fpage>
          <lpage>85</lpage>
          <pub-id pub-id-type="doi">10.1139/juvs-2014-0006</pub-id>
        </element-citation>
      </ref>
      <ref id="B14-remotesensing-10-00641">
        <label>14.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Whitehead</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Hugenholtz</surname>
              <given-names>C.H.</given-names>
            </name>
            <name>
              <surname>Myshak</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Brown</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>LeClair</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Tamminga</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Barchyn</surname>
              <given-names>T.E.</given-names>
            </name>
            <name>
              <surname>Moorman</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Eaton</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <article-title>Remote sensing of the environment with small unmanned aircraft systems (UASs), part 2: Scientific and commercial applications</article-title>
          <source>J. Unmanned Veh. Syst.</source>
          <year>2014</year>
          <volume>2</volume>
          <fpage>86</fpage>
          <lpage>102</lpage>
          <pub-id pub-id-type="doi">10.1139/juvs-2014-0007</pub-id>
        </element-citation>
      </ref>
      <ref id="B15-remotesensing-10-00641">
        <label>15.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ad&#xE3;o</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Hru&#x161;ka</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>P&#xE1;dua</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Bessa</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Peres</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Morais</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Sousa</surname>
              <given-names>J.J.</given-names>
            </name>
          </person-group>
          <article-title>Hyperspectral imaging: A review on UAV-based sensors, data processing and applications for agriculture and forestry</article-title>
          <source>Remote Sens.</source>
          <year>2017</year>
          <volume>9</volume>
          <elocation-id>1110</elocation-id>
          <pub-id pub-id-type="doi">10.3390/rs9111110</pub-id>
        </element-citation>
      </ref>
      <ref id="B16-remotesensing-10-00641">
        <label>16.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tauro</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Selker</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>van de Giesen</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Abrate</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Uijlenhoet</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Porfiri</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Manfreda Oscaro</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Caylor</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Moramarco</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Benveniste</surname>
              <given-names>J.</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Measurements and Observations in the XXI century (MOXXI): Innovation and multidisciplinarity to sense the hydrological cycle</article-title>
          <source>Hydrolog. Sci. J.</source>
          <year>2018</year>
          <volume>63</volume>
          <fpage>169</fpage>
          <lpage>196</lpage>
          <pub-id pub-id-type="doi">10.1080/02626667.2017.1420191</pub-id>
        </element-citation>
      </ref>
      <ref id="B17-remotesensing-10-00641">
        <label>17.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Singh</surname>
              <given-names>K.K.</given-names>
            </name>
            <name>
              <surname>Frazier</surname>
              <given-names>A.E.</given-names>
            </name>
          </person-group>
          <article-title>A meta-analysis and review of unmanned aircraft system (UAS) imagery for terrestrial applications</article-title>
          <source>Int. J. Remote Sens.</source>
          <year>2018</year>
          <fpage>1</fpage>
          <lpage>21</lpage>
          <pub-id pub-id-type="doi">10.1080/01431161.2017.1420941</pub-id>
        </element-citation>
      </ref>
      <ref id="B18-remotesensing-10-00641">
        <label>18.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bryson</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Reid</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Ramos</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Sukkarieh</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Airborne Vision-Based Mapping and Classification of Large Farmland Environments</article-title>
          <source>J. Field Robot.</source>
          <year>2010</year>
          <volume>27</volume>
          <fpage>632</fpage>
          <lpage>655</lpage>
          <pub-id pub-id-type="doi">10.1002/rob.20343</pub-id>
        </element-citation>
      </ref>
      <ref id="B19-remotesensing-10-00641">
        <label>19.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Akar</surname>
              <given-names>O.</given-names>
            </name>
          </person-group>
          <article-title>Mapping land use with using Rotation Forest algorithm from UAV images</article-title>
          <source>Eur. J. Remote Sens.</source>
          <year>2017</year>
          <volume>50</volume>
          <fpage>269</fpage>
          <lpage>279</lpage>
          <pub-id pub-id-type="doi">10.1080/22797254.2017.1319252</pub-id>
        </element-citation>
      </ref>
      <ref id="B20-remotesensing-10-00641">
        <label>20.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bueren</surname>
              <given-names>S.K.</given-names>
            </name>
            <name>
              <surname>Burkart</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Hueni</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Rascher</surname>
              <given-names>U.</given-names>
            </name>
            <name>
              <surname>Tuohy</surname>
              <given-names>M.P.</given-names>
            </name>
            <name>
              <surname>Yule</surname>
              <given-names>I.J.</given-names>
            </name>
          </person-group>
          <article-title>Deploying four optical UAV-based sensors over grassland: Challenges and limitations</article-title>
          <source>Biogeosciences</source>
          <year>2015</year>
          <volume>12</volume>
          <fpage>163</fpage>
          <lpage>175</lpage>
          <pub-id pub-id-type="doi">10.5194/bg-12-163-2015</pub-id>
        </element-citation>
      </ref>
      <ref id="B21-remotesensing-10-00641">
        <label>21.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ludovisi</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Tauro</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Salvati</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Khoury</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Mugnozza Scarascia</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Harfouche</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>UAV-based thermal imaging for high-throughput field phenotyping of black poplar response to drought</article-title>
          <source>Front. Plant Sci.</source>
          <year>2017</year>
          <volume>8</volume>
          <fpage>1681</fpage>
          <pub-id pub-id-type="doi">10.3389/fpls.2017.01681</pub-id>
          <pub-id pub-id-type="pmid">29021803</pub-id>
        </element-citation>
      </ref>
      <ref id="B22-remotesensing-10-00641">
        <label>22.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Zhu</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Deng</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Harmon</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>Quantifying Nitrogen Status of Rice Using Low Altitude UAV-Mounted System and Object-Oriented Segmentation Methodology</article-title>
          <source>Proceedings of the ASME International Design Engineering Technical Conferences and Computers and Information in Engineering Conference</source>
          <conf-loc>San Diego, CA, USA</conf-loc>
          <conf-date>30 August&#x2013;2 September 2009</conf-date>
          <fpage>1</fpage>
          <lpage>7</lpage>
        </element-citation>
      </ref>
      <ref id="B23-remotesensing-10-00641">
        <label>23.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Urbahs</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Jonaite</surname>
              <given-names>I.</given-names>
            </name>
          </person-group>
          <article-title>Features of the use of unmanned aerial vehicles for agriculture applications</article-title>
          <source>Aviation</source>
          <year>2013</year>
          <volume>17</volume>
          <fpage>170</fpage>
          <lpage>175</lpage>
          <pub-id pub-id-type="doi">10.3846/16487788.2013.861224</pub-id>
        </element-citation>
      </ref>
      <ref id="B24-remotesensing-10-00641">
        <label>24.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Jeunnette</surname>
              <given-names>M.N.</given-names>
            </name>
            <name>
              <surname>Hart</surname>
              <given-names>D.P.</given-names>
            </name>
          </person-group>
          <article-title>Remote sensing for developing world agriculture: Opportunities and areas for technical development</article-title>
          <source>Proceedings of the Remote Sensing for Agriculture, Ecosystems, and Hydrology XVIII</source>
          <conf-loc>Edinburgh, UK</conf-loc>
          <conf-date>26&#x2013;29 September 2016</conf-date>
        </element-citation>
      </ref>
      <ref id="B25-remotesensing-10-00641">
        <label>25.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Samseemoung</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Soni</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Jayasuriya</surname>
              <given-names>H.P.W.</given-names>
            </name>
            <name>
              <surname>Salokhe</surname>
              <given-names>V.M.</given-names>
            </name>
          </person-group>
          <article-title>An Application of low altitude remote sensing (LARS) platform for monitoring crop growth and weed infestation in a soybean plantation</article-title>
          <source>Precis. Agric.</source>
          <year>2012</year>
          <volume>13</volume>
          <fpage>611</fpage>
          <lpage>627</lpage>
          <pub-id pub-id-type="doi">10.1007/s11119-012-9271-8</pub-id>
        </element-citation>
      </ref>
      <ref id="B26-remotesensing-10-00641">
        <label>26.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Alvarez-Taboada</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Paredes</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Juli&#xE1;n-Pelaz</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Mapping of the Invasive Species Hakea sericea Using Unmanned Aerial Vehicle (UAV) and WorldView-2 Imagery and an Object-Oriented Approach</article-title>
          <source>Remote Sens.</source>
          <year>2017</year>
          <volume>9</volume>
          <elocation-id>913</elocation-id>
          <pub-id pub-id-type="doi">10.3390/rs9090913</pub-id>
        </element-citation>
      </ref>
      <ref id="B27-remotesensing-10-00641">
        <label>27.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Witte</surname>
              <given-names>B.M.</given-names>
            </name>
            <name>
              <surname>Singler</surname>
              <given-names>R.F.</given-names>
            </name>
            <name>
              <surname>Bailey</surname>
              <given-names>S.C.C.</given-names>
            </name>
          </person-group>
          <article-title>Development of an Unmanned Aerial Vehicle for the Measurement of Turbulence in the Atmospheric Boundary Layer</article-title>
          <source>Atmosphere</source>
          <year>2017</year>
          <volume>8</volume>
          <elocation-id>195</elocation-id>
          <pub-id pub-id-type="doi">10.3390/atmos8100195</pub-id>
        </element-citation>
      </ref>
      <ref id="B28-remotesensing-10-00641">
        <label>28.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Stone</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>D&#x2019;Ayala</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Wilkinson</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <source>The Use of Emerging Technology in Post-Disaster Reconnaissance Missions</source>
          <comment>EEFIT Report</comment>
          <publisher-name>Institution of Structural Engineers</publisher-name>
          <publisher-loc>London, UK</publisher-loc>
          <year>2017</year>
          <size units="pages">25p</size>
        </element-citation>
      </ref>
      <ref id="B29-remotesensing-10-00641">
        <label>29.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Frankenberger</surname>
              <given-names>J.R.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Nouwakpo</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>Low-altitude digital photogrammetry technique to assess ephemeral gully erosion</article-title>
          <source>Proceedings of the IEEE International Geoscience and Remote Sensing Symposium (IGARSS 2008)</source>
          <conf-loc>Boston, MA, USA</conf-loc>
          <conf-date>7&#x2013;11 July 2008</conf-date>
          <fpage>117</fpage>
          <lpage>120</lpage>
        </element-citation>
      </ref>
      <ref id="B30-remotesensing-10-00641">
        <label>30.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>d&#x2019;Oleire-Oltmanns</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Marzolff</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Peter</surname>
              <given-names>K.D.</given-names>
            </name>
            <name>
              <surname>Ries</surname>
              <given-names>J.B.</given-names>
            </name>
          </person-group>
          <article-title>Unmanned aerial vehicle (UAV) for monitoring soil erosion in Morocco</article-title>
          <source>Remote Sens.</source>
          <year>2012</year>
          <volume>4</volume>
          <fpage>3390</fpage>
          <lpage>3416</lpage>
          <pub-id pub-id-type="doi">10.3390/rs4113390</pub-id>
        </element-citation>
      </ref>
      <ref id="B31-remotesensing-10-00641">
        <label>31.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Quiquerez</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Chevigny</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Allemand</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Curmi</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Petit</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Grandjean</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>Assessing the impact of soil surface characteristics on vineyard erosion from very high spatial resolution aerial images (C&#xF4;te de Beaune, Burgundy, France)</article-title>
          <source>Catena</source>
          <year>2014</year>
          <volume>116</volume>
          <fpage>163</fpage>
          <lpage>172</lpage>
          <pub-id pub-id-type="doi">10.1016/j.catena.2013.12.002</pub-id>
        </element-citation>
      </ref>
      <ref id="B32-remotesensing-10-00641">
        <label>32.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Aldana-Jague</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Heckrath</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Macdonald</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>van Wesemael</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Van Oost</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>UAS-based soil carbon mapping using VIS-NIR (480-1000 nm) multi-spectral imaging: Potential and limitations</article-title>
          <source>Geoderma</source>
          <year>2016</year>
          <volume>275</volume>
          <fpage>55</fpage>
          <lpage>66</lpage>
          <pub-id pub-id-type="doi">10.1016/j.geoderma.2016.04.012</pub-id>
        </element-citation>
      </ref>
      <ref id="B33-remotesensing-10-00641">
        <label>33.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Niethammer</surname>
              <given-names>U.</given-names>
            </name>
            <name>
              <surname>James</surname>
              <given-names>M.R.</given-names>
            </name>
            <name>
              <surname>Rothmund</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Travelletti</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Joswig</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>UAV-based remote sensing of the Super Sauze landslide: Evaluation and results</article-title>
          <source>Eng. Geol.</source>
          <year>2012</year>
          <volume>128</volume>
          <fpage>2</fpage>
          <lpage>11</lpage>
          <pub-id pub-id-type="doi">10.1016/j.enggeo.2011.03.012</pub-id>
        </element-citation>
      </ref>
      <ref id="B34-remotesensing-10-00641">
        <label>34.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sieberth</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Wackrow</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Chandler</surname>
              <given-names>J.H.</given-names>
            </name>
          </person-group>
          <article-title>Automatic detection of blurred images in UAV image sets</article-title>
          <source>ISPRS J. Photogramm. Remote Sens.</source>
          <year>2016</year>
          <volume>122</volume>
          <fpage>1</fpage>
          <lpage>16</lpage>
          <pub-id pub-id-type="doi">10.1016/j.isprsjprs.2016.09.010</pub-id>
        </element-citation>
      </ref>
      <ref id="B35-remotesensing-10-00641">
        <label>35.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Colomina</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Molina</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>Unmanned aerial systems for photogrammetry and remote sensing: A review</article-title>
          <source>ISPRS J. Photogramm. Remote Sens.</source>
          <year>2014</year>
          <volume>92</volume>
          <fpage>79</fpage>
          <lpage>97</lpage>
          <pub-id pub-id-type="doi">10.1016/j.isprsjprs.2014.02.013</pub-id>
        </element-citation>
      </ref>
      <ref id="B36-remotesensing-10-00641">
        <label>36.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Mesas-Carrascosa</surname>
              <given-names>F.J.</given-names>
            </name>
            <name>
              <surname>Rumbao</surname>
              <given-names>I.C.</given-names>
            </name>
            <name>
              <surname>Berrocal</surname>
              <given-names>J.A.B.</given-names>
            </name>
            <name>
              <surname>Porras</surname>
              <given-names>A.G.F.</given-names>
            </name>
          </person-group>
          <article-title>Positional quality assessment of orthophotos obtained from sensors on board multi-rotor UAV platforms</article-title>
          <source>Sensors</source>
          <year>2014</year>
          <volume>14</volume>
          <fpage>22394</fpage>
          <lpage>22407</lpage>
          <pub-id pub-id-type="doi">10.3390/s141222394</pub-id>
          <pub-id pub-id-type="pmid">25587877</pub-id>
        </element-citation>
      </ref>
      <ref id="B37-remotesensing-10-00641">
        <label>37.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ai</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Yuan</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>A robust photogrammetric processing method of low-altitude UAV images</article-title>
          <source>Remote Sens.</source>
          <year>2015</year>
          <volume>7</volume>
          <fpage>2302</fpage>
          <lpage>2333</lpage>
          <pub-id pub-id-type="doi">10.3390/rs70302302</pub-id>
        </element-citation>
      </ref>
      <ref id="B38-remotesensing-10-00641">
        <label>38.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>James</surname>
              <given-names>M.R.</given-names>
            </name>
            <name>
              <surname>Robson</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Mitigating systematic error in topographic models derived from UAV and ground-based image networks</article-title>
          <source>Earth Surf. Process. Landf.</source>
          <year>2014</year>
          <volume>39</volume>
          <fpage>1413</fpage>
          <lpage>1420</lpage>
          <pub-id pub-id-type="doi">10.1002/esp.3609</pub-id>
        </element-citation>
      </ref>
      <ref id="B39-remotesensing-10-00641">
        <label>39.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Peppa</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Mills</surname>
              <given-names>J.P.</given-names>
            </name>
            <name>
              <surname>Moore</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Miller</surname>
              <given-names>P.E.</given-names>
            </name>
            <name>
              <surname>Chambers</surname>
              <given-names>J.C.</given-names>
            </name>
          </person-group>
          <article-title>Accuracy assessment of a UAV-based landslide monitoring system</article-title>
          <source>Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci.</source>
          <year>2016</year>
          <volume>41</volume>
          <fpage>895</fpage>
          <lpage>902</lpage>
          <pub-id pub-id-type="doi">10.5194/isprsarchives-XLI-B5-895-2016</pub-id>
        </element-citation>
      </ref>
      <ref id="B40-remotesensing-10-00641">
        <label>40.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Eltner</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Schneider</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>Analysis of Different Methods for 3D Reconstruction of Natural Surfaces from Parallel-Axes UAV Images</article-title>
          <source>Photogramm. Record</source>
          <year>2015</year>
          <volume>30</volume>
          <fpage>279</fpage>
          <lpage>299</lpage>
          <pub-id pub-id-type="doi">10.1111/phor.12115</pub-id>
        </element-citation>
      </ref>
      <ref id="B41-remotesensing-10-00641">
        <label>41.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>James</surname>
              <given-names>M.R.</given-names>
            </name>
            <name>
              <surname>Robson</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>d&#x2019;Oleire-Oltmanns</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Niethammer</surname>
              <given-names>U.</given-names>
            </name>
          </person-group>
          <article-title>Optimising UAV topographic surveys processed with structure-from-motion: Ground control quality, quantity and bundle adjustment</article-title>
          <source>Geomorphology</source>
          <year>2017</year>
          <volume>280</volume>
          <fpage>51</fpage>
          <lpage>66</lpage>
          <pub-id pub-id-type="doi">10.1016/j.geomorph.2016.11.021</pub-id>
        </element-citation>
      </ref>
      <ref id="B42-remotesensing-10-00641">
        <label>42.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Toth</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>J&#xF3;&#x17A;k&#xF3;w</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Remote sensing platforms and sensors: A survey</article-title>
          <source>ISPRS J. Photogramm. Remote Sens.</source>
          <year>2016</year>
          <volume>115</volume>
          <fpage>22</fpage>
          <lpage>36</lpage>
          <pub-id pub-id-type="doi">10.1016/j.isprsjprs.2015.10.004</pub-id>
        </element-citation>
      </ref>
      <ref id="B43-remotesensing-10-00641">
        <label>43.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Geipel</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Link</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Claupein</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <article-title>Combined spectral and spatial modeling of corn yield based on aerial images and crop surface models acquired with an unmanned aircraft system</article-title>
          <source>Remote Sens.</source>
          <year>2014</year>
          <volume>6</volume>
          <fpage>10335</fpage>
          <lpage>10355</lpage>
          <pub-id pub-id-type="doi">10.3390/rs61110335</pub-id>
        </element-citation>
      </ref>
      <ref id="B44-remotesensing-10-00641">
        <label>44.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Torres-Sanchez</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Pena</surname>
              <given-names>J.M.</given-names>
            </name>
            <name>
              <surname>de Castro</surname>
              <given-names>A.I.</given-names>
            </name>
            <name>
              <surname>Lopez-Granados</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <article-title>Multi-temporal mapping of the vegetation fraction in early-season wheat fields using images from UAV</article-title>
          <source>Comput. Electron. Agric.</source>
          <year>2014</year>
          <volume>103</volume>
          <fpage>104</fpage>
          <lpage>113</lpage>
          <pub-id pub-id-type="doi">10.1016/j.compag.2014.02.009</pub-id>
        </element-citation>
      </ref>
      <ref id="B45-remotesensing-10-00641">
        <label>45.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Saberioon</surname>
              <given-names>M.M.</given-names>
            </name>
            <name>
              <surname>Amina</surname>
              <given-names>M.S.M.</given-names>
            </name>
            <name>
              <surname>Anuar</surname>
              <given-names>A.R.</given-names>
            </name>
            <name>
              <surname>Gholizadeh</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Wayayokd</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Khairunniza-Bejo</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Assessment of rice leaf chlorophyll content using visible bands at different growth stages at both the leaf and canopy scale</article-title>
          <source>Int. J. Appl. Earth Obs. Geoinform.</source>
          <year>2014</year>
          <volume>32</volume>
          <fpage>35</fpage>
          <lpage>45</lpage>
          <pub-id pub-id-type="doi">10.1016/j.jag.2014.03.018</pub-id>
        </element-citation>
      </ref>
      <ref id="B46-remotesensing-10-00641">
        <label>46.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Jannoura</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Brinkmann</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Uteau</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Bruns</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Joergensen</surname>
              <given-names>R.G.</given-names>
            </name>
          </person-group>
          <article-title>Monitoring of crop biomass using true colour aerial photographs taken from a remote controlled hexacopter</article-title>
          <source>Biosyst. Eng.</source>
          <year>2015</year>
          <volume>129</volume>
          <fpage>341</fpage>
          <lpage>351</lpage>
          <pub-id pub-id-type="doi">10.1016/j.biosystemseng.2014.11.007</pub-id>
        </element-citation>
      </ref>
      <ref id="B47-remotesensing-10-00641">
        <label>47.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hunt</surname>
              <given-names>E.R.</given-names>
            </name>
            <name>
              <surname>Hivel</surname>
              <given-names>W.D.</given-names>
            </name>
            <name>
              <surname>Fujikawa</surname>
              <given-names>S.J.</given-names>
            </name>
            <name>
              <surname>Linden</surname>
              <given-names>D.S.</given-names>
            </name>
            <name>
              <surname>Daughtry</surname>
              <given-names>C.S.T.</given-names>
            </name>
            <name>
              <surname>McCarty</surname>
              <given-names>G.W.</given-names>
            </name>
          </person-group>
          <article-title>Acquisition of NIR-green-blue digital photographs from unmanned aircraft for crop monitoring</article-title>
          <source>Remote Sens.</source>
          <year>2010</year>
          <volume>2</volume>
          <fpage>290</fpage>
          <lpage>305</lpage>
          <pub-id pub-id-type="doi">10.3390/rs2010290</pub-id>
        </element-citation>
      </ref>
      <ref id="B48-remotesensing-10-00641">
        <label>48.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Laliberte</surname>
              <given-names>A.S.</given-names>
            </name>
            <name>
              <surname>Goforth</surname>
              <given-names>M.A.</given-names>
            </name>
            <name>
              <surname>Steele</surname>
              <given-names>C.M.</given-names>
            </name>
            <name>
              <surname>Rango</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Multispectral Remote Sensing from Unmanned Aircraft: Image Processing Workflows and Applications for Rangeland Environments</article-title>
          <source>Remote Sens.</source>
          <year>2011</year>
          <volume>3</volume>
          <fpage>2529</fpage>
          <lpage>2551</lpage>
          <pub-id pub-id-type="doi">10.3390/rs3112529</pub-id>
        </element-citation>
      </ref>
      <ref id="B49-remotesensing-10-00641">
        <label>49.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Jhan</surname>
              <given-names>J.-P.</given-names>
            </name>
            <name>
              <surname>Rau</surname>
              <given-names>J.-Y.</given-names>
            </name>
            <name>
              <surname>Haala</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Cramer</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Investigation of parallax issues for multi-lens multispectral camera band co-registration</article-title>
          <source>Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci.</source>
          <year>2017</year>
          <volume>XLII-2/W6</volume>
          <fpage>157</fpage>
          <lpage>163</lpage>
          <pub-id pub-id-type="doi">10.5194/isprs-archives-XLII-2-W6-157-2017</pub-id>
        </element-citation>
      </ref>
      <ref id="B50-remotesensing-10-00641">
        <label>50.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lu</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>Species classification using Unmanned Aerial Vehicle (UAV)-acquired high spatial resolution imagery in a heterogeneous grassland</article-title>
          <source>ISPRS J. Photogramm.</source>
          <year>2017</year>
          <volume>128</volume>
          <fpage>73</fpage>
          <lpage>85</lpage>
          <pub-id pub-id-type="doi">10.1016/j.isprsjprs.2017.03.011</pub-id>
        </element-citation>
      </ref>
      <ref id="B51-remotesensing-10-00641">
        <label>51.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Brook</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Ben-Dor</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <article-title>Supervised vicarious calibration (SVC) of hyperspectral remote-sensing data</article-title>
          <source>Remote Sens. Environ.</source>
          <year>2011</year>
          <volume>115</volume>
          <fpage>1543</fpage>
          <lpage>1555</lpage>
          <pub-id pub-id-type="doi">10.1016/j.rse.2011.02.013</pub-id>
        </element-citation>
      </ref>
      <ref id="B52-remotesensing-10-00641">
        <label>52.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zarco-Tejada</surname>
              <given-names>P.J.</given-names>
            </name>
            <name>
              <surname>Gonzalez-Dugo</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Berni</surname>
              <given-names>J.A.J.</given-names>
            </name>
          </person-group>
          <article-title>Fluorescence, temperature and narrow-band indices acquired from a UAV platform for water stress detection using a microhyperspectral imager and a thermal camera</article-title>
          <source>Remote Sens. Environ.</source>
          <year>2012</year>
          <volume>117</volume>
          <fpage>322</fpage>
          <lpage>337</lpage>
          <pub-id pub-id-type="doi">10.1016/j.rse.2011.10.007</pub-id>
        </element-citation>
      </ref>
      <ref id="B53-remotesensing-10-00641">
        <label>53.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Honkavaara</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Rosnell</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Oliveira</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Tommaselli</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Band registration of tuneable frame format hyperspectral UAV imagers in complex scenes</article-title>
          <source>ISPRS J. Photogramm.</source>
          <year>2017</year>
          <volume>134</volume>
          <fpage>96</fpage>
          <lpage>109</lpage>
          <pub-id pub-id-type="doi">10.1016/j.isprsjprs.2017.10.014</pub-id>
        </element-citation>
      </ref>
      <ref id="B54-remotesensing-10-00641">
        <label>54.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Burkart</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Aasen</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Alonso</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Menz</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Bareth</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Rascher</surname>
              <given-names>U.</given-names>
            </name>
          </person-group>
          <article-title>Angular dependency of hyperspectral measurements over wheat characterized by a novel UAV based goniometer</article-title>
          <source>Remote Sens.</source>
          <year>2015</year>
          <volume>7</volume>
          <fpage>725</fpage>
          <lpage>746</lpage>
          <pub-id pub-id-type="doi">10.3390/rs70100725</pub-id>
        </element-citation>
      </ref>
      <ref id="B55-remotesensing-10-00641">
        <label>55.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ben-Dor</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Chabrillat</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Dematt&#xEA;</surname>
              <given-names>J.A.M.</given-names>
            </name>
            <name>
              <surname>Taylor</surname>
              <given-names>G.R.</given-names>
            </name>
            <name>
              <surname>Hill</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Whiting</surname>
              <given-names>M.L.</given-names>
            </name>
            <name>
              <surname>Sommer</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Using imaging spectroscopy to study soil properties</article-title>
          <source>Remote Sens. Environ.</source>
          <year>2009</year>
          <volume>113</volume>
          <fpage>S38</fpage>
          <lpage>S55</lpage>
          <pub-id pub-id-type="doi">10.1016/j.rse.2008.09.019</pub-id>
        </element-citation>
      </ref>
      <ref id="B56-remotesensing-10-00641">
        <label>56.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Brook</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Ben-Dor</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <article-title>Supervised vicarious calibration (SVC) of multi-source hyperspectral remote-sensing data</article-title>
          <source>Remote Sens.</source>
          <year>2015</year>
          <volume>7</volume>
          <fpage>6196</fpage>
          <lpage>6223</lpage>
          <pub-id pub-id-type="doi">10.3390/rs70506196</pub-id>
        </element-citation>
      </ref>
      <ref id="B57-remotesensing-10-00641">
        <label>57.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Smigaj</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Gaulton</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suarez</surname>
              <given-names>J.C.</given-names>
            </name>
            <name>
              <surname>Barr</surname>
              <given-names>S.L.</given-names>
            </name>
          </person-group>
          <article-title>Use of miniature thermal cameras for detection of physiological stress in conifers</article-title>
          <source>Remote Sens.</source>
          <year>2017</year>
          <volume>9</volume>
          <elocation-id>20</elocation-id>
          <pub-id pub-id-type="doi">10.3390/rs9090957</pub-id>
        </element-citation>
      </ref>
      <ref id="B58-remotesensing-10-00641">
        <label>58.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sankey</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Donager</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>McVay</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Sankey</surname>
              <given-names>J.B.</given-names>
            </name>
          </person-group>
          <article-title>UAV LiDAR and hyperspectral fusion for forest monitoring in the southwestern USA</article-title>
          <source>Remote Sens. Environ.</source>
          <year>2017</year>
          <volume>195</volume>
          <fpage>30</fpage>
          <lpage>43</lpage>
          <pub-id pub-id-type="doi">10.1016/j.rse.2017.04.007</pub-id>
        </element-citation>
      </ref>
      <ref id="B59-remotesensing-10-00641">
        <label>59.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>James</surname>
              <given-names>M.R.</given-names>
            </name>
            <name>
              <surname>Robson</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Smith</surname>
              <given-names>M.W.</given-names>
            </name>
          </person-group>
          <article-title>3-D uncertainty-based topographic change detection with structure-from-motion photogrammetry: Precision maps for ground controland directly georeferenced surveys</article-title>
          <source>Earth Surf. Process. Landf.</source>
          <year>2017</year>
          <volume>42</volume>
          <fpage>1769</fpage>
          <lpage>1788</lpage>
          <pub-id pub-id-type="doi">10.1002/esp.4125</pub-id>
        </element-citation>
      </ref>
      <ref id="B60-remotesensing-10-00641">
        <label>60.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Manfreda Oscaro</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Caylor</surname>
              <given-names>K.K.</given-names>
            </name>
          </person-group>
          <article-title>On the Vulnerability of Water Limited Ecosystems to Climate Change</article-title>
          <source>Water</source>
          <year>2013</year>
          <volume>5</volume>
          <fpage>819</fpage>
          <lpage>833</lpage>
          <pub-id pub-id-type="doi">10.3390/w5020819</pub-id>
        </element-citation>
      </ref>
      <ref id="B61-remotesensing-10-00641">
        <label>61.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Manfreda Oscaro</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Caylor</surname>
              <given-names>K.K.</given-names>
            </name>
            <name>
              <surname>Good</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>An Ecohydrological framework to explain shifts in vegetation organization across climatological gradients</article-title>
          <source>Ecohydrology</source>
          <year>2017</year>
          <volume>10</volume>
          <fpage>1</fpage>
          <lpage>14</lpage>
          <pub-id pub-id-type="doi">10.1002/eco.1809</pub-id>
        </element-citation>
      </ref>
      <ref id="B62-remotesensing-10-00641">
        <label>62.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Atzberger</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>Advances in remote sensing of agriculture: Context description, existing operational monitoring systems and major information needs</article-title>
          <source>Remote Sens.</source>
          <year>2013</year>
          <volume>5</volume>
          <fpage>949</fpage>
          <lpage>981</lpage>
          <pub-id pub-id-type="doi">10.3390/rs5020949</pub-id>
        </element-citation>
      </ref>
      <ref id="B63-remotesensing-10-00641">
        <label>63.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Kovacs</surname>
              <given-names>J.M.</given-names>
            </name>
          </person-group>
          <article-title>The application of small unmanned aerial systems for precision agriculture: A review</article-title>
          <source>Precis. Agric.</source>
          <year>2012</year>
          <volume>13</volume>
          <fpage>693</fpage>
          <pub-id pub-id-type="doi">10.1007/s11119-012-9274-5</pub-id>
        </element-citation>
      </ref>
      <ref id="B64-remotesensing-10-00641">
        <label>64.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Huang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Thomson</surname>
              <given-names>S.J.</given-names>
            </name>
            <name>
              <surname>Hoffmann</surname>
              <given-names>W.C.</given-names>
            </name>
            <name>
              <surname>Lan</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Fritz</surname>
              <given-names>B.K.</given-names>
            </name>
          </person-group>
          <article-title>Development and prospect of unmanned aerial vehicle technologies for agricultural production management</article-title>
          <source>Int. J. Agric. Biol. Eng.</source>
          <year>2013</year>
          <volume>6</volume>
          <fpage>1</fpage>
          <lpage>10</lpage>
        </element-citation>
      </ref>
      <ref id="B65-remotesensing-10-00641">
        <label>65.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Link</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Senner</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Claupein</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <article-title>Developing and evaluating an aerial sensor platform (ASP) to collect multispectral data for deriving management decisions in precision farming</article-title>
          <source>Comput. Electron. Agric.</source>
          <year>2013</year>
          <volume>94</volume>
          <fpage>20</fpage>
          <lpage>28</lpage>
          <pub-id pub-id-type="doi">10.1016/j.compag.2013.03.003</pub-id>
        </element-citation>
      </ref>
      <ref id="B66-remotesensing-10-00641">
        <label>66.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Walters</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Kovacs</surname>
              <given-names>J.M.</given-names>
            </name>
          </person-group>
          <article-title>Applications of low altitude remote sensing in agriculture upon farmer requests&#x2014;A case study in northeastern Ontario, Canada</article-title>
          <source>PLoS ONE</source>
          <year>2014</year>
          <volume>9</volume>
          <elocation-id>e112894</elocation-id>
          <pub-id pub-id-type="doi">10.1371/journal.pone.0112894</pub-id>
          <pub-id pub-id-type="pmid">25386696</pub-id>
        </element-citation>
      </ref>
      <ref id="B67-remotesensing-10-00641">
        <label>67.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Helman</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Givati</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Lensky</surname>
              <given-names>I.M.</given-names>
            </name>
          </person-group>
          <article-title>Annual evapotranspiration retrieved from satellite vegetation indices for the Eastern Mediterranean at 250 m spatial resolution</article-title>
          <source>Atmos. Chem. Phys.</source>
          <year>2015</year>
          <volume>15</volume>
          <fpage>12567</fpage>
          <lpage>12579</lpage>
          <pub-id pub-id-type="doi">10.5194/acp-15-12567-2015</pub-id>
        </element-citation>
      </ref>
      <ref id="B68-remotesensing-10-00641">
        <label>68.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gago</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Douthe</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Coopman</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Gallego</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Ribas-Carbo</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Flexas</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Escalona</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Medrano</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>UAVs challenge to assess water stress for sustainable agriculture</article-title>
          <source>Agric. Water Manag.</source>
          <year>2015</year>
          <volume>153</volume>
          <fpage>9</fpage>
          <lpage>19</lpage>
          <pub-id pub-id-type="doi">10.1016/j.agwat.2015.01.020</pub-id>
        </element-citation>
      </ref>
      <ref id="B69-remotesensing-10-00641">
        <label>69.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Helman</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Lensky</surname>
              <given-names>I.M.</given-names>
            </name>
            <name>
              <surname>Osem</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Rohatyn</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Rotenberg</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Yakir</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>A biophysical approach using water deficit factor for daily estimations of evapotranspiration and CO<sub>2</sub> uptake in Mediterranean environments</article-title>
          <source>Biogeosciences</source>
          <year>2017</year>
          <volume>14</volume>
          <fpage>3909</fpage>
          <lpage>3926</lpage>
          <pub-id pub-id-type="doi">10.5194/bg-14-3909-2017</pub-id>
        </element-citation>
      </ref>
      <ref id="B70-remotesensing-10-00641">
        <label>70.</label>
        <element-citation publication-type="other">
          <person-group person-group-type="author">
            <name>
              <surname>Lacaze</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Caselles</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Coll</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Hill</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Hoff</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>de Jong</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Mehl</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Negendank</surname>
              <given-names>J.F.</given-names>
            </name>
            <name>
              <surname>Riesebos</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Rubio</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Sommer</surname>
              <given-names>S.</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>DeMon, Integrated approaches to desertification mapping and monitoring in the Mediterranean basin</article-title>
          <comment>Final report of De-Mon I Project, Joint</comment>
          <publisher-name>Research Centre of European Commission</publisher-name>
          <publisher-loc>Ispra (VA), Italy</publisher-loc>
          <year>1996</year>
        </element-citation>
      </ref>
      <ref id="B71-remotesensing-10-00641">
        <label>71.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gigante</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Milella</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Iacobellis</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Manfreda Oscaro</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Portoghese</surname>
              <given-names>I.</given-names>
            </name>
          </person-group>
          <article-title>Influences of Leaf Area Index estimations on the soil water balance predictions in Mediterranean regions</article-title>
          <source>Nat. Hazard Earth Syst. Sci.</source>
          <year>2009</year>
          <volume>9</volume>
          <fpage>979</fpage>
          <lpage>991</lpage>
          <pub-id pub-id-type="doi">10.5194/nhess-9-979-2009</pub-id>
        </element-citation>
      </ref>
      <ref id="B72-remotesensing-10-00641">
        <label>72.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Helman</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>Land surface phenology: What do we really &#x2018;see&#x2019; from space?</article-title>
          <source>Sci. Total Environ.</source>
          <year>2018</year>
          <volume>618</volume>
          <fpage>665</fpage>
          <lpage>673</lpage>
          <pub-id pub-id-type="doi">10.1016/j.scitotenv.2017.07.237</pub-id>
          <pub-id pub-id-type="pmid">29037474</pub-id>
        </element-citation>
      </ref>
      <ref id="B73-remotesensing-10-00641">
        <label>73.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Primicerio</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Di Gennaro</surname>
              <given-names>S.F.</given-names>
            </name>
            <name>
              <surname>Fiorillo</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Genesio</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Lugato</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Matese</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Vaccari</surname>
              <given-names>F.P.</given-names>
            </name>
          </person-group>
          <article-title>A Flexible Unmanned Aerial Vehicle for Precision Agriculture</article-title>
          <source>Precis. Agric.</source>
          <year>2012</year>
          <volume>13</volume>
          <fpage>517</fpage>
          <lpage>523</lpage>
          <pub-id pub-id-type="doi">10.1007/s11119-012-9257-6</pub-id>
        </element-citation>
      </ref>
      <ref id="B74-remotesensing-10-00641">
        <label>74.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>McGwire</surname>
              <given-names>K.C.</given-names>
            </name>
            <name>
              <surname>Weltz</surname>
              <given-names>M.A.</given-names>
            </name>
            <name>
              <surname>Finzel</surname>
              <given-names>J.A.</given-names>
            </name>
            <name>
              <surname>Morris</surname>
              <given-names>C.E.</given-names>
            </name>
            <name>
              <surname>Fenstermaker</surname>
              <given-names>L.F.</given-names>
            </name>
            <name>
              <surname>McGraw</surname>
              <given-names>D.S.</given-names>
            </name>
          </person-group>
          <article-title>Multiscale Assessment of Green Leaf Cover in a Semi-Arid Rangeland with a Small Unmanned Aerial Vehicle</article-title>
          <source>Int. J. Remote Sens.</source>
          <year>2013</year>
          <volume>34</volume>
          <fpage>1615</fpage>
          <lpage>1632</lpage>
          <pub-id pub-id-type="doi">10.1080/01431161.2012.723836</pub-id>
        </element-citation>
      </ref>
      <ref id="B75-remotesensing-10-00641">
        <label>75.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hmimina</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Dufrene</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Pontailler</surname>
              <given-names>J.Y.</given-names>
            </name>
            <name>
              <surname>Delpierre</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Aubinet</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Caquet</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>de Grandcourt</surname>
              <given-names>A.S.</given-names>
            </name>
            <name>
              <surname>Burban</surname>
              <given-names>B.T.</given-names>
            </name>
            <name>
              <surname>Flechard</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Granier</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Evaluation of the potential of MODIS satellite data to predict vegetation phenology in different biomes: An investigation using ground-based NDVI measurements</article-title>
          <source>Remote Sens. Environ.</source>
          <year>2013</year>
          <volume>132</volume>
          <fpage>145</fpage>
          <lpage>158</lpage>
          <pub-id pub-id-type="doi">10.1016/j.rse.2013.01.010</pub-id>
        </element-citation>
      </ref>
      <ref id="B76-remotesensing-10-00641">
        <label>76.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Johnson</surname>
              <given-names>L.F.</given-names>
            </name>
            <name>
              <surname>Herwitz</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Dunagan</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Lobitz</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Sullivan</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Slye</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Collection of Ultra High Spatial and Spectral Resolution Image Data over California Vineyards with a Small UAV</article-title>
          <source>Proceedings of the 30th International Symposium on Remote Sensing of Environment</source>
          <conf-loc>Honolulu, Hawaii</conf-loc>
          <conf-date>10&#x2013;14 November 2003</conf-date>
          <fpage>845</fpage>
          <lpage>849</lpage>
        </element-citation>
      </ref>
      <ref id="B77-remotesensing-10-00641">
        <label>77.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zarco-Tejada</surname>
              <given-names>P.J.</given-names>
            </name>
            <name>
              <surname>Catalina</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Gonzalez</surname>
              <given-names>M.R.</given-names>
            </name>
            <name>
              <surname>Martin</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>Relationships between net photosynthesis and steady-state chlorophyll fluorescence retrieved from airborne hyperspectral imagery</article-title>
          <source>Remote Sens. Environ.</source>
          <year>2013</year>
          <volume>136</volume>
          <fpage>247</fpage>
          <lpage>258</lpage>
          <pub-id pub-id-type="doi">10.1016/j.rse.2013.05.011</pub-id>
        </element-citation>
      </ref>
      <ref id="B78-remotesensing-10-00641">
        <label>78.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zarco-Tejada</surname>
              <given-names>P.J.</given-names>
            </name>
            <name>
              <surname>Gonzalez-Dugo</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Williams</surname>
              <given-names>L.E.</given-names>
            </name>
            <name>
              <surname>Suarez</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Berni</surname>
              <given-names>J.A.J.</given-names>
            </name>
            <name>
              <surname>Goldhamer</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Fereres</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <article-title>A PRI-based water stress index combining structural and chlorophyll effects: Assessment using diurnal narrow-band airborne imagery and the CWSI thermal index</article-title>
          <source>Remote Sens. Environ.</source>
          <year>2013</year>
          <volume>138</volume>
          <fpage>38</fpage>
          <lpage>50</lpage>
          <pub-id pub-id-type="doi">10.1016/j.rse.2013.07.024</pub-id>
        </element-citation>
      </ref>
      <ref id="B79-remotesensing-10-00641">
        <label>79.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zarco-Tejada</surname>
              <given-names>P.J.</given-names>
            </name>
            <name>
              <surname>Guillen-Climent</surname>
              <given-names>M.L.</given-names>
            </name>
            <name>
              <surname>Hernandez-Clement</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Catalinac</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Gonzalez</surname>
              <given-names>M.R.</given-names>
            </name>
            <name>
              <surname>Martin</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>Estimating leaf carotenoid content in vineyards using high resolution hyperspectral imagery acquired from an unmanned aerial vehicle (UAV)</article-title>
          <source>Agric. For. Meteorol.</source>
          <year>2013</year>
          <volume>171&#x2013;172</volume>
          <fpage>281</fpage>
          <lpage>294</lpage>
          <pub-id pub-id-type="doi">10.1016/j.agrformet.2012.12.013</pub-id>
        </element-citation>
      </ref>
      <ref id="B80-remotesensing-10-00641">
        <label>80.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zarco-Tejada</surname>
              <given-names>P.J.</given-names>
            </name>
            <name>
              <surname>Suarez</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Gonzalez-Dugo</surname>
              <given-names>V.</given-names>
            </name>
          </person-group>
          <article-title>Spatial resolution effects on chlorophyll fluorescence retrieval in a heterogeneous canopy using hyperspectral imagery and radiative transfer simulation</article-title>
          <source>IEEE Geosci. Remote Sens. Lett.</source>
          <year>2013</year>
          <volume>10</volume>
          <fpage>937</fpage>
          <lpage>941</lpage>
          <pub-id pub-id-type="doi">10.1109/LGRS.2013.2252877</pub-id>
        </element-citation>
      </ref>
      <ref id="B81-remotesensing-10-00641">
        <label>81.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hassan-Esfahani</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Torres-Rua</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Jensen</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>McKee</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Assessment of Surface Soil Moisture Using High Resolution Multi-Spectral Imagery and Artificial Neural Networks</article-title>
          <source>Remote Sens.</source>
          <year>2015</year>
          <volume>7</volume>
          <fpage>2627</fpage>
          <lpage>2646</lpage>
          <pub-id pub-id-type="doi">10.3390/rs70302627</pub-id>
        </element-citation>
      </ref>
      <ref id="B82-remotesensing-10-00641">
        <label>82.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Manfreda Oscaro</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Brocca</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Moramarco</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Melone</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Sheffield</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>A physically based approach for the estimation of root-zone soil moisture from surface measurements</article-title>
          <source>Hydrol. Earth Syst. Sci.</source>
          <year>2014</year>
          <volume>18</volume>
          <fpage>1199</fpage>
          <lpage>1212</lpage>
          <pub-id pub-id-type="doi">10.5194/hess-18-1199-2014</pub-id>
        </element-citation>
      </ref>
      <ref id="B83-remotesensing-10-00641">
        <label>83.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Baldwin</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Manfreda Oscaro</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Keller</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Smithwick</surname>
              <given-names>E.A.H.</given-names>
            </name>
          </person-group>
          <article-title>Predicting root zone soil moisture with soil properties and satellite near-surface moisture data at locations across the United States</article-title>
          <source>J. Hydrol.</source>
          <year>2017</year>
          <volume>546</volume>
          <fpage>393</fpage>
          <lpage>404</lpage>
          <pub-id pub-id-type="doi">10.1016/j.jhydrol.2017.01.020</pub-id>
        </element-citation>
      </ref>
      <ref id="B84-remotesensing-10-00641">
        <label>84.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sullivan</surname>
              <given-names>D.G.</given-names>
            </name>
            <name>
              <surname>Fulton</surname>
              <given-names>J.P.</given-names>
            </name>
            <name>
              <surname>Shaw</surname>
              <given-names>J.N.</given-names>
            </name>
            <name>
              <surname>Bland</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Evaluating the sensitivity of an unmanned thermal infrared aerial system to detect water stress in a cotton canopy</article-title>
          <source>Trans. Am. Soc. Agric. Eng.</source>
          <year>2007</year>
          <volume>50</volume>
          <fpage>1955</fpage>
          <lpage>1962</lpage>
          <pub-id pub-id-type="doi">10.13031/2013.24091</pub-id>
        </element-citation>
      </ref>
      <ref id="B85-remotesensing-10-00641">
        <label>85.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>de Lima</surname>
              <given-names>J.L.M.P.</given-names>
            </name>
            <name>
              <surname>Abrantes</surname>
              <given-names>J.R.C.B.</given-names>
            </name>
          </person-group>
          <article-title>Can infrared thermography be used to estimate soil surface microrelief and rill morphology?</article-title>
          <source>Catena</source>
          <year>2014</year>
          <volume>113</volume>
          <fpage>314</fpage>
          <lpage>322</lpage>
          <pub-id pub-id-type="doi">10.1016/j.catena.2013.08.011</pub-id>
        </element-citation>
      </ref>
      <ref id="B86-remotesensing-10-00641">
        <label>86.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Abrantes</surname>
              <given-names>J.R.C.B.</given-names>
            </name>
            <name>
              <surname>de Lima</surname>
              <given-names>J.L.M.P.</given-names>
            </name>
            <name>
              <surname>Prats</surname>
              <given-names>S.A.</given-names>
            </name>
            <name>
              <surname>Keizer</surname>
              <given-names>J.J.</given-names>
            </name>
          </person-group>
          <article-title>Assessing soil water repellency spatial variability using a thermographic technique: An exploratory study using a small-scale laboratory soil flume</article-title>
          <source>Geoderma</source>
          <year>2017</year>
          <volume>287</volume>
          <fpage>98</fpage>
          <lpage>104</lpage>
          <pub-id pub-id-type="doi">10.1016/j.geoderma.2016.08.014</pub-id>
        </element-citation>
      </ref>
      <ref id="B87-remotesensing-10-00641">
        <label>87.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>De Lima</surname>
              <given-names>J.L.M.P.</given-names>
            </name>
            <name>
              <surname>Abrantes</surname>
              <given-names>J.R.C.B.</given-names>
            </name>
            <name>
              <surname>Silva</surname>
              <given-names>V.P.</given-names>
              <suffix>Jr.</suffix>
            </name>
            <name>
              <surname>de Lima</surname>
              <given-names>M.I.P.</given-names>
            </name>
            <name>
              <surname>Montenegro</surname>
              <given-names>A.A.A.</given-names>
            </name>
          </person-group>
          <article-title>Mapping soil surface macropores using infrared thermography: An exploratory laboratory study</article-title>
          <source>Sci. World J.</source>
          <year>2014</year>
          <pub-id pub-id-type="doi">10.1155/2014/845460</pub-id>
          <pub-id pub-id-type="pmid">25371915</pub-id>
        </element-citation>
      </ref>
      <ref id="B88-remotesensing-10-00641">
        <label>88.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>de Lima</surname>
              <given-names>J.L.M.P.</given-names>
            </name>
            <name>
              <surname>Abrantes</surname>
              <given-names>J.R.C.B.</given-names>
            </name>
            <name>
              <surname>Silva</surname>
              <given-names>V.P.</given-names>
              <suffix>Jr.</suffix>
            </name>
            <name>
              <surname>Montenegro</surname>
              <given-names>A.A.A.</given-names>
            </name>
          </person-group>
          <article-title>Prediction of skin surface soil permeability by infrared thermography: A soil flume experiment</article-title>
          <source>Quant. Infrared Thermogr. J.</source>
          <year>2014</year>
          <volume>11</volume>
          <fpage>161</fpage>
          <lpage>169</lpage>
          <pub-id pub-id-type="doi">10.1080/17686733.2014.945325</pub-id>
        </element-citation>
      </ref>
      <ref id="B89-remotesensing-10-00641">
        <label>89.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>de Lima</surname>
              <given-names>J.L.M.P.</given-names>
            </name>
            <name>
              <surname>Abrantes</surname>
              <given-names>J.R.C.B.</given-names>
            </name>
          </person-group>
          <article-title>Using a thermal tracer to estimate overland and rill flow velocities</article-title>
          <source>Earth Surf. Process. Landf.</source>
          <year>2014b</year>
          <volume>39</volume>
          <fpage>1293</fpage>
          <lpage>1300</lpage>
          <pub-id pub-id-type="doi">10.1002/esp.3523</pub-id>
        </element-citation>
      </ref>
      <ref id="B90-remotesensing-10-00641">
        <label>90.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Abrantes</surname>
              <given-names>J.R.C.B.</given-names>
            </name>
            <name>
              <surname>Moruzzi</surname>
              <given-names>R.B.</given-names>
            </name>
            <name>
              <surname>Silveira</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>de Lima</surname>
              <given-names>J.L.M.P.</given-names>
            </name>
          </person-group>
          <article-title>Comparison of thermal, salt and dye tracing to estimate shallow flow velocities: Novel triple tracer approach</article-title>
          <source>J. Hydrol.</source>
          <year>2018</year>
          <volume>557</volume>
          <fpage>362</fpage>
          <lpage>377</lpage>
          <pub-id pub-id-type="doi">10.1016/j.jhydrol.2017.12.048</pub-id>
        </element-citation>
      </ref>
      <ref id="B91-remotesensing-10-00641">
        <label>91.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Jackson</surname>
              <given-names>R.D.</given-names>
            </name>
            <name>
              <surname>Idso</surname>
              <given-names>S.B.</given-names>
            </name>
            <name>
              <surname>Reginato</surname>
              <given-names>R.J.</given-names>
            </name>
          </person-group>
          <article-title>Canopy temperature as a crop water stress indicator</article-title>
          <source>Water Resour. Res.</source>
          <year>1981</year>
          <volume>17</volume>
          <fpage>1133</fpage>
          <lpage>1138</lpage>
          <pub-id pub-id-type="doi">10.1029/WR017i004p01133</pub-id>
        </element-citation>
      </ref>
      <ref id="B92-remotesensing-10-00641">
        <label>92.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Cohen</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Alchanatis</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Saranga</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Rosenberg</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Sela</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Bosak</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Mapping water status based on aerial thermal imagery: Comparison of methodologies for upscaling from a single leaf to commercial fields</article-title>
          <source>Precis. Agric.</source>
          <year>2017</year>
          <volume>18</volume>
          <fpage>801</fpage>
          <lpage>822</lpage>
          <pub-id pub-id-type="doi">10.1007/s11119-016-9484-3</pub-id>
        </element-citation>
      </ref>
      <ref id="B93-remotesensing-10-00641">
        <label>93.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Baluja</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Diago</surname>
              <given-names>M.P.</given-names>
            </name>
            <name>
              <surname>Balda</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Zorer</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Meggio</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Morales</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Tardaguila</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Assessment of vineyard water status variability by thermal and multispectral imagery using an unmanned aerial vehicle (UAV)</article-title>
          <source>Irrig. Sci.</source>
          <year>2012</year>
          <volume>30</volume>
          <fpage>511</fpage>
          <lpage>522</lpage>
        </element-citation>
      </ref>
      <ref id="B94-remotesensing-10-00641">
        <label>94.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gago</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Douthe</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Florez-Sarasa</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Escalona</surname>
              <given-names>J.M.</given-names>
            </name>
            <name>
              <surname>Galmes</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Fernie</surname>
              <given-names>A.R.</given-names>
            </name>
            <name>
              <surname>Flexas</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Medrano</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>Opportunities for improving leaf water use efficiency under climate change conditions</article-title>
          <source>Plant Sci.</source>
          <year>2014</year>
          <volume>226</volume>
          <fpage>108</fpage>
          <lpage>119</lpage>
          <pub-id pub-id-type="doi">10.1016/j.plantsci.2014.04.007</pub-id>
          <pub-id pub-id-type="pmid">25113456</pub-id>
        </element-citation>
      </ref>
      <ref id="B95-remotesensing-10-00641">
        <label>95.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gonzalez-Dugo</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Zarco-Tejada</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Nicolas</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Nortes</surname>
              <given-names>P.A.</given-names>
            </name>
            <name>
              <surname>Alarcon</surname>
              <given-names>J.J.</given-names>
            </name>
            <name>
              <surname>Intrigliolo</surname>
              <given-names>D.S.</given-names>
            </name>
            <name>
              <surname>Fereres</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <article-title>Using high resolution UAV thermal imagery to assess the variability in the water status of five fruit tree species within a commercial orchard</article-title>
          <source>Precis. Agric.</source>
          <year>2013</year>
          <volume>14</volume>
          <fpage>660</fpage>
          <lpage>678</lpage>
          <pub-id pub-id-type="doi">10.1007/s11119-013-9322-9</pub-id>
        </element-citation>
      </ref>
      <ref id="B96-remotesensing-10-00641">
        <label>96.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bellvert</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zarco-Tejada</surname>
              <given-names>P.J.</given-names>
            </name>
            <name>
              <surname>Girona</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Fereres</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <article-title>Mapping crop water stress index in a &#x2018;Pinot-noir&#x2019; vineyard: Comparing ground measurements with thermal remote sensing imagery from an unmanned aerial vehicle</article-title>
          <source>Precis. Agric.</source>
          <year>2014</year>
          <volume>15</volume>
          <fpage>361</fpage>
          <lpage>376</lpage>
          <pub-id pub-id-type="doi">10.1007/s11119-013-9334-5</pub-id>
        </element-citation>
      </ref>
      <ref id="B97-remotesensing-10-00641">
        <label>97.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Santesteban</surname>
              <given-names>L.G.</given-names>
            </name>
            <name>
              <surname>Di Gennaro</surname>
              <given-names>S.F.</given-names>
            </name>
            <name>
              <surname>Herrero-Langreo</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Miranda</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Royo</surname>
              <given-names>J.B.</given-names>
            </name>
            <name>
              <surname>Matese</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>High-resolution UAV-based thermal imaging to estimate the instantaneous and seasonal variability of plant water status within a vineyard</article-title>
          <source>Agr. Water Manag.</source>
          <year>2017</year>
          <volume>183</volume>
          <fpage>49</fpage>
          <lpage>59</lpage>
          <pub-id pub-id-type="doi">10.1016/j.agwat.2016.08.026</pub-id>
        </element-citation>
      </ref>
      <ref id="B98-remotesensing-10-00641">
        <label>98.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ben-Dor</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Banin</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Visible and near-infrared (0.4&#x2013;1.1 &#x3BC;m) analysis of arid and semiarid soils</article-title>
          <source>Remote Sens. Environ.</source>
          <year>1994</year>
          <volume>48</volume>
          <fpage>261</fpage>
          <lpage>274</lpage>
          <pub-id pub-id-type="doi">10.1016/0034-4257(94)90001-9</pub-id>
        </element-citation>
      </ref>
      <ref id="B99-remotesensing-10-00641">
        <label>99.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Ben-Dor</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Banin</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Evaluation of several soil properties using convolved TM spectra</article-title>
          <source>Monitoring Soils in the Environment with Remote Sensing and GIS</source>
          <publisher-name>ORSTOM</publisher-name>
          <publisher-loc>Paris, France</publisher-loc>
          <year>1996</year>
          <fpage>135</fpage>
          <lpage>149</lpage>
        </element-citation>
      </ref>
      <ref id="B100-remotesensing-10-00641">
        <label>100.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Soriano-Disla</surname>
              <given-names>J.M.</given-names>
            </name>
            <name>
              <surname>Janik</surname>
              <given-names>L.J.</given-names>
            </name>
            <name>
              <surname>Viscarra Rossel</surname>
              <given-names>R.A.</given-names>
            </name>
            <name>
              <surname>Macdonald</surname>
              <given-names>L.M.</given-names>
            </name>
            <name>
              <surname>McLaughlin</surname>
              <given-names>M.J.</given-names>
            </name>
          </person-group>
          <article-title>The performance of visible, near-, and mid-infrared reflectance spectroscopy for prediction of soil physical, chemical, and biological properties</article-title>
          <source>Appl. Spectrosc. Rev.</source>
          <year>2014</year>
          <volume>49</volume>
          <fpage>139</fpage>
          <lpage>186</lpage>
          <pub-id pub-id-type="doi">10.1080/05704928.2013.811081</pub-id>
        </element-citation>
      </ref>
      <ref id="B101-remotesensing-10-00641">
        <label>101.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Costa</surname>
              <given-names>F.G.</given-names>
            </name>
            <name>
              <surname>Ueyama</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Braun</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Pessin</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Osorio</surname>
              <given-names>F.S.</given-names>
            </name>
            <name>
              <surname>Vargas</surname>
              <given-names>P.A.</given-names>
            </name>
          </person-group>
          <article-title>The use of unmanned aerial vehicles and wireless sensor network in agricultural applications</article-title>
          <source>Proceedings of the IEEE International Geoscience and Remote Sensing Symposium (IGARSS 2012)</source>
          <conf-loc>Munich, Germany</conf-loc>
          <conf-date>22&#x2013;27 July 2012</conf-date>
          <fpage>5045</fpage>
          <lpage>5048</lpage>
        </element-citation>
      </ref>
      <ref id="B102-remotesensing-10-00641">
        <label>102.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Pe&#xF1;a</surname>
              <given-names>J.M.</given-names>
            </name>
            <name>
              <surname>Torres-Sanchez</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>de Castro</surname>
              <given-names>A.I.</given-names>
            </name>
            <name>
              <surname>Kelly</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Lopez-Granados</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <article-title>Weed mapping in early-season maize fields using object-based analysis of unmanned aerial vehicle (UAV) images</article-title>
          <source>PLoS ONE</source>
          <year>2013</year>
          <volume>8</volume>
          <elocation-id>e77151</elocation-id>
          <pub-id pub-id-type="doi">10.1371/journal.pone.0077151</pub-id>
          <pub-id pub-id-type="pmid">24146963</pub-id>
        </element-citation>
      </ref>
      <ref id="B103-remotesensing-10-00641">
        <label>103.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Pe&#xF1;a</surname>
              <given-names>J.M.</given-names>
            </name>
            <name>
              <surname>Torres-Sanchez</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Serrano-Perez</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>de Castro</surname>
              <given-names>A.I.</given-names>
            </name>
            <name>
              <surname>Lopez-Granados</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <article-title>Quantifying Efficacy and Limits of Unmanned Aerial Vehicle (UAV) Technology for Weed Seedling Detection as Affected by Sensor Resolution</article-title>
          <source>Sensors</source>
          <year>2015</year>
          <volume>15</volume>
          <fpage>5609</fpage>
          <lpage>5626</lpage>
          <pub-id pub-id-type="doi">10.3390/s150305609</pub-id>
          <pub-id pub-id-type="pmid">25756867</pub-id>
        </element-citation>
      </ref>
      <ref id="B104-remotesensing-10-00641">
        <label>104.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tang</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Shao</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Drone remote sensing for forestry research and practices</article-title>
          <source>J. For. Res.</source>
          <year>2015</year>
          <volume>26</volume>
          <fpage>791</fpage>
          <lpage>797</lpage>
          <pub-id pub-id-type="doi">10.1007/s11676-015-0088-y</pub-id>
        </element-citation>
      </ref>
      <ref id="B105-remotesensing-10-00641">
        <label>105.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Torresan</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Berton</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Carotenuto</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Di Gennaro</surname>
              <given-names>S.F.</given-names>
            </name>
            <name>
              <surname>Gioli</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Matese</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Miglietta</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Vagnoli</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Zaldei</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Wallace</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <article-title>Forestry applications of UAVs in Europe: A review</article-title>
          <source>Int. J. Remote Sens.</source>
          <year>2017</year>
          <volume>38</volume>
          <fpage>2427</fpage>
          <lpage>2447</lpage>
        </element-citation>
      </ref>
      <ref id="B106-remotesensing-10-00641">
        <label>106.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Ventura</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Bonifazi</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Gravina</surname>
              <given-names>M.F.</given-names>
            </name>
            <name>
              <surname>Ardizzone</surname>
              <given-names>G.D.</given-names>
            </name>
          </person-group>
          <article-title>Unmanned Aerial Systems (UASs) for Environmental Monitoring: A Review with Applications in Coastal Habitats</article-title>
          <source>Aerial Robots-Aerodynamics, Control and Applications</source>
          <publisher-name>InTech</publisher-name>
          <publisher-loc>Rijeka, Croatia</publisher-loc>
          <year>2017</year>
          <pub-id pub-id-type="doi">10.5772/intechopen.69598</pub-id>
        </element-citation>
      </ref>
      <ref id="B107-remotesensing-10-00641">
        <label>107.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Jones</surname>
              <given-names>G.P.</given-names>
            </name>
            <name>
              <surname>Pearlstine</surname>
              <given-names>L.G.</given-names>
            </name>
            <name>
              <surname>Percival</surname>
              <given-names>H.F.</given-names>
            </name>
          </person-group>
          <article-title>An assessment of small unmanned aerial vehicles for wildlife research</article-title>
          <source>Wildl. Soc. Bull.</source>
          <year>2006</year>
          <volume>34</volume>
          <fpage>750</fpage>
          <lpage>758</lpage>
          <pub-id pub-id-type="doi">10.2193/0091-7648(2006)34[750:AAOSUA]2.0.CO;2</pub-id>
        </element-citation>
      </ref>
      <ref id="B108-remotesensing-10-00641">
        <label>108.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Chabot</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Bird</surname>
              <given-names>D.M.</given-names>
            </name>
          </person-group>
          <article-title>Evaluation of an off-the-shelf unmanned aircraft system for surveying flocks of geese</article-title>
          <source>Waterbirds</source>
          <year>2012</year>
          <volume>35</volume>
          <fpage>170</fpage>
          <lpage>174</lpage>
          <pub-id pub-id-type="doi">10.1675/063.035.0119</pub-id>
        </element-citation>
      </ref>
      <ref id="B109-remotesensing-10-00641">
        <label>109.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Getzin</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Wiegand</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Sch&#xF6;ning</surname>
              <given-names>I.</given-names>
            </name>
          </person-group>
          <article-title>Assessing biodiversity in forests using very high-resolution images and unmanned aerial vehicles</article-title>
          <source>Methods Ecol. Evol.</source>
          <year>2012</year>
          <volume>3</volume>
          <fpage>397</fpage>
          <lpage>404</lpage>
          <pub-id pub-id-type="doi">10.1111/j.2041-210X.2011.00158.x</pub-id>
        </element-citation>
      </ref>
      <ref id="B110-remotesensing-10-00641">
        <label>110.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Koh</surname>
              <given-names>L.P.</given-names>
            </name>
            <name>
              <surname>Wich</surname>
              <given-names>S.A.</given-names>
            </name>
          </person-group>
          <article-title>Dawn of drone ecology: Low-cost autonomous aerial vehicles for conservation</article-title>
          <source>Trop. Conserv. Sci.</source>
          <year>2012</year>
          <volume>5</volume>
          <fpage>121</fpage>
          <lpage>132</lpage>
          <pub-id pub-id-type="doi">10.1177/194008291200500202</pub-id>
        </element-citation>
      </ref>
      <ref id="B111-remotesensing-10-00641">
        <label>111.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Michez</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Pi&#xE9;gay</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Jonathan</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Claessens</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Lejeune</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>Mapping of riparian invasive species with supervised classification of Unmanned Aerial System (UAS) imagery</article-title>
          <source>Int. J. Appl. Earth Obs. Geoinform.</source>
          <year>2016</year>
          <volume>44</volume>
          <fpage>88</fpage>
          <lpage>94</lpage>
          <pub-id pub-id-type="doi">10.1016/j.jag.2015.06.014</pub-id>
        </element-citation>
      </ref>
      <ref id="B112-remotesensing-10-00641">
        <label>112.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Reif</surname>
              <given-names>M.K.</given-names>
            </name>
            <name>
              <surname>Theel</surname>
              <given-names>H.J.</given-names>
            </name>
          </person-group>
          <article-title>Remote sensing for restoration ecology: Application for restoring degraded, damaged, transformed, or destroyed ecosystems</article-title>
          <source>Integr. Environ. Assess. Manag.</source>
          <year>2017</year>
          <volume>13</volume>
          <fpage>614</fpage>
          <lpage>630</lpage>
          <pub-id pub-id-type="doi">10.1002/ieam.1847</pub-id>
          <pub-id pub-id-type="pmid">27627787</pub-id>
        </element-citation>
      </ref>
      <ref id="B113-remotesensing-10-00641">
        <label>113.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>McKenna</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Erskine</surname>
              <given-names>P.D.</given-names>
            </name>
            <name>
              <surname>Lechner</surname>
              <given-names>A.M.</given-names>
            </name>
            <name>
              <surname>Phinn</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Measuring fire severity using UAV imagery in semi-arid central Queensland, Australia</article-title>
          <source>Int. J. Remote Sens.</source>
          <year>2017</year>
          <volume>38</volume>
          <fpage>4244</fpage>
          <lpage>4264</lpage>
          <pub-id pub-id-type="doi">10.1080/01431161.2017.1317942</pub-id>
        </element-citation>
      </ref>
      <ref id="B114-remotesensing-10-00641">
        <label>114.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Klosterman</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Richardson</surname>
              <given-names>A.D.</given-names>
            </name>
          </person-group>
          <article-title>Observing Spring and Fall Phenology in a Deciduous Forest with Aerial Drone Imagery</article-title>
          <source>Sensors</source>
          <year>2017</year>
          <volume>17</volume>
          <elocation-id>2852</elocation-id>
          <pub-id pub-id-type="doi">10.3390/s17122852</pub-id>
          <pub-id pub-id-type="pmid">29292742</pub-id>
        </element-citation>
      </ref>
      <ref id="B115-remotesensing-10-00641">
        <label>115.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lehmann</surname>
              <given-names>J.R.K.</given-names>
            </name>
            <name>
              <surname>Nieberding</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Prinz</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Knoth</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>Analysis of unmanned aerial system-based CIR images in forestry&#x2014;A new perspective to monitor pest infestation levels</article-title>
          <source>Forests</source>
          <year>2015</year>
          <volume>6</volume>
          <fpage>594</fpage>
          <lpage>612</lpage>
          <pub-id pub-id-type="doi">10.3390/f6030594</pub-id>
        </element-citation>
      </ref>
      <ref id="B116-remotesensing-10-00641">
        <label>116.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Mina&#x159;&#xED;k</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Langhammer</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Use of a multispectral UAV photogrammetry for detection and tracking of forest disturbance dynamics</article-title>
          <source>Proceedings of the International Archives of the Photogrammetry, Remote Sensing &amp; Spatial Information Sciences</source>
          <conf-loc>Prague, Czech Republic</conf-loc>
          <conf-date>12&#x2013;19 July 2016</conf-date>
          <fpage>41</fpage>
        </element-citation>
      </ref>
      <ref id="B117-remotesensing-10-00641">
        <label>117.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ahmed</surname>
              <given-names>O.S.</given-names>
            </name>
            <name>
              <surname>Shemrock</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Chabot</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Dillon</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Williams</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Wasson</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Franklin</surname>
              <given-names>S.E.</given-names>
            </name>
          </person-group>
          <article-title>Hierarchical land cover and vegetation classification using multispectral data acquired from an unmanned aerial vehicle</article-title>
          <source>Int. J. Remote Sens.</source>
          <year>2017</year>
          <volume>38</volume>
          <fpage>2037</fpage>
          <lpage>2052</lpage>
        </element-citation>
      </ref>
      <ref id="B118-remotesensing-10-00641">
        <label>118.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Dandois</surname>
              <given-names>J.P.</given-names>
            </name>
            <name>
              <surname>Ellis</surname>
              <given-names>E.C.</given-names>
            </name>
          </person-group>
          <article-title>High spatial resolution three-dimensional mapping of vegetation spectral dynamics using computer vision</article-title>
          <source>Remote Sens. Environ.</source>
          <year>2013</year>
          <volume>136</volume>
          <fpage>259</fpage>
          <lpage>276</lpage>
          <pub-id pub-id-type="doi">10.1016/j.rse.2013.04.005</pub-id>
        </element-citation>
      </ref>
      <ref id="B119-remotesensing-10-00641">
        <label>119.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Puliti</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>&#xD8;rka</surname>
              <given-names>H.O.</given-names>
            </name>
            <name>
              <surname>Gobakken</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>N&#xE6;sset</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <article-title>Inventory of small forest areas using an unmanned aerial system</article-title>
          <source>Remote Sens.</source>
          <year>2015</year>
          <volume>7</volume>
          <fpage>9632</fpage>
          <lpage>9654</lpage>
          <pub-id pub-id-type="doi">10.3390/rs70809632</pub-id>
        </element-citation>
      </ref>
      <ref id="B120-remotesensing-10-00641">
        <label>120.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Dittmann</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Thiessen</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Hartung</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <article-title>Applicability of different non-invasive methods for tree mass estimation: A review</article-title>
          <source>For. Ecol. Manag.</source>
          <year>2017</year>
          <volume>398</volume>
          <fpage>208</fpage>
          <lpage>215</lpage>
          <pub-id pub-id-type="doi">10.1016/j.foreco.2017.05.013</pub-id>
        </element-citation>
      </ref>
      <ref id="B121-remotesensing-10-00641">
        <label>121.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Otero</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Van De Kerchove</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Satyanarayana</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Mart&#xED;nez-Espinosa</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Fisol</surname>
              <given-names>M.A.B.</given-names>
            </name>
            <name>
              <surname>Ibrahim</surname>
              <given-names>M.R.B.</given-names>
            </name>
            <name>
              <surname>Sulong</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Mohd-Lokman</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Lucas</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Dahdouh-Guebas</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <article-title>Managing mangrove forests from the sky: Forest inventory using field data and Unmanned Aerial Vehicle (UAV) imagery in the Matang Mangrove Forest Reserve, peninsular Malaysia</article-title>
          <source>For. Ecol. Manag.</source>
          <year>2018</year>
          <volume>411</volume>
          <fpage>35</fpage>
          <lpage>45</lpage>
          <pub-id pub-id-type="doi">10.1016/j.foreco.2017.12.049</pub-id>
        </element-citation>
      </ref>
      <ref id="B122-remotesensing-10-00641">
        <label>122.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Calvi&#xF1;o-Cancela</surname>
              <given-names>M.R.</given-names>
            </name>
            <name>
              <surname>Mendez-Rial</surname>
              <given-names>J.R.</given-names>
            </name>
            <name>
              <surname>Reguera-Salgado</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Mart&#xED;n-Herrero</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Alien plant monitoring with ultralight airborne imaging spectroscopy</article-title>
          <source>PLoS ONE</source>
          <year>2014</year>
          <volume>9</volume>
          <elocation-id>e102381</elocation-id>
        </element-citation>
      </ref>
      <ref id="B123-remotesensing-10-00641">
        <label>123.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hill</surname>
              <given-names>D.J.C.</given-names>
            </name>
            <name>
              <surname>Tarasoff</surname>
              <given-names>G.E.</given-names>
            </name>
            <name>
              <surname>Whitworth</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Baron</surname>
              <given-names>J.L.</given-names>
            </name>
            <name>
              <surname>Bradshaw</surname>
              <given-names>J.S.</given-names>
            </name>
          </person-group>
          <article-title>Church, Utility of unmanned aerial vehicles for mapping invasive plant species: A case study on yellow flag iris (<italic>Iris pseudacorus</italic> L.)</article-title>
          <source>Int. J. Remote Sens.</source>
          <year>2017</year>
          <volume>38</volume>
          <fpage>2083</fpage>
          <lpage>2105</lpage>
          <pub-id pub-id-type="doi">10.1080/01431161.2016.1264030</pub-id>
        </element-citation>
      </ref>
      <ref id="B124-remotesensing-10-00641">
        <label>124.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>M&#xFC;llerov&#xE1;</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Bartalo&#x161;</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Br&#x16F;na</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Dvo&#x159;&#xE1;k</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>V&#xED;tkov&#xE1;</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Unmanned aircraft in nature conservation&#x2014;An example from plant invasions</article-title>
          <source>Int. J. Remote Sens.</source>
          <year>2017</year>
          <volume>38</volume>
          <fpage>2177</fpage>
          <lpage>2198</lpage>
          <pub-id pub-id-type="doi">10.1080/01431161.2016.1275059</pub-id>
        </element-citation>
      </ref>
      <ref id="B125-remotesensing-10-00641">
        <label>125.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>M&#xFC;llerov&#xE1;</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Br&#x16F;na</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Bartalo&#x161;</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Dvo&#x159;&#xE1;k</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>V&#xED;tkov&#xE1;</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Py&#x161;ek</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>Timing is important: Unmanned aircraft versus satellite imagery in plant invasion monitoring</article-title>
          <source>Front. Plant Sci.</source>
          <year>2017</year>
          <volume>8</volume>
          <fpage>887</fpage>
          <pub-id pub-id-type="doi">10.3389/fpls.2017.00887</pub-id>
          <pub-id pub-id-type="pmid">28620399</pub-id>
        </element-citation>
      </ref>
      <ref id="B126-remotesensing-10-00641">
        <label>126.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Rocchini</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Andreo</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>F&#xF6;rster</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Garzon Lopez</surname>
              <given-names>C.X.</given-names>
            </name>
            <name>
              <surname>Gutierrez</surname>
              <given-names>A.P.</given-names>
            </name>
            <name>
              <surname>Gillespie</surname>
              <given-names>T.W.</given-names>
            </name>
            <name>
              <surname>Hauffe</surname>
              <given-names>H.C.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>K.S.</given-names>
            </name>
            <name>
              <surname>Kleinschmit</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Mairota</surname>
              <given-names>P.</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Potential of remote sensing to predict species invasions: A modelling perspective</article-title>
          <source>Prog. Phys. Geogr.</source>
          <year>2015</year>
          <volume>39</volume>
          <fpage>283</fpage>
          <lpage>309</lpage>
          <pub-id pub-id-type="doi">10.1177/0309133315574659</pub-id>
        </element-citation>
      </ref>
      <ref id="B127-remotesensing-10-00641">
        <label>127.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lehmann</surname>
              <given-names>J.R.</given-names>
            </name>
            <name>
              <surname>Prinz</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Ziller</surname>
              <given-names>S.R.</given-names>
            </name>
            <name>
              <surname>Thiele</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Heringer</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Meira-Neto</surname>
              <given-names>J.A.</given-names>
            </name>
            <name>
              <surname>Buttschardt</surname>
              <given-names>T.K.</given-names>
            </name>
          </person-group>
          <article-title>Open-source processing and analysis of aerial imagery acquired with a low-cost unmanned aerial system to support invasive plant management</article-title>
          <source>Front. Environm. Sci.</source>
          <year>2017</year>
          <volume>5</volume>
          <fpage>44</fpage>
          <pub-id pub-id-type="doi">10.3389/fenvs.2017.00044</pub-id>
        </element-citation>
      </ref>
      <ref id="B128-remotesensing-10-00641">
        <label>128.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Getzin</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Nuske</surname>
              <given-names>R.S.</given-names>
            </name>
            <name>
              <surname>Wiegand</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>Using unmanned aerial vehicles (UAV) to quantify spatial gap patterns in forests</article-title>
          <source>Remote Sens.</source>
          <year>2014</year>
          <volume>6</volume>
          <fpage>6988</fpage>
          <lpage>7004</lpage>
          <pub-id pub-id-type="doi">10.3390/rs6086988</pub-id>
        </element-citation>
      </ref>
      <ref id="B129-remotesensing-10-00641">
        <label>129.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Quilter</surname>
              <given-names>M.C.</given-names>
            </name>
            <name>
              <surname>Anderson</surname>
              <given-names>V.J.</given-names>
            </name>
          </person-group>
          <article-title>Low altitude/large scale aerial photographs: A tool for range and resource managers</article-title>
          <source>Rangel. Arch.</source>
          <year>2000</year>
          <volume>22</volume>
          <fpage>13</fpage>
          <lpage>17</lpage>
          <pub-id pub-id-type="doi">10.2458/azu_rangelands_v22i2_quilter</pub-id>
        </element-citation>
      </ref>
      <ref id="B130-remotesensing-10-00641">
        <label>130.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Knoth</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Klein</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Prinz</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Kleinebecker</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>Unmanned aerial vehicles as innovative remote sensing platforms for high-resolution infrared imagery to support restoration monitoring in cut-over bogs</article-title>
          <source>Appl. Veg. Sci.</source>
          <year>2013</year>
          <volume>16</volume>
          <fpage>509</fpage>
          <lpage>517</lpage>
          <pub-id pub-id-type="doi">10.1111/avsc.12024</pub-id>
        </element-citation>
      </ref>
      <ref id="B131-remotesensing-10-00641">
        <label>131.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tralli</surname>
              <given-names>D.M.</given-names>
            </name>
            <name>
              <surname>Blom</surname>
              <given-names>R.G.</given-names>
            </name>
            <name>
              <surname>Zlotnicki</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Donnellan</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Evans</surname>
              <given-names>D.L.</given-names>
            </name>
          </person-group>
          <article-title>Satellite Remote Sensing of Earthquake, Volcano, Flood, Landslide and Coastal Inundation Hazards</article-title>
          <source>ISPRS J. Photogramm. Remote Sens.</source>
          <year>2005</year>
          <volume>59</volume>
          <fpage>185</fpage>
          <lpage>198</lpage>
          <pub-id pub-id-type="doi">10.1016/j.isprsjprs.2005.02.002</pub-id>
        </element-citation>
      </ref>
      <ref id="B132-remotesensing-10-00641">
        <label>132.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gillespie</surname>
              <given-names>T.W.</given-names>
            </name>
            <name>
              <surname>Chu</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Frankenberg</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Thomas</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>Assessment and Prediction of Natural Hazards from Satellite Imagery</article-title>
          <source>Prog. Phys. Geogr.</source>
          <year>2007</year>
          <volume>31</volume>
          <fpage>459</fpage>
          <lpage>470</lpage>
          <pub-id pub-id-type="doi">10.1177/0309133307083296</pub-id>
          <pub-id pub-id-type="pmid">25170186</pub-id>
        </element-citation>
      </ref>
      <ref id="B133-remotesensing-10-00641">
        <label>133.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Joyce</surname>
              <given-names>K.E.</given-names>
            </name>
            <name>
              <surname>Belliss</surname>
              <given-names>S.E.</given-names>
            </name>
            <name>
              <surname>Samsonov</surname>
              <given-names>S.V.</given-names>
            </name>
            <name>
              <surname>McNeill</surname>
              <given-names>S.J.</given-names>
            </name>
            <name>
              <surname>Glassey</surname>
              <given-names>P.J.</given-names>
            </name>
          </person-group>
          <article-title>A Review of the Status of Satellite Remote Sensing and Image Processing Techniques for Mapping Natural Hazards and Disasters</article-title>
          <source>Prog. Phys. Geogr.</source>
          <year>2009</year>
          <volume>33</volume>
          <fpage>183</fpage>
          <lpage>207</lpage>
          <pub-id pub-id-type="doi">10.1177/0309133309339563</pub-id>
        </element-citation>
      </ref>
      <ref id="B134-remotesensing-10-00641">
        <label>134.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Quaritsch</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Kruggl</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Wischounig-Strucl</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Bhattacharya</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Shah</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Rinner</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <article-title>Networked UAVs as aerial sensor network for disaster management applications</article-title>
          <source>Elektrotech. Informationstech.</source>
          <year>2010</year>
          <volume>127</volume>
          <fpage>56</fpage>
          <lpage>63</lpage>
          <pub-id pub-id-type="doi">10.1007/s00502-010-0717-2</pub-id>
        </element-citation>
      </ref>
      <ref id="B135-remotesensing-10-00641">
        <label>135.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Erdelj</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Kr&#xF3;l</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Natalizio</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <article-title>Wireless sensor networks and multi-UAV systems for natural disaster management</article-title>
          <source>Comput. Netw.</source>
          <year>2017</year>
          <volume>124</volume>
          <fpage>72</fpage>
          <lpage>86</lpage>
          <pub-id pub-id-type="doi">10.1016/j.comnet.2017.05.021</pub-id>
        </element-citation>
      </ref>
      <ref id="B136-remotesensing-10-00641">
        <label>136.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Syvitski</surname>
              <given-names>J.P.M.</given-names>
            </name>
            <name>
              <surname>Overeem</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Brakenridge</surname>
              <given-names>G.R.</given-names>
            </name>
            <name>
              <surname>Hannon</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Floods, Floodplains, Delta Plains&#x2014;A Satellite Imaging Approach</article-title>
          <source>Sediment. Geol.</source>
          <year>2012</year>
          <volume>267&#x2013;268</volume>
          <fpage>1</fpage>
          <lpage>14</lpage>
          <pub-id pub-id-type="doi">10.1016/j.sedgeo.2012.05.014</pub-id>
        </element-citation>
      </ref>
      <ref id="B137-remotesensing-10-00641">
        <label>137.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Yilmaz</surname>
              <given-names>K.K.</given-names>
            </name>
            <name>
              <surname>Adlerab</surname>
              <given-names>R.F.</given-names>
            </name>
            <name>
              <surname>Tianbc</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Hongd</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Piercebe</surname>
              <given-names>H.F.</given-names>
            </name>
          </person-group>
          <article-title>Evaluation of a Satellite-Based Global Flood Monitoring System</article-title>
          <source>Int. J. Remote Sens.</source>
          <year>2010</year>
          <volume>31</volume>
          <fpage>3763</fpage>
          <lpage>3782</lpage>
          <pub-id pub-id-type="doi">10.1080/01431161.2010.483489</pub-id>
        </element-citation>
      </ref>
      <ref id="B138-remotesensing-10-00641">
        <label>138.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>D&#x2019;Addabbo</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Refice</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Pasquariello</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Lovergine</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Capolongo</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Manfreda Oscaro</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>A Bayesian Network for Flood Detection Combining SAR Imagery and Ancillary Data</article-title>
          <source>IEEE Trans. Geosci. Remote Sens.</source>
          <year>2016</year>
          <volume>54</volume>
          <fpage>3612</fpage>
          <lpage>3625</lpage>
          <pub-id pub-id-type="doi">10.1109/TGRS.2016.2520487</pub-id>
        </element-citation>
      </ref>
      <ref id="B139-remotesensing-10-00641">
        <label>139.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Fujita</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Muste</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Kruger</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Large-scale particle image velocimetry for flow analysis in hydraulic engineering applications</article-title>
          <source>J. Hydraul. Res.</source>
          <year>1997</year>
          <volume>36</volume>
          <fpage>397</fpage>
          <lpage>414</lpage>
          <pub-id pub-id-type="doi">10.1080/00221689809498626</pub-id>
        </element-citation>
      </ref>
      <ref id="B140-remotesensing-10-00641">
        <label>140.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Brevis</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Ni&#xF1;o</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Jirka</surname>
              <given-names>G.H.</given-names>
            </name>
          </person-group>
          <article-title>Integrating cross-correlation and relaxation algorithms for particle tracking velocimetry</article-title>
          <source>Exp. Fluids</source>
          <year>2011</year>
          <volume>50</volume>
          <fpage>135</fpage>
          <lpage>147</lpage>
          <pub-id pub-id-type="doi">10.1007/s00348-010-0907-z</pub-id>
        </element-citation>
      </ref>
      <ref id="B141-remotesensing-10-00641">
        <label>141.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Fujita</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Hino</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>Unseeded and seeded PIV measurements of river flows video from a helicopter</article-title>
          <source>J. Vis.</source>
          <year>2003</year>
          <volume>6</volume>
          <fpage>245</fpage>
          <lpage>252</lpage>
          <pub-id pub-id-type="doi">10.1007/BF03181465</pub-id>
        </element-citation>
      </ref>
      <ref id="B142-remotesensing-10-00641">
        <label>142.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Fujita</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Kunita</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>Application of aerial LSPIV to the 2002 flood of the Yodo River using a helicopter mounted high density video camera</article-title>
          <source>J. Hydro-Environ. Res.</source>
          <year>2011</year>
          <volume>5</volume>
          <fpage>323</fpage>
          <lpage>331</lpage>
          <pub-id pub-id-type="doi">10.1016/j.jher.2011.05.003</pub-id>
        </element-citation>
      </ref>
      <ref id="B143-remotesensing-10-00641">
        <label>143.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Detert</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Weitbrecht</surname>
              <given-names>V.</given-names>
            </name>
          </person-group>
          <article-title>A low-cost airborne velocimetry system: Proof of concept</article-title>
          <source>J. Hydraul. Res.</source>
          <year>2015</year>
          <volume>53</volume>
          <fpage>532</fpage>
          <lpage>539</lpage>
          <pub-id pub-id-type="doi">10.1080/00221686.2015.1054322</pub-id>
        </element-citation>
      </ref>
      <ref id="B144-remotesensing-10-00641">
        <label>144.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tauro</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Pagano</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Phamduy</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Grimaldi</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Porfiri</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Large-scale particle image velocimetry from an unmanned aerial vehicle</article-title>
          <source>IEEE/ASME Trans. Mechatron.</source>
          <year>2015</year>
          <volume>20</volume>
          <fpage>3269</fpage>
          <lpage>3275</lpage>
          <pub-id pub-id-type="doi">10.1109/TMECH.2015.2408112</pub-id>
        </element-citation>
      </ref>
      <ref id="B145-remotesensing-10-00641">
        <label>145.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tauro</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Porfiri</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Grimaldi</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Surface flow measurements from drones</article-title>
          <source>J. Hydrol.</source>
          <year>2016</year>
          <volume>540</volume>
          <fpage>240</fpage>
          <lpage>245</lpage>
          <pub-id pub-id-type="doi">10.1016/j.jhydrol.2016.06.012</pub-id>
        </element-citation>
      </ref>
      <ref id="B146-remotesensing-10-00641">
        <label>146.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tauro</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Petroselli</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Arcangeletti</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <article-title>Assessment of drone-based surface flow observations</article-title>
          <source>Hydrol. Process.</source>
          <year>2016</year>
          <volume>30</volume>
          <fpage>1114</fpage>
          <lpage>1130</lpage>
          <pub-id pub-id-type="doi">10.1002/hyp.10698</pub-id>
        </element-citation>
      </ref>
      <ref id="B147-remotesensing-10-00641">
        <label>147.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tauro</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Piscopia</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Grimaldi</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Streamflow observations from cameras: Large Scale Particle Image Velocimetry of Particle Tracking Velocimetry?</article-title>
          <source>Water Resour. Res.</source>
          <year>2018</year>
          <volume>53</volume>
          <fpage>10374</fpage>
          <lpage>10394</lpage>
          <pub-id pub-id-type="doi">10.1002/2017WR020848</pub-id>
        </element-citation>
      </ref>
      <ref id="B148-remotesensing-10-00641">
        <label>148.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sanyal</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Lu</surname>
              <given-names>X.X.</given-names>
            </name>
          </person-group>
          <article-title>Application of Remote Sensing in Flood Management with Special Reference to Monsoon Asia: A Review</article-title>
          <source>Na. Hazards</source>
          <year>2004</year>
          <volume>33</volume>
          <fpage>283</fpage>
          <lpage>301</lpage>
          <pub-id pub-id-type="doi">10.1023/B:NHAZ.0000037035.65105.95</pub-id>
        </element-citation>
      </ref>
      <ref id="B149-remotesensing-10-00641">
        <label>149.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Perks</surname>
              <given-names>M.T.</given-names>
            </name>
            <name>
              <surname>Russell</surname>
              <given-names>A.J.</given-names>
            </name>
            <name>
              <surname>Large</surname>
              <given-names>A.R.G.</given-names>
            </name>
          </person-group>
          <article-title>Technical Note: Advances in flash flood monitoring using unmanned aerial vehicles (UAVs)</article-title>
          <source>Hydrol. Earth Syst. Sci.</source>
          <year>2016</year>
          <volume>20</volume>
          <fpage>4005</fpage>
          <lpage>4015</lpage>
          <pub-id pub-id-type="doi">10.5194/hess-20-4005-2016</pub-id>
        </element-citation>
      </ref>
      <ref id="B150-remotesensing-10-00641">
        <label>150.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ferreira</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Chandler</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Wackrow</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Shiono</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>Automated extraction of free surface topography using SfM-MVS photogrammetry</article-title>
          <source>Flow Meas. Instrum.</source>
          <year>2017</year>
          <volume>54</volume>
          <fpage>243</fpage>
          <lpage>249</lpage>
          <pub-id pub-id-type="doi">10.1016/j.flowmeasinst.2017.02.001</pub-id>
        </element-citation>
      </ref>
      <ref id="B151-remotesensing-10-00641">
        <label>151.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bandini</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Butts</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Jacobsen Torsten</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Bauer-Gottwein</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>Water level observations from unmanned aerial vehicles for improving estimates of surface water&#x2013;groundwater interaction</article-title>
          <source>Hydrol. Process.</source>
          <year>2017</year>
          <volume>31</volume>
          <fpage>4371</fpage>
          <lpage>4383</lpage>
          <pub-id pub-id-type="doi">10.1002/hyp.11366</pub-id>
        </element-citation>
      </ref>
      <ref id="B152-remotesensing-10-00641">
        <label>152.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Detert</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Johnson</surname>
              <given-names>E.D.</given-names>
            </name>
            <name>
              <surname>Weitbrecht</surname>
              <given-names>V.</given-names>
            </name>
          </person-group>
          <article-title>Proof-of-concept for low-cost and non-contact synoptic airborne river flow measurements</article-title>
          <source>Int. J. Remote Sens.</source>
          <year>2017</year>
          <volume>38</volume>
          <fpage>2780</fpage>
          <lpage>2807</lpage>
          <pub-id pub-id-type="doi">10.1080/01431161.2017.1294782</pub-id>
        </element-citation>
      </ref>
      <ref id="B153-remotesensing-10-00641">
        <label>153.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Flynn</surname>
              <given-names>K.F.</given-names>
            </name>
            <name>
              <surname>Chapra</surname>
              <given-names>S.C.</given-names>
            </name>
          </person-group>
          <article-title>Remote sensing of submerged aquatic vegetation in a shallow non-turbid river using an unmanned aerial vehicle</article-title>
          <source>Remote Sens.</source>
          <year>2014</year>
          <volume>6</volume>
          <fpage>12815</fpage>
          <lpage>12836</lpage>
          <pub-id pub-id-type="doi">10.3390/rs61212815</pub-id>
        </element-citation>
      </ref>
      <ref id="B154-remotesensing-10-00641">
        <label>154.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Klemas</surname>
              <given-names>V.V.</given-names>
            </name>
          </person-group>
          <article-title>Coastal and Environmental Remote Sensing from Unmanned Aerial Vehicles: An Overview</article-title>
          <source>J. Coast. Res.</source>
          <year>2015</year>
          <volume>31</volume>
          <fpage>1260</fpage>
          <lpage>1267</lpage>
          <pub-id pub-id-type="doi">10.2112/JCOASTRES-D-15-00005.1</pub-id>
        </element-citation>
      </ref>
      <ref id="B155-remotesensing-10-00641">
        <label>155.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wigmore</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Bryan</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Monitoring tropical debris-covered glacier dynamics from high-resolution unmanned aerial vehicle photogrammetry, Cordillera Blanca, Peru</article-title>
          <source>Cryosphere</source>
          <year>2017</year>
          <volume>11</volume>
          <fpage>2463</fpage>
          <lpage>2480</lpage>
          <pub-id pub-id-type="doi">10.5194/tc-11-2463-2017</pub-id>
        </element-citation>
      </ref>
      <ref id="B156-remotesensing-10-00641">
        <label>156.</label>
        <element-citation publication-type="web">
          <person-group person-group-type="author">
            <name>
              <surname>Langridge</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Edwards</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <article-title>Future Batteries, Coming Soon: Charge in Seconds, Last Months and Power over the Air</article-title>
          <comment>Gadgets 2017</comment>
          <comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.pocket-lint.com/" ext-link-type="uri">https://www.pocket-lint.com/</ext-link></comment>
          <date-in-citation content-type="access-date" iso-8601-date="2017-02-13">(accessed on 13 February 2017)</date-in-citation>
        </element-citation>
      </ref>
    </ref-list>
    <sec sec-type="display-objects">
      <title>Figures</title>
      <fig id="remotesensing-10-00641-f001" position="float">
        <label>Figure 1</label>
        <caption>
          <p>Number of articles extracted from the database ISI-web of knowledge published from 1990 up to 2017 (last access 15 January 2018).</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="remotesensing-10-00641-g001.tif"/>
      </fig>
      <fig id="remotesensing-10-00641-f002" position="float">
        <label>Figure 2</label>
        <caption>
          <p>A thermal survey over an Aglianico vineyard in the Basilicata region (southern Italy) overlaying an RGB orthophoto obtained by a multicopter mounted with both optical and FLIR Tau 2 cameras. Insets (<bold>A</bold>) and (<bold>B</bold>) provide magnified portions of the thermal map, where it is possible to distinguish vineyard rows (<bold>B</bold>) and surface temperature distribution on bare soil with a spot of colder temperature due to higher soil water content (<bold>B</bold>).</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="remotesensing-10-00641-g002.tif"/>
      </fig>
      <fig id="remotesensing-10-00641-f003" position="float">
        <label>Figure 3</label>
        <caption>
          <p>Multi-spectral false colour (near infrared, red, green) imagery collected over the RoBo Alsahba date palm farm near Al Kharj, Saudi Arabia. Imagery (from L-R) shows the resolution differences between: (<bold>A</bold>) UAV mounted Parrot Sequoia sensor at 50 m height (0.05 m); (<bold>B</bold>) a WorldView-3 image (1.24 m); and (<bold>C</bold>) Planet CubeSat data (approx. 3 m), collected on the 13th, 29th and 27th March 2018, respectively.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="remotesensing-10-00641-g003.tif"/>
      </fig>
      <fig id="remotesensing-10-00641-f004" position="float">
        <label>Figure 4</label>
        <caption>
          <p>(<bold>A</bold>) A single RGB image of mangrove forest clearances, Matang Mangrove Forest Reserve, Malaysia, as observed using an RGB digital camera mounted on a DJI Phantom 3; (<bold>B</bold>) RGB orthomosaic from which individual (upper canopy) tree crowns can be identified as well as different mangrove species; and (<bold>C</bold>) the Canopy Height Model (CHM) derived from stereo RGB imagery, with darker green colors representing tall mangroves (typically &gt; 15 m) [<xref ref-type="bibr" rid="B121-remotesensing-10-00641">121</xref>].</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="remotesensing-10-00641-g004.tif"/>
      </fig>
      <fig id="remotesensing-10-00641-f005" position="float">
        <label>Figure 5</label>
        <caption>
          <p>Comparison of the most important aspects of UAS and satellite monitoring.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="remotesensing-10-00641-g005.tif"/>
      </fig>
    </sec>
  </back>
</article>
